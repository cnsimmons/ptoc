{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Correlation Analysis: Testing Dorsal Independence\n",
    "\n",
    "Analysis of pIPS connectivity after regressing out ventral (LO) influence.\n",
    "\n",
    "**Research Question**: Is dorsal pathway connectivity during object viewing independent of ventral pathway activity, or does it represent passive spillover?\n",
    "\n",
    "**Approach**: Compare original pIPS FC maps with 'cleaned' pIPS FC maps (ventral variance removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csimmon2/anaconda3/envs/fmri/lib/python3.9/site-packages/nilearn/input_data/__init__.py:23: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nilearn import image, plotting, datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 control subjects\n",
      "Original data path: /lab_data/behrmannlab/vlad/ptoc\n",
      "Residual data path: /user_data/csimmon2/ptoc_residuals\n",
      "Subjects: ['sub-025', 'sub-038', 'sub-057', 'sub-059', 'sub-064']...\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "study_dir = \"/lab_data/behrmannlab/vlad/ptoc\"  # For original FC maps\n",
    "residual_dir = \"/user_data/csimmon2/ptoc_residuals\"  # For cleaned FC maps\n",
    "results_dir = \"/user_data/csimmon2/git_repos/ptoc/results\"\n",
    "curr_dir = \"/user_data/csimmon2/git_repos/ptoc\"\n",
    "\n",
    "# Load subject info\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "subjects = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "\n",
    "print(f\"Found {len(subjects)} control subjects\")\n",
    "print(f\"Original data path: {study_dir}\")\n",
    "print(f\"Residual data path: {residual_dir}\")\n",
    "print(f\"Subjects: {subjects[:5]}...\")  # Show first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_fc_maps(subjects, analysis_type, roi, hemisphere, task='loc'):\n",
    "    \"\"\"Load FC maps for all subjects\"\"\"\n",
    "    maps = []\n",
    "    valid_subjects = []\n",
    "    \n",
    "    for sub in subjects:\n",
    "        if analysis_type == 'original':\n",
    "            # Original maps are in the main study directory\n",
    "            filepath = f\"{study_dir}/{sub}/ses-01/derivatives/fc_mni/{sub}_{roi}_{hemisphere}_{task}_fc_mni.nii.gz\"\n",
    "        elif analysis_type == 'cleaned':\n",
    "            # Cleaned maps are in the residual directory\n",
    "            filepath = f\"{residual_dir}/{sub}/ses-01/derivatives/fc_mni/{sub}_pIPS_clean_{hemisphere}_{task}_fc_mni.nii.gz\"\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            maps.append(filepath)\n",
    "            valid_subjects.append(sub)\n",
    "        else:\n",
    "            print(f\"Missing: {os.path.basename(filepath)} for {sub}\")\n",
    "    \n",
    "    roi_name = roi if analysis_type == 'original' else 'pIPS_clean'\n",
    "    print(f\"Found {len(maps)} {analysis_type} {roi_name} {hemisphere} maps\")\n",
    "    return maps, valid_subjects\n",
    "\n",
    "def compute_group_stats(maps, mask=None):\n",
    "    \"\"\"Compute group-level statistics\"\"\"\n",
    "    if not maps:\n",
    "        return None, None\n",
    "        \n",
    "    # Load all maps\n",
    "    imgs = [nib.load(map_path) for map_path in maps]\n",
    "    \n",
    "    # Compute group mean\n",
    "    group_img = image.mean_img(imgs)\n",
    "    \n",
    "    # Compute one-sample t-test if mask provided\n",
    "    if mask is not None:\n",
    "        masker = NiftiMasker(mask_img=mask)\n",
    "        data = masker.fit_transform(imgs)\n",
    "        t_stats, p_vals = stats.ttest_1samp(data, 0, axis=0)\n",
    "        t_img = masker.inverse_transform(t_stats)\n",
    "        return group_img, t_img\n",
    "    else:\n",
    "        return group_img, None\n",
    "\n",
    "def compute_dice_coefficient(img1, img2, threshold=2.3):\n",
    "    \"\"\"Compute Dice coefficient between two statistical maps\"\"\"\n",
    "    # Threshold images\n",
    "    mask1 = image.math_img(f'img > {threshold}', img=img1)\n",
    "    mask2 = image.math_img(f'img > {threshold}', img=img2)\n",
    "    \n",
    "    # Get binary arrays\n",
    "    data1 = mask1.get_fdata().flatten()\n",
    "    data2 = mask2.get_fdata().flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid = ~(np.isnan(data1) | np.isnan(data2))\n",
    "    data1, data2 = data1[valid], data2[valid]\n",
    "    \n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.sum(data1 * data2)\n",
    "    total = np.sum(data1) + np.sum(data2)\n",
    "    dice = (2 * intersection) / total if total > 0 else 0\n",
    "    \n",
    "    return dice\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FC Maps\n",
    "\n",
    "**Note: Make sure your MNI transformation completed successfully before running this section!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FC maps...\n",
      "\n",
      "Verifying data directories:\n",
      "\n",
      "sub-025:\n",
      "  Original dir exists: True\n",
      "  Residual dir exists: True\n",
      "  Residual files: 4 files\n",
      "    Examples: ['sub-025_LO_clean_right_loc_fc_mni.nii.gz', 'sub-025_pIPS_clean_right_loc_fc_mni.nii.gz']\n",
      "\n",
      "sub-038:\n",
      "  Original dir exists: True\n",
      "  Residual dir exists: True\n",
      "  Residual files: 4 files\n",
      "    Examples: ['sub-038_LO_clean_left_loc_fc_mni.nii.gz', 'sub-038_LO_clean_right_loc_fc_mni.nii.gz']\n",
      "\n",
      "sub-057:\n",
      "  Original dir exists: True\n",
      "  Residual dir exists: True\n",
      "  Residual files: 4 files\n",
      "    Examples: ['sub-057_LO_clean_right_loc_fc_mni.nii.gz', 'sub-057_pIPS_clean_left_loc_fc_mni.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading FC maps...\")\n",
    "\n",
    "# Quick verification - check if directories exist\n",
    "print(\"\\nVerifying data directories:\")\n",
    "for sub in subjects[:3]:  # Check first 3 subjects\n",
    "    orig_dir = f\"{study_dir}/{sub}/ses-01/derivatives/fc_mni\"\n",
    "    resid_dir = f\"{residual_dir}/{sub}/ses-01/derivatives/fc_mni\"\n",
    "    \n",
    "    print(f\"\\n{sub}:\")\n",
    "    print(f\"  Original dir exists: {os.path.exists(orig_dir)}\")\n",
    "    print(f\"  Residual dir exists: {os.path.exists(resid_dir)}\")\n",
    "    \n",
    "    if os.path.exists(resid_dir):\n",
    "        resid_files = [f for f in os.listdir(resid_dir) if f.endswith('.nii.gz')]\n",
    "        print(f\"  Residual files: {len(resid_files)} files\")\n",
    "        if resid_files:\n",
    "            print(f\"    Examples: {resid_files[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Loading all maps...\n",
      "Found 19 original pIPS left maps\n",
      "Found 19 original pIPS right maps\n",
      "Missing: sub-084_pIPS_clean_left_loc_fc_mni.nii.gz for sub-084\n",
      "Found 18 cleaned pIPS_clean left maps\n",
      "Missing: sub-084_pIPS_clean_right_loc_fc_mni.nii.gz for sub-084\n",
      "Found 18 cleaned pIPS_clean right maps\n",
      "Found 19 original LO left maps\n",
      "Found 19 original LO right maps\n",
      "\n",
      "Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Loading all maps...\")\n",
    "\n",
    "# Load original pIPS maps\n",
    "pips_orig_left, subs_pips_orig_left = load_fc_maps(subjects, 'original', 'pIPS', 'left')\n",
    "pips_orig_right, subs_pips_orig_right = load_fc_maps(subjects, 'original', 'pIPS', 'right')\n",
    "\n",
    "# Load cleaned pIPS maps  \n",
    "pips_clean_left, subs_pips_clean_left = load_fc_maps(subjects, 'cleaned', 'pIPS', 'left')\n",
    "pips_clean_right, subs_pips_clean_right = load_fc_maps(subjects, 'cleaned', 'pIPS', 'right')\n",
    "\n",
    "# Load LO maps for comparison\n",
    "lo_orig_left, subs_lo_left = load_fc_maps(subjects, 'original', 'LO', 'left')\n",
    "lo_orig_right, subs_lo_right = load_fc_maps(subjects, 'original', 'LO', 'right')\n",
    "\n",
    "print(\"\\nData loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Group Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing group statistics...\n",
      "Group statistics computed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing group statistics...\")\n",
    "\n",
    "# Get MNI brain mask\n",
    "mni_mask = datasets.load_mni152_brain_mask()\n",
    "\n",
    "# Original pIPS\n",
    "pips_orig_left_mean, pips_orig_left_t = compute_group_stats(pips_orig_left, mni_mask)\n",
    "pips_orig_right_mean, pips_orig_right_t = compute_group_stats(pips_orig_right, mni_mask)\n",
    "\n",
    "# Cleaned pIPS\n",
    "pips_clean_left_mean, pips_clean_left_t = compute_group_stats(pips_clean_left, mni_mask)\n",
    "pips_clean_right_mean, pips_clean_right_t = compute_group_stats(pips_clean_right, mni_mask)\n",
    "\n",
    "# LO for comparison\n",
    "lo_left_mean, lo_left_t = compute_group_stats(lo_orig_left, mni_mask)\n",
    "lo_right_mean, lo_right_t = compute_group_stats(lo_orig_right, mni_mask)\n",
    "\n",
    "print(\"Group statistics computed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Difference Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing difference maps...\")\n",
    "\n",
    "# Difference: Original - Cleaned (what was removed)\n",
    "diff_left = None\n",
    "diff_right = None\n",
    "\n",
    "if pips_orig_left_mean and pips_clean_left_mean:\n",
    "    diff_left = image.math_img('img1 - img2', \n",
    "                               img1=pips_orig_left_mean, \n",
    "                               img2=pips_clean_left_mean)\n",
    "    print(\"Left hemisphere difference map computed\")\n",
    "\n",
    "if pips_orig_right_mean and pips_clean_right_mean:\n",
    "    diff_right = image.math_img('img1 - img2', \n",
    "                                img1=pips_orig_right_mean, \n",
    "                                img2=pips_clean_right_mean)\n",
    "    print(\"Right hemisphere difference map computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(20, 24))\n",
    "\n",
    "# Left Hemisphere Analysis\n",
    "if pips_orig_left_mean and pips_clean_left_mean:\n",
    "    # Original pIPS Left\n",
    "    ax1 = plt.subplot(6, 2, 1)\n",
    "    plotting.plot_stat_map(pips_orig_left_mean, \n",
    "                          title='Original pIPS Left FC',\n",
    "                          threshold=0.1,\n",
    "                          axes=ax1,\n",
    "                          colorbar=True)\n",
    "    \n",
    "    # Cleaned pIPS Left  \n",
    "    ax2 = plt.subplot(6, 2, 2)\n",
    "    plotting.plot_stat_map(pips_clean_left_mean,\n",
    "                          title='Cleaned pIPS Left FC (Ventral Removed)', \n",
    "                          threshold=0.1,\n",
    "                          axes=ax2,\n",
    "                          colorbar=True)\n",
    "    \n",
    "    # Difference Left\n",
    "    if diff_left:\n",
    "        ax3 = plt.subplot(6, 2, 3)\n",
    "        plotting.plot_stat_map(diff_left,\n",
    "                              title='What Was Removed (Original - Cleaned)',\n",
    "                              threshold=0.05,\n",
    "                              axes=ax3,\n",
    "                              colorbar=True,\n",
    "                              cmap='RdBu_r')\n",
    "\n",
    "# Right Hemisphere Analysis  \n",
    "if pips_orig_right_mean and pips_clean_right_mean:\n",
    "    # Original pIPS Right\n",
    "    ax4 = plt.subplot(6, 2, 4)\n",
    "    plotting.plot_stat_map(pips_orig_right_mean,\n",
    "                          title='Original pIPS Right FC',\n",
    "                          threshold=0.1,\n",
    "                          axes=ax4,\n",
    "                          colorbar=True)\n",
    "    \n",
    "    # Cleaned pIPS Right\n",
    "    ax5 = plt.subplot(6, 2, 5)  \n",
    "    plotting.plot_stat_map(pips_clean_right_mean,\n",
    "                          title='Cleaned pIPS Right FC (Ventral Removed)',\n",
    "                          threshold=0.1,\n",
    "                          axes=ax5,\n",
    "                          colorbar=True)\n",
    "    \n",
    "    # Difference Right\n",
    "    if diff_right:\n",
    "        ax6 = plt.subplot(6, 2, 6)\n",
    "        plotting.plot_stat_map(diff_right,\n",
    "                              title='What Was Removed (Original - Cleaned)', \n",
    "                              threshold=0.05,\n",
    "                              axes=ax6,\n",
    "                              colorbar=True,\n",
    "                              cmap='RdBu_r')\n",
    "\n",
    "# LO Reference Maps\n",
    "if lo_left_mean:\n",
    "    ax7 = plt.subplot(6, 2, 7)\n",
    "    plotting.plot_stat_map(lo_left_mean,\n",
    "                          title='LO Left FC (Reference)',\n",
    "                          threshold=0.1,\n",
    "                          axes=ax7,\n",
    "                          colorbar=True)\n",
    "\n",
    "if lo_right_mean:\n",
    "    ax8 = plt.subplot(6, 2, 8)\n",
    "    plotting.plot_stat_map(lo_right_mean,\n",
    "                          title='LO Right FC (Reference)',\n",
    "                          threshold=0.1,\n",
    "                          axes=ax8,\n",
    "                          colorbar=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/partial_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Visualization saved to: {results_dir}/partial_correlation_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUANTITATIVE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compute overlap metrics\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "hemispheres = ['left', 'right']\n",
    "for hemi in hemispheres:\n",
    "    if hemi == 'left':\n",
    "        orig_mean = pips_orig_left_mean\n",
    "        clean_mean = pips_clean_left_mean  \n",
    "        lo_mean = lo_left_mean\n",
    "        orig_t = pips_orig_left_t\n",
    "        clean_t = pips_clean_left_t\n",
    "    else:\n",
    "        orig_mean = pips_orig_right_mean\n",
    "        clean_mean = pips_clean_right_mean\n",
    "        lo_mean = lo_right_mean\n",
    "        orig_t = pips_orig_right_t\n",
    "        clean_t = pips_clean_right_t\n",
    "    \n",
    "    if orig_mean and clean_mean and lo_mean:\n",
    "        # Dice coefficients\n",
    "        dice_orig_lo = compute_dice_coefficient(orig_mean, lo_mean)\n",
    "        dice_clean_lo = compute_dice_coefficient(clean_mean, lo_mean)\n",
    "        dice_orig_clean = compute_dice_coefficient(orig_mean, clean_mean)\n",
    "        \n",
    "        # Count significant voxels (if t-stats available)\n",
    "        if orig_t and clean_t:\n",
    "            orig_sig_voxels = np.sum(np.abs(orig_t.get_fdata()) > 2.3)\n",
    "            clean_sig_voxels = np.sum(np.abs(clean_t.get_fdata()) > 2.3)\n",
    "            percent_retained = (clean_sig_voxels / orig_sig_voxels) * 100 if orig_sig_voxels > 0 else 0\n",
    "        else:\n",
    "            orig_sig_voxels = clean_sig_voxels = percent_retained = np.nan\n",
    "        \n",
    "        # Store results\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            'hemisphere': [hemi],\n",
    "            'dice_original_LO': [dice_orig_lo],\n",
    "            'dice_cleaned_LO': [dice_clean_lo], \n",
    "            'dice_original_cleaned': [dice_orig_clean],\n",
    "            'orig_sig_voxels': [orig_sig_voxels],\n",
    "            'clean_sig_voxels': [clean_sig_voxels],\n",
    "            'percent_connectivity_retained': [percent_retained]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nOVERLAP ANALYSIS:\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quantitative results\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "results_df.to_csv(f'{results_dir}/partial_correlation_metrics.csv', index=False)\n",
    "print(f\"ðŸ“Š Results saved to: {results_dir}/partial_correlation_metrics.csv\")\n",
    "print(f\"ðŸ–¼ï¸  Visualization saved to: {results_dir}/partial_correlation_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Questions to Ask:\n",
    "\n",
    "**1. CONNECTIVITY RETENTION:**\n",
    "- **>70% retained** â†’ Strong evidence for dorsal independence\n",
    "- **30-70% retained** â†’ Moderate independence with some ventral dependence  \n",
    "- **<30% retained** â†’ Suggests significant ventral dependence\n",
    "\n",
    "**2. OVERLAP REDUCTION:**\n",
    "- **Large reduction** in pIPS-LO overlap â†’ Successful removal of shared variance\n",
    "- **Small reduction** â†’ Limited shared variance to begin with\n",
    "\n",
    "**3. SPATIAL PATTERNS:**\n",
    "- Check difference maps: What regions show largest changes?\n",
    "- Are remaining connections in expected dorsal areas?\n",
    "- Do cleaned maps still show object-selective patterns?\n",
    "\n",
    "### Next Steps:\n",
    "- Compare with your existing PPI results\n",
    "- Consider running group-level statistics on cleaned maps\n",
    "- Examine specific ROI-to-ROI connectivity changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Review the visualizations and metrics above to determine dorsal pathway independence.\")\n",
    "print(\"\\nKey files generated:\")\n",
    "print(f\"- Visualization: {results_dir}/partial_correlation_analysis.png\")\n",
    "print(f\"- Metrics: {results_dir}/partial_correlation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this right before computing dice coefficients to see what's in your data:\n",
    "print(f\"\\n=== DEBUGGING {hemi} HEMISPHERE ===\")\n",
    "print(f\"Original pIPS range: {orig_mean.get_fdata().min():.4f} to {orig_mean.get_fdata().max():.4f}\")\n",
    "print(f\"LO range: {lo_mean.get_fdata().min():.4f} to {lo_mean.get_fdata().max():.4f}\")  \n",
    "print(f\"Cleaned pIPS range: {clean_mean.get_fdata().min():.4f} to {clean_mean.get_fdata().max():.4f}\")\n",
    "\n",
    "# Count positive voxels\n",
    "orig_pos = np.sum(orig_mean.get_fdata() > 0)\n",
    "lo_pos = np.sum(lo_mean.get_fdata() > 0)\n",
    "clean_pos = np.sum(clean_mean.get_fdata() > 0)\n",
    "\n",
    "print(f\"Positive voxels - Original pIPS: {orig_pos}, LO: {lo_pos}, Cleaned pIPS: {clean_pos}\")\n",
    "print(f\"Total voxels in each map: {orig_mean.get_fdata().size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test actual spatial overlap\n",
    "orig_binary = (orig_mean.get_fdata() > 0).astype(int)\n",
    "lo_binary = (lo_mean.get_fdata() > 0).astype(int)\n",
    "clean_binary = (clean_mean.get_fdata() > 0).astype(int)\n",
    "\n",
    "# Count overlaps\n",
    "overlap_orig_lo = np.sum(orig_binary * lo_binary)\n",
    "overlap_clean_lo = np.sum(clean_binary * lo_binary)\n",
    "overlap_orig_clean = np.sum(orig_binary * clean_binary)\n",
    "\n",
    "print(f\"Actual overlapping voxels:\")\n",
    "print(f\"  Original pIPS âˆ© LO: {overlap_orig_lo}\")\n",
    "print(f\"  Cleaned pIPS âˆ© LO: {overlap_clean_lo}\")\n",
    "print(f\"  Original âˆ© Cleaned pIPS: {overlap_orig_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual dice calculation for debugging\n",
    "orig_pos = np.sum(orig_mean.get_fdata() > 0)  # 281,768\n",
    "lo_pos = np.sum(lo_mean.get_fdata() > 0)      # 274,747\n",
    "intersection = np.sum((orig_mean.get_fdata() > 0) * (lo_mean.get_fdata() > 0))  # 271,216\n",
    "\n",
    "manual_dice = 2.0 * intersection / (orig_pos + lo_pos)\n",
    "print(f\"Manual dice calculation: {manual_dice:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity checks\n",
    "# Load one subject's data manually\n",
    "sub = 'sub-025'  # or any subject\n",
    "roi_coords = pd.read_csv(f'{study_dir}/{sub}/ses-01/derivatives/rois/spheres/sphere_coords_hemisphere.csv')\n",
    "\n",
    "\n",
    "# Extract the same timeseries your script used\n",
    "pips_coords = roi_coords[(roi_coords['index'] == 0) & (roi_coords['roi'] == 'pIPS') & (roi_coords['hemisphere'] == 'left')]\n",
    "lo_coords = roi_coords[(roi_coords['index'] == 0) & (roi_coords['roi'] == 'LO') & (roi_coords['hemisphere'] == 'left')]\n",
    "\n",
    "# Check: Do the coordinates look reasonable?\n",
    "print(f\"pIPS coords: {pips_coords[['x', 'y', 'z']].values}\")\n",
    "print(f\"LO coords: {lo_coords[['x', 'y', 'z']].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cleaning had reasonable effect on group statistics\n",
    "print(\"GROUP STATISTICS COMPARISON:\")\n",
    "print(f\"Original pIPS t-stats range: {pips_orig_left_t.get_fdata().min():.2f} to {pips_orig_left_t.get_fdata().max():.2f}\")\n",
    "print(f\"Cleaned pIPS t-stats range: {pips_clean_left_t.get_fdata().min():.2f} to {pips_clean_left_t.get_fdata().max():.2f}\")\n",
    "\n",
    "# Check mean connectivity strength\n",
    "print(f\"\\nOriginal pIPS mean connectivity: {pips_orig_left_mean.get_fdata().mean():.4f}\")\n",
    "print(f\"Cleaned pIPS mean connectivity: {pips_clean_left_mean.get_fdata().mean():.4f}\")\n",
    "print(f\"LO mean connectivity: {lo_left_mean.get_fdata().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the spatial patterns look neurobiologically reasonable\n",
    "from nilearn import plotting\n",
    "\n",
    "# Quick visualization check\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "plotting.plot_stat_map(pips_orig_left_mean, axes=axes[0], title='Original pIPS', \n",
    "                      threshold=0.1, colorbar=False)\n",
    "plotting.plot_stat_map(pips_clean_left_mean, axes=axes[1], title='Cleaned pIPS', \n",
    "                      threshold=0.1, colorbar=False)  \n",
    "plotting.plot_stat_map(lo_left_mean, axes=axes[2], title='LO Reference', \n",
    "                      threshold=0.1, colorbar=False)\n",
    "plt.show()\n",
    "\n",
    "# Do the patterns look reasonable? Cleaned should be similar to original but somewhat reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the dice coefficient function\n",
    "def test_dice_debug():\n",
    "    # Test with your actual data\n",
    "    orig_data = pips_orig_left_mean.get_fdata()\n",
    "    lo_data = lo_left_mean.get_fdata()\n",
    "    \n",
    "    print(f\"Data shapes: orig={orig_data.shape}, lo={lo_data.shape}\")\n",
    "    \n",
    "    # Binary threshold\n",
    "    orig_bin = (orig_data > 0).astype(int)\n",
    "    lo_bin = (lo_data > 0).astype(int)\n",
    "    \n",
    "    intersection = np.sum(orig_bin * lo_bin)\n",
    "    total = np.sum(orig_bin) + np.sum(lo_bin)\n",
    "    dice = 2.0 * intersection / total\n",
    "    \n",
    "    print(f\"Manual calculation: {dice:.4f}\")\n",
    "    \n",
    "    # Now test your function\n",
    "    dice_func = compute_dice_coefficient(pips_orig_left_mean, lo_left_mean, threshold=0)\n",
    "    print(f\"Function result: {dice_func:.4f}\")\n",
    "    \n",
    "    return dice\n",
    "\n",
    "test_dice_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D slice visualization - much clearer\n",
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Left hemisphere comparison\n",
    "plotting.plot_stat_map(pips_orig_left_mean, \n",
    "                      axes=axes[0,0],\n",
    "                      title='Original pIPS Left\\n(77.5% retained)',\n",
    "                      threshold=0.1, \n",
    "                      colorbar=True,\n",
    "                      cut_coords=[44, -60, 27])  # pIPS coordinates\n",
    "\n",
    "plotting.plot_stat_map(pips_clean_left_mean, \n",
    "                      axes=axes[0,1],\n",
    "                      title='Cleaned pIPS Left\\n(Ventral removed)',\n",
    "                      threshold=0.1, \n",
    "                      colorbar=True,\n",
    "                      cut_coords=[44, -60, 27])\n",
    "\n",
    "# Right hemisphere comparison  \n",
    "plotting.plot_stat_map(pips_orig_right_mean, \n",
    "                      axes=axes[1,0],\n",
    "                      title='Original pIPS Right\\n(84.4% retained)',\n",
    "                      threshold=0.1, \n",
    "                      colorbar=True,\n",
    "                      cut_coords=[-44, -60, 27])\n",
    "\n",
    "plotting.plot_stat_map(pips_clean_right_mean, \n",
    "                      axes=axes[1,1],\n",
    "                      title='Cleaned pIPS Right\\n(Ventral removed)',\n",
    "                      threshold=0.1, \n",
    "                      colorbar=True,\n",
    "                      cut_coords=[-44, -60, 27])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualization - copy this into a notebook cell\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "\n",
    "# Load results\n",
    "results_dir = \"/user_data/csimmon2/git_repos/ptoc/results\"\n",
    "thresh_dir = f\"{results_dir}/partial_correlation_thresh\"\n",
    "results_df = pd.read_csv(f'{thresh_dir}/retention_analysis_results.csv')\n",
    "\n",
    "print(\"RETENTION ANALYSIS RESULTS:\")\n",
    "print(results_df.round(1))\n",
    "\n",
    "# Load images and create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for i, hemi in enumerate(['left', 'right']):\n",
    "    # Load thresholded images\n",
    "    orig_img = nib.load(f'{thresh_dir}/pIPS_{hemi}_original_thresh.nii.gz')\n",
    "    clean_img = nib.load(f'{thresh_dir}/pIPS_{hemi}_cleaned_thresh.nii.gz')\n",
    "    \n",
    "    # Get retention value\n",
    "    retention = results_df[results_df['hemisphere']==hemi]['percent_retained'].iloc[0]\n",
    "    \n",
    "    # Plot original\n",
    "    plotting.plot_stat_map(orig_img, axes=axes[i,0], \n",
    "                          title=f'Original pIPS {hemi.title()}',\n",
    "                          threshold=0.01, colorbar=True)\n",
    "    \n",
    "    # Plot cleaned\n",
    "    plotting.plot_stat_map(clean_img, axes=axes[i,1],\n",
    "                          title=f'Independent pIPS {hemi.title()}\\n({retention:.1f}% retained)',\n",
    "                          threshold=0.01, colorbar=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Partial Correlation Analysis: 91% Mean Retention\\nStrong Evidence for Dorsal Independence', \n",
    "             y=0.98, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "mean_retention = results_df['percent_retained'].mean()\n",
    "print(f\"\\nMean retention: {mean_retention:.1f}%\")\n",
    "print(\"Interpretation: STRONG evidence for dorsal independence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed visualization with consistent coordinates\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "\n",
    "# Load results\n",
    "results_dir = \"/user_data/csimmon2/git_repos/ptoc/results\"\n",
    "thresh_dir = f\"{results_dir}/partial_correlation_thresh\"\n",
    "results_df = pd.read_csv(f'{thresh_dir}/retention_analysis_results.csv')\n",
    "\n",
    "print(\"RETENTION ANALYSIS RESULTS:\")\n",
    "print(results_df.round(1))\n",
    "\n",
    "# Load images\n",
    "imgs = {}\n",
    "for hemi in ['left', 'right']:\n",
    "    imgs[f'orig_{hemi}'] = nib.load(f'{thresh_dir}/pIPS_{hemi}_original_thresh.nii.gz')\n",
    "    imgs[f'clean_{hemi}'] = nib.load(f'{thresh_dir}/pIPS_{hemi}_cleaned_thresh.nii.gz')\n",
    "\n",
    "# Calculate global max for consistent scaling\n",
    "all_data = [img.get_fdata() for img in imgs.values()]\n",
    "global_max = max([np.max(data[data > 0]) for data in all_data if np.any(data > 0)])\n",
    "\n",
    "# FIXED: Use same coordinates for all images\n",
    "cut_coords = [-44, -60, 27]  # pIPS coordinates\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot with consistent coordinates\n",
    "plotting.plot_stat_map(imgs['orig_left'], axes=axes[0,0], \n",
    "                      cut_coords=cut_coords, threshold=0.01, vmax=global_max, colorbar=True)\n",
    "axes[0,0].set_title('Original pIPS Left', fontsize=12)\n",
    "\n",
    "plotting.plot_stat_map(imgs['clean_left'], axes=axes[0,1], \n",
    "                      cut_coords=cut_coords, threshold=0.01, vmax=global_max, colorbar=True)\n",
    "left_retention = results_df[results_df['hemisphere']=='left']['percent_retained'].iloc[0]\n",
    "axes[0,1].set_title(f'Independent pIPS Left\\n({left_retention:.1f}% retained)', fontsize=12)\n",
    "\n",
    "plotting.plot_stat_map(imgs['orig_right'], axes=axes[1,0], \n",
    "                      cut_coords=cut_coords, threshold=0.01, vmax=global_max, colorbar=True)\n",
    "axes[1,0].set_title('Original pIPS Right', fontsize=12)\n",
    "\n",
    "plotting.plot_stat_map(imgs['clean_right'], axes=axes[1,1], \n",
    "                      cut_coords=cut_coords, threshold=0.01, vmax=global_max, colorbar=True)\n",
    "right_retention = results_df[results_df['hemisphere']=='right']['percent_retained'].iloc[0]\n",
    "axes[1,1].set_title(f'Independent pIPS Right\\n({right_retention:.1f}% retained)', fontsize=12)\n",
    "\n",
    "# Add main title\n",
    "mean_retention = results_df['percent_retained'].mean()\n",
    "fig.suptitle(f'Partial Correlation Analysis: {mean_retention:.1f}% Mean Retention\\nStrong Evidence for Dorsal Independence', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{thresh_dir}/final_consistent_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAll images now show the same brain coordinates: {cut_coords}\")\n",
    "print(f\"Left: {left_retention:.1f}% retention\")\n",
    "print(f\"Right: {right_retention:.1f}% retention\") \n",
    "print(f\"Mean: {mean_retention:.1f}% retention\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
