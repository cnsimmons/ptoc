{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: create merged atlas function\n",
    "# Complete atlas creation cell with all necessary imports and path setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn import image, datasets, plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define paths (adjust these to match your existing paths)\n",
    "base_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "roi_dir = f'{base_dir}/roiParcels'\n",
    "results_dir = f'{base_dir}/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "def create_merged_atlas_improved():\n",
    "    \"\"\"\n",
    "    Create and validate a merged atlas where Wang ROIs replace overlapping regions in Schaefer atlas\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Step 1: Creating Merged Atlas (Improved Version) ===\")\n",
    "    \n",
    "    # Load Wang ROIs - pIPS and LO\n",
    "    roi_files = {\n",
    "        'pIPS': f'{roi_dir}/pIPS.nii.gz',\n",
    "        'LO': f'{roi_dir}/LO.nii.gz'\n",
    "    }\n",
    "    \n",
    "    rois = {}\n",
    "    for roi_name, roi_path in roi_files.items():\n",
    "        if os.path.exists(roi_path):\n",
    "            rois[roi_name] = nib.load(roi_path)\n",
    "            print(f\"Loaded {roi_name} ROI\")\n",
    "        else:\n",
    "            print(f\"Error: ROI file {roi_path} not found!\")\n",
    "            return None\n",
    "    \n",
    "    # Load Schaefer atlas\n",
    "    atlas = datasets.fetch_atlas_schaefer_2018(n_rois=200, yeo_networks=7, resolution_mm=2)\n",
    "    atlas_img = nib.load(atlas.maps)\n",
    "    atlas_labels = atlas.labels\n",
    "    print(f\"Loaded Schaefer atlas with {len(atlas_labels)} parcels\")\n",
    "    \n",
    "    # Validate spatial dimensions and resolution\n",
    "    vox_dims = {}\n",
    "    for roi_name, roi_img in rois.items():\n",
    "        vox_dims[roi_name] = roi_img.header.get_zooms()[:3]\n",
    "    atlas_dims = atlas_img.header.get_zooms()[:3]\n",
    "    \n",
    "    # Check for dimension mismatches\n",
    "    for roi_name, dims in vox_dims.items():\n",
    "        if not np.allclose(dims, atlas_dims, rtol=1e-3):\n",
    "            print(f\"WARNING: Resolution mismatch between {roi_name} ({dims}) and atlas ({atlas_dims})\")\n",
    "            print(f\"Resampling may be needed for proper integration\")\n",
    "    \n",
    "    # Get atlas data\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    modified_atlas_data = atlas_data.copy()\n",
    "    \n",
    "    # Create a dictionary to store new labels\n",
    "    new_labels = list(atlas_labels)\n",
    "    \n",
    "    # Assign values for new ROIs (continuing from the end of the Schaefer atlas)\n",
    "    roi_values = {'pIPS': 201, 'LO': 202}\n",
    "    overlap_info = {}\n",
    "    \n",
    "    # Set up a figure for quality control visualization\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(131)\n",
    "    plotting.plot_roi(atlas_img, title=\"Original Schaefer Atlas\", \n",
    "                     cmap='nipy_spectral', colorbar=True, figure=plt.gcf(), \n",
    "                     axes=plt.gca(), draw_cross=False)\n",
    "    \n",
    "    # Process each ROI\n",
    "    for roi_name, roi_img in rois.items():\n",
    "        # Get ROI data and create mask\n",
    "        roi_data = roi_img.get_fdata()\n",
    "        roi_mask = roi_data > 0\n",
    "        \n",
    "        # Validate ROI mask isn't empty\n",
    "        if np.sum(roi_mask) == 0:\n",
    "            print(f\"ERROR: {roi_name} ROI mask is empty! Please check the ROI file.\")\n",
    "            return None\n",
    "        \n",
    "        # Find overlapping parcels\n",
    "        overlap_mask = (atlas_data > 0) & roi_mask\n",
    "        overlapping_labels = np.unique(atlas_data[overlap_mask])\n",
    "        overlapping_labels = overlapping_labels[overlapping_labels > 0]\n",
    "        \n",
    "        # Compute total ROI size and overlap percentage\n",
    "        roi_size = np.sum(roi_mask)\n",
    "        overlap_size = np.sum(overlap_mask)\n",
    "        overlap_percentage = (overlap_size / roi_size) * 100\n",
    "        \n",
    "        print(f\"\\n{roi_name} ROI:\")\n",
    "        print(f\"  Total size: {roi_size} voxels\")\n",
    "        print(f\"  Overlap with atlas: {overlap_size} voxels ({overlap_percentage:.2f}%)\")\n",
    "        \n",
    "        # Get number of voxels in overlap\n",
    "        overlap_voxels = {}\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (atlas_data == label) & roi_mask\n",
    "            overlap_voxels[int(label)] = np.sum(label_mask)\n",
    "        \n",
    "        # Store overlap information\n",
    "        overlap_info[roi_name] = {\n",
    "            'overlapping_labels': overlapping_labels.tolist(),\n",
    "            'overlap_voxels': overlap_voxels,\n",
    "            'total_size': int(roi_size),\n",
    "            'overlap_size': int(overlap_size),\n",
    "            'overlap_percentage': float(overlap_percentage)\n",
    "        }\n",
    "        \n",
    "        # Print overlap details\n",
    "        print(f\"  {roi_name} overlaps with {len(overlapping_labels)} atlas parcels:\")\n",
    "        for label, voxels in overlap_voxels.items():\n",
    "            label_idx = int(label) - 1  # Convert to 0-indexed\n",
    "            if 0 <= label_idx < len(atlas_labels):\n",
    "                label_name = atlas_labels[label_idx]\n",
    "                label_name = label_name.decode('utf-8') if isinstance(label_name, bytes) else str(label_name)\n",
    "                print(f\"    Label {label} ({label_name}): {voxels} voxels\")\n",
    "        \n",
    "        # Remove overlapping parcels from the atlas\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (modified_atlas_data == label) & roi_mask\n",
    "            modified_atlas_data[label_mask] = 0\n",
    "        \n",
    "        # Add ROI with new label\n",
    "        modified_atlas_data[roi_mask] = roi_values[roi_name]\n",
    "        \n",
    "        # Add new label name\n",
    "        new_labels.append(f\"Wang_{roi_name}\")\n",
    "    \n",
    "    # Create the modified atlas\n",
    "    modified_atlas_img = nib.Nifti1Image(modified_atlas_data, atlas_img.affine, atlas_img.header)\n",
    "    merged_atlas_file = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    nib.save(modified_atlas_img, merged_atlas_file)\n",
    "    print(f\"\\nSaved merged atlas to: {merged_atlas_file}\")\n",
    "    \n",
    "    # Plot modified atlas for quality control\n",
    "    plt.subplot(132)\n",
    "    plotting.plot_roi(modified_atlas_img, title=\"Merged Schaefer-Wang Atlas\", \n",
    "                     cmap='nipy_spectral', colorbar=True, figure=plt.gcf(), \n",
    "                     axes=plt.gca(), draw_cross=False)\n",
    "    \n",
    "    # Create difference map to highlight changes\n",
    "    diff_data = (modified_atlas_data != atlas_data).astype(int)\n",
    "    diff_img = nib.Nifti1Image(diff_data, atlas_img.affine, atlas_img.header)\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plotting.plot_roi(diff_img, title=\"Changed Regions\", \n",
    "                     cmap='hot', colorbar=True, figure=plt.gcf(), \n",
    "                     axes=plt.gca(), draw_cross=False)\n",
    "    \n",
    "    # Save quality control figure\n",
    "    qc_file = f'{results_dir}/atlas_merge_quality_control.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(qc_file, dpi=150)\n",
    "    print(f\"Saved quality control visualization to: {qc_file}\")\n",
    "    \n",
    "    # Validate the merged atlas\n",
    "    # Check for empty parcels or disconnected regions\n",
    "    original_parcel_counts = {}\n",
    "    merged_parcel_counts = {}\n",
    "    \n",
    "    for i in range(1, 201):  # Original Schaefer parcels\n",
    "        original_parcel_counts[i] = np.sum(atlas_data == i)\n",
    "        merged_parcel_counts[i] = np.sum(modified_atlas_data == i)\n",
    "    \n",
    "    # Check for and report parcels that were completely removed\n",
    "    completely_removed = []\n",
    "    for parcel_id, original_count in original_parcel_counts.items():\n",
    "        if original_count > 0 and merged_parcel_counts[parcel_id] == 0:\n",
    "            completely_removed.append(parcel_id)\n",
    "    \n",
    "    if completely_removed:\n",
    "        print(f\"\\nWARNING: {len(completely_removed)} parcels were completely removed from the atlas:\")\n",
    "        for parcel_id in completely_removed:\n",
    "            label_idx = parcel_id - 1\n",
    "            if 0 <= label_idx < len(atlas_labels):\n",
    "                label_name = atlas_labels[label_idx]\n",
    "                label_name = label_name.decode('utf-8') if isinstance(label_name, bytes) else str(label_name)\n",
    "                print(f\"  Parcel {parcel_id} ({label_name})\")\n",
    "    \n",
    "    # Check Wang ROIs in merged atlas\n",
    "    for roi_name, roi_value in roi_values.items():\n",
    "        roi_count = np.sum(modified_atlas_data == roi_value)\n",
    "        if roi_count == 0:\n",
    "            print(f\"ERROR: {roi_name} ROI is missing from the merged atlas!\")\n",
    "        else:\n",
    "            print(f\"\\n{roi_name} ROI in merged atlas: {roi_count} voxels\")\n",
    "    \n",
    "    # Save atlas statistics\n",
    "    atlas_stats = {\n",
    "        'original_roi_counts': original_parcel_counts,\n",
    "        'merged_roi_counts': merged_parcel_counts,\n",
    "        'roi_overlaps': overlap_info,\n",
    "        'completely_removed_parcels': completely_removed\n",
    "    }\n",
    "    \n",
    "    # Save new labels array\n",
    "    np.save(f'{results_dir}/merged_atlas_labels.npy', new_labels)\n",
    "    print(f\"Saved {len(new_labels)} merged atlas labels\")\n",
    "    \n",
    "    # Save atlas statistics as JSON for reference\n",
    "    # Convert numpy types to Python native types for JSON serialization\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    with open(f'{results_dir}/atlas_merge_statistics.json', 'w') as f:\n",
    "        json_data = json.dumps(atlas_stats, default=convert_to_serializable)\n",
    "        f.write(json_data)\n",
    "    \n",
    "    print(f\"Saved atlas merge statistics to: {results_dir}/atlas_merge_statistics.json\")\n",
    "    \n",
    "    # Return atlas information\n",
    "    return {\n",
    "        'atlas_img': atlas_img,\n",
    "        'merged_atlas_img': modified_atlas_img,\n",
    "        'atlas_labels': atlas_labels,\n",
    "        'merged_labels': new_labels,\n",
    "        'roi_values': roi_values,\n",
    "        'overlap_info': overlap_info,\n",
    "        'atlas_stats': atlas_stats\n",
    "    }\n",
    "\n",
    "# Execute the function when the cell runs\n",
    "atlas_info = create_merged_atlas_improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: create load merged atlas function\n",
    "def load_merged_atlas():\n",
    "    \"\"\"\n",
    "    Load the merged Schaefer-Wang atlas with improved error handling\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    atlas_img : nibabel.nifti1.Nifti1Image\n",
    "        The loaded atlas image\n",
    "    atlas_labels : list\n",
    "        Labels for the atlas ROIs\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading Merged Atlas ===\")\n",
    "    \n",
    "    # Define paths\n",
    "    atlas_path = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    labels_path = f'{results_dir}/merged_atlas_labels.npy'\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(atlas_path):\n",
    "        print(f\"ERROR: Atlas file not found at {atlas_path}\")\n",
    "        print(\"Please run the create_merged_atlas_improved() function first\")\n",
    "        return None, None\n",
    "    \n",
    "    if not os.path.exists(labels_path):\n",
    "        print(f\"ERROR: Labels file not found at {labels_path}\")\n",
    "        print(\"Please run the create_merged_atlas_improved() function first\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load the files\n",
    "    try:\n",
    "        atlas_img = nib.load(atlas_path)\n",
    "        labels = np.load(labels_path, allow_pickle=True)\n",
    "        \n",
    "        # Validate the atlas\n",
    "        atlas_data = atlas_img.get_fdata()\n",
    "        unique_values = np.unique(atlas_data)\n",
    "        unique_values = unique_values[unique_values > 0]  # Remove background\n",
    "        \n",
    "        print(f\"Loaded merged atlas with {len(labels)} regions\")\n",
    "        print(f\"Atlas contains {len(unique_values)} unique ROI values\")\n",
    "        \n",
    "        # Verify Wang ROIs are present\n",
    "        wang_values = [201, 202]  # pIPS and LO\n",
    "        wang_present = all(v in unique_values for v in wang_values)\n",
    "        \n",
    "        if not wang_present:\n",
    "            print(\"WARNING: Not all Wang ROIs (pIPS=201, LO=202) found in the atlas\")\n",
    "            missing = [v for v in wang_values if v not in unique_values]\n",
    "            print(f\"Missing ROIs: {missing}\")\n",
    "        \n",
    "        return atlas_img, labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading atlas: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: load thresholded GCA images\n",
    "def load_thresholded_images():\n",
    "    \"\"\"Load previously thresholded GCA images for experiment 1\"\"\"\n",
    "    thresholded_imgs = {}\n",
    "    \n",
    "    # Define the directory containing the thresholded GCA images\n",
    "    gca_dir = f'{results_dir}/group_averages/gca_group_averages'\n",
    "    \n",
    "    # Define ROIs and hemispheres\n",
    "    rois = ['pIPS', 'LO']\n",
    "    hemispheres = ['left', 'right']\n",
    "    \n",
    "    # Load each file\n",
    "    for roi in rois:\n",
    "        for hemi in hemispheres:\n",
    "            img_path = f'{gca_dir}/{roi}_{hemi}_gca_thresh.nii.gz'\n",
    "            if os.path.exists(img_path):\n",
    "                thresholded_imgs[f\"{roi}_{hemi}\"] = nib.load(img_path)\n",
    "            else:\n",
    "                print(f\"Warning: {img_path} not found\")\n",
    "    \n",
    "    return thresholded_imgs\n",
    "\n",
    "thresholded_imgs = load_thresholded_images()\n",
    "print(f\"Loaded {len(thresholded_imgs)} thresholded GCA images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: create extract atlas data function\n",
    "def extract_roi_values_improved(thresholded_imgs, atlas_img, atlas_labels):\n",
    "    \"\"\"\n",
    "    Extract values from thresholded GCA images for each ROI in the merged atlas\n",
    "    with improved validation and error handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    thresholded_imgs : dict\n",
    "        Dictionary of thresholded GCA images {img_key: img}\n",
    "    atlas_img : nibabel.nifti1.Nifti1Image\n",
    "        The merged atlas image\n",
    "    atlas_labels : list\n",
    "        Labels for the atlas ROIs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    roi_values : dict\n",
    "        Dictionary of ROI values {img_key: {roi_id: values}}\n",
    "    roi_statistics : dict\n",
    "        Dictionary of ROI statistics for quality control\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Extracting ROI Values with Improved Validation ===\")\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not isinstance(thresholded_imgs, dict) or len(thresholded_imgs) == 0:\n",
    "        print(\"ERROR: No valid thresholded images provided\")\n",
    "        return None, None\n",
    "    \n",
    "    # Function to validate image alignment\n",
    "    def validate_image_alignment(img1, img2):\n",
    "        \"\"\"Validate that two images have compatible dimensions and alignment\"\"\"\n",
    "        # Check shape match\n",
    "        if img1.shape != img2.shape:\n",
    "            print(f\"WARNING: Images have different shapes: {img1.shape} vs {img2.shape}\")\n",
    "            return False\n",
    "        \n",
    "        # Check affine match (allowing for small numerical differences)\n",
    "        if not np.allclose(img1.affine, img2.affine, rtol=1e-5, atol=1e-5):\n",
    "            print(\"WARNING: Images have different affine transformations\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    # Get atlas data\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    \n",
    "    # Get unique ROI values in atlas\n",
    "    unique_rois = np.unique(atlas_data)\n",
    "    unique_rois = unique_rois[unique_rois > 0]  # Remove background\n",
    "    \n",
    "    print(f\"Atlas contains {len(unique_rois)} unique ROI values\")\n",
    "    \n",
    "    # Create dictionaries to store results and statistics\n",
    "    roi_values = {}\n",
    "    roi_statistics = {'atlas_info': {}, 'image_compatibility': {}, 'extraction_stats': {}}\n",
    "    \n",
    "    # Store atlas info for reference\n",
    "    roi_statistics['atlas_info'] = {\n",
    "        'num_unique_rois': int(len(unique_rois)),\n",
    "        'roi_values': [int(v) for v in unique_rois],\n",
    "        'dimensions': atlas_img.shape,\n",
    "        'voxel_size': atlas_img.header.get_zooms()[:3]\n",
    "    }\n",
    "    \n",
    "    # Define ROI name lookup function (handles special ROIs and regular parcels)\n",
    "    def get_roi_name(roi_value):\n",
    "        \"\"\"Get name for a ROI value, handling special cases like Wang ROIs\"\"\"\n",
    "        if roi_value == 201:\n",
    "            return \"Wang_pIPS\"\n",
    "        elif roi_value == 202:\n",
    "            return \"Wang_LO\"\n",
    "        elif 1 <= roi_value <= 200:  # Schaefer ROIs\n",
    "            idx = int(roi_value) - 1\n",
    "            if idx < len(atlas_labels):\n",
    "                label = atlas_labels[idx]\n",
    "                return label.decode('utf-8') if isinstance(label, bytes) else str(label)\n",
    "        return f\"Unknown_ROI_{roi_value}\"\n",
    "    \n",
    "    # Process each thresholded image\n",
    "    for img_key, img in thresholded_imgs.items():\n",
    "        print(f\"Processing {img_key}...\")\n",
    "        \n",
    "        # Validate alignment\n",
    "        is_aligned = validate_image_alignment(img, atlas_img)\n",
    "        roi_statistics['image_compatibility'][img_key] = {\n",
    "            'aligned_with_atlas': is_aligned,\n",
    "            'dimensions': img.shape,\n",
    "            'voxel_size': img.header.get_zooms()[:3]\n",
    "        }\n",
    "        \n",
    "        if not is_aligned:\n",
    "            print(f\"WARNING: {img_key} may not be properly aligned with atlas\")\n",
    "            # Continue anyway but with caution flag\n",
    "        \n",
    "        img_data = img.get_fdata()\n",
    "        \n",
    "        # Initialize dictionaries for this image\n",
    "        roi_values[img_key] = {}\n",
    "        roi_statistics['extraction_stats'][img_key] = {}\n",
    "        \n",
    "        # Extract values for all ROIs using a unified approach\n",
    "        for roi_value in unique_rois:\n",
    "            # Create mask for this ROI\n",
    "            roi_mask = atlas_data == roi_value\n",
    "            mask_size = np.sum(roi_mask)\n",
    "            \n",
    "            # Skip if no voxels in mask\n",
    "            if mask_size == 0:\n",
    "                print(f\"  WARNING: ROI {roi_value} has no voxels in atlas\")\n",
    "                continue\n",
    "            \n",
    "            # Extract values within mask\n",
    "            values = img_data[roi_mask]\n",
    "            \n",
    "            # Remove invalid values (NaN, Inf)\n",
    "            valid_mask = ~np.isnan(values) & ~np.isinf(values)\n",
    "            valid_values = values[valid_mask]\n",
    "            \n",
    "            # Calculate statistics on valid values\n",
    "            num_valid = len(valid_values)\n",
    "            num_invalid = len(values) - num_valid\n",
    "            \n",
    "            # Store statistics\n",
    "            roi_statistics['extraction_stats'][img_key][int(roi_value)] = {\n",
    "                'roi_name': get_roi_name(roi_value),\n",
    "                'mask_size': int(mask_size),\n",
    "                'valid_values': int(num_valid),\n",
    "                'invalid_values': int(num_invalid),\n",
    "                'percent_valid': float(num_valid / mask_size * 100) if mask_size > 0 else 0\n",
    "            }\n",
    "            \n",
    "            # Only store if we have valid values\n",
    "            if num_valid > 0:\n",
    "                # Store using consistent indexing - all ROIs use their value as index\n",
    "                roi_values[img_key][int(roi_value)] = valid_values\n",
    "            else:\n",
    "                print(f\"  WARNING: ROI {roi_value} has no valid values in {img_key}\")\n",
    "        \n",
    "        # Summary for this image\n",
    "        num_rois_with_values = len(roi_values[img_key])\n",
    "        print(f\"  Extracted values for {num_rois_with_values}/{len(unique_rois)} ROIs\")\n",
    "        \n",
    "        # Identify ROIs with no valid values\n",
    "        missing_rois = [int(r) for r in unique_rois if int(r) not in roi_values[img_key]]\n",
    "        if missing_rois:\n",
    "            print(f\"  Missing values for {len(missing_rois)} ROIs: {missing_rois[:5]}...\")\n",
    "            if len(missing_rois) > 5:\n",
    "                print(f\"    (and {len(missing_rois)-5} more)\")\n",
    "    \n",
    "    # Create a mapping from ROI values to more convenient keys for GCA analysis\n",
    "    value_to_key_mapping = {}\n",
    "    \n",
    "    # Special handling for Wang ROIs\n",
    "    value_to_key_mapping[201] = 'pIPS'\n",
    "    value_to_key_mapping[202] = 'LO'\n",
    "    \n",
    "    # Regular Schaefer ROIs\n",
    "    for i in range(1, 201):\n",
    "        value_to_key_mapping[i] = i - 1  # Convert to 0-indexed\n",
    "    \n",
    "    # Create a more convenient version of roi_values using simplified keys\n",
    "    simplified_roi_values = {}\n",
    "    \n",
    "    for img_key, roi_dict in roi_values.items():\n",
    "        simplified_roi_values[img_key] = {}\n",
    "        \n",
    "        for roi_value, values in roi_dict.items():\n",
    "            # Get simplified key\n",
    "            if roi_value in value_to_key_mapping:\n",
    "                simple_key = value_to_key_mapping[roi_value]\n",
    "                simplified_roi_values[img_key][simple_key] = values\n",
    "            else:\n",
    "                # Keep original value if no mapping exists\n",
    "                simplified_roi_values[img_key][roi_value] = values\n",
    "    \n",
    "    # Save extraction statistics for reference\n",
    "    import json\n",
    "    \n",
    "    # Convert numpy types to Python native types for JSON serialization\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    with open(f'{results_dir}/roi_extraction_statistics.json', 'w') as f:\n",
    "        json_data = json.dumps(roi_statistics, default=convert_to_serializable)\n",
    "        f.write(json_data)\n",
    "    \n",
    "    print(f\"\\nSaved ROI extraction statistics to: {results_dir}/roi_extraction_statistics.json\")\n",
    "    print(f\"Extraction complete with {len(simplified_roi_values)} images processed\")\n",
    "    \n",
    "    return simplified_roi_values, roi_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load the merged atlas and GCA images\n",
    "atlas_img, atlas_labels = load_merged_atlas()\n",
    "\n",
    "if atlas_img is not None and atlas_labels is not None:\n",
    "    # Extract ROI values with improved validation\n",
    "    roi_values, roi_stats = extract_roi_values_improved(thresholded_imgs, atlas_img, atlas_labels)\n",
    "    \n",
    "    print(\"\\nROI values extracted successfully. Ready for visualization.\")\n",
    "else:\n",
    "    print(\"\\nERROR: Could not proceed with ROI extraction due to atlas loading failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: GCA bar plots with anatomical organization, LOOV, and bootstrapping\n",
    "def plot_gca_anatomical_organization_enhanced(roi_values, atlas_labels, output_dir, bootstrap_iterations=1000):\n",
    "    \"\"\"Create anatomical organization visualization for GCA data with LOOV and bootstrapping\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CREATING GCA ANATOMICAL ORGANIZATION VISUALIZATION - pIPS AND LO SEEDS\")\n",
    "    print(f\"Using LOOV and Bootstrapping ({bootstrap_iterations} iterations)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    bar_plot_dir = os.path.join(output_dir, 'gca_anatomical')\n",
    "    os.makedirs(bar_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Function to map ROI names to anatomical lobes\n",
    "    def map_to_anatomical_lobe(roi_name):\n",
    "        # Special handling for Wang atlas ROIs\n",
    "        if 'Wang_pIPS' in roi_name:\n",
    "            return 'Parietal'\n",
    "        elif 'Wang_LO' in roi_name:\n",
    "            return 'Temporal'\n",
    "        \n",
    "        # Map Schaefer networks to anatomical regions\n",
    "        if 'Vis' in roi_name:\n",
    "            return 'Occipital'\n",
    "        elif 'SomMot' in roi_name:\n",
    "            return 'Somatomotor'\n",
    "        elif 'DorsAttn' in roi_name:\n",
    "            if 'Par' in roi_name or 'IPL' in roi_name or 'IPS' in roi_name:\n",
    "                return 'Parietal'\n",
    "            elif 'Temp' in roi_name or 'MT' in roi_name:\n",
    "                return 'Temporal'\n",
    "            else:\n",
    "                return 'Parietal'\n",
    "        elif 'SalVentAttn' in roi_name:\n",
    "            if 'Ins' in roi_name:\n",
    "                return 'Insular'\n",
    "            elif 'Cing' in roi_name or 'ACC' in roi_name:\n",
    "                return 'Cingulate'\n",
    "            elif 'Temp' in roi_name:\n",
    "                return 'Temporal'\n",
    "            elif 'Par' in roi_name:\n",
    "                return 'Parietal'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        elif 'Limbic' in roi_name:\n",
    "            if 'Temp' in roi_name:\n",
    "                return 'Temporal'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        elif 'Cont' in roi_name:\n",
    "            if 'Par' in roi_name or 'IPL' in roi_name or 'IPS' in roi_name:\n",
    "                return 'Parietal'\n",
    "            elif 'Temp' in roi_name or 'MT' in roi_name:\n",
    "                return 'Temporal'\n",
    "            elif 'Cing' in roi_name:\n",
    "                return 'Cingulate'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        elif 'Default' in roi_name:\n",
    "            if 'Par' in roi_name:\n",
    "                return 'Parietal'\n",
    "            elif 'Temp' in roi_name:\n",
    "                return 'Temporal'\n",
    "            elif 'PCC' in roi_name or 'Cing' in roi_name:\n",
    "                return 'Cingulate'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Clean up ROI names for simpler labeling (just network and region)\n",
    "    def clean_roi_name(roi_name):\n",
    "        if isinstance(roi_name, bytes):\n",
    "            roi_name = roi_name.decode('utf-8')\n",
    "        \n",
    "        # Handle Wang ROIs\n",
    "        if 'Wang_' in roi_name:\n",
    "            return roi_name.replace('Wang_', '')\n",
    "        \n",
    "        # Remove common prefixes\n",
    "        cleaned = roi_name.replace('7Networks_', '')\n",
    "        \n",
    "        # Extract just the network and region part\n",
    "        parts = cleaned.split('_')\n",
    "        if len(parts) > 2:\n",
    "            # For format like LH_Network_Region_Number\n",
    "            network = parts[1]  # e.g., Vis, SomMot\n",
    "            region_parts = parts[2:]  # Everything after network\n",
    "            region = '_'.join(region_parts)\n",
    "            return f\"{network}_{region}\"\n",
    "        elif len(parts) > 1:\n",
    "            # For simpler format\n",
    "            return parts[-1]\n",
    "        else:\n",
    "            return cleaned\n",
    "    \n",
    "    # Determine hemisphere and base region for organizing pairs\n",
    "    def get_hemisphere_and_region(roi_name):\n",
    "        if isinstance(roi_name, bytes):\n",
    "            roi_name = roi_name.decode('utf-8')\n",
    "        \n",
    "        # Determine hemisphere \n",
    "        if 'LH' in roi_name:\n",
    "            hemisphere = 'L'\n",
    "        elif 'RH' in roi_name:\n",
    "            hemisphere = 'R'\n",
    "        else:\n",
    "            # For Wang ROIs or ROIs without clear hemisphere\n",
    "            if 'Wang_pIPS' in roi_name or 'Wang_LO' in roi_name:\n",
    "                hemisphere = 'X'  # Bilateral\n",
    "            else:\n",
    "                hemisphere = 'X'  # Unknown\n",
    "        \n",
    "        # Extract base region by removing hemisphere and number\n",
    "        cleaned = roi_name.replace('7Networks_', '')\n",
    "        cleaned = cleaned.replace('LH_', '').replace('RH_', '')\n",
    "        \n",
    "        # Handle Wang ROIs\n",
    "        if 'Wang_' in cleaned:\n",
    "            base_region = cleaned.replace('Wang_', '')\n",
    "        else:\n",
    "            # Remove trailing numbers which often differentiate regions\n",
    "            base_region = re.sub(r'_\\d+$', '', cleaned)\n",
    "        \n",
    "        return hemisphere, base_region\n",
    "    \n",
    "    # Improved vectorized bootstrap for better performance\n",
    "    def bootstrap_analysis(values, n_iterations=1000, confidence=0.95):\n",
    "        if len(values) < 2:\n",
    "            return np.nan, np.nan, np.nan\n",
    "        \n",
    "        # Create bootstrap samples matrix at once\n",
    "        n_values = len(values)\n",
    "        indices = np.random.randint(0, n_values, size=(n_iterations, n_values))\n",
    "        bootstrap_samples = values[indices]\n",
    "        bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        sorted_means = np.sort(bootstrap_means)\n",
    "        lower_idx = int((1 - confidence) / 2 * n_iterations)\n",
    "        upper_idx = int((1 - (1 - confidence) / 2) * n_iterations)\n",
    "        \n",
    "        return np.mean(values), sorted_means[lower_idx], sorted_means[upper_idx]\n",
    "    \n",
    "    # Perform Leave-One-Out Validation with better vectorization\n",
    "    def loov_analysis(values):\n",
    "        if len(values) < 2:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Vectorized approach for better performance\n",
    "        n = len(values)\n",
    "        total_sum = np.sum(values)\n",
    "        loo_means = (total_sum - values) / (n - 1)\n",
    "        \n",
    "        return np.mean(loo_means), np.std(loo_means)\n",
    "    \n",
    "    # Function to check for potential auto-correlation\n",
    "    def check_autocorrelation(seed_roi, target_roi, atlas_overlap_info=None):\n",
    "        \"\"\"\n",
    "        Check if a seed-target pair has autocorrelation issues\n",
    "        \n",
    "        Only excludes the exact seed ROI from its own results, ignoring atlas overlaps\n",
    "        \"\"\"\n",
    "        # Only exclude the target ROI if it's exactly the same as the seed\n",
    "        if str(seed_roi) == str(target_roi):\n",
    "            return True\n",
    "        \n",
    "        # Allow all other ROIs, including other seed ROIs\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # Filter and group data by seed roi\n",
    "    print(\"Filtering for pIPS and LO seed ROIs and averaging left and right hemispheres...\")\n",
    "    \n",
    "    # Group keys by seed\n",
    "    grouped_data = {}\n",
    "    seed_keys = []\n",
    "    \n",
    "    # First, find all keys for pIPS and LO\n",
    "    for img_key in roi_values.keys():\n",
    "        seed_roi, hemi = img_key.split('_')\n",
    "        \n",
    "        # Filter for pIPS and LO seeds\n",
    "        if 'pIPS' in seed_roi or 'LO' in seed_roi:\n",
    "            group_key = f\"{seed_roi}\"\n",
    "            if group_key not in grouped_data:\n",
    "                grouped_data[group_key] = {'left': None, 'right': None}\n",
    "            \n",
    "            grouped_data[group_key][hemi] = img_key\n",
    "            seed_keys.append(img_key)\n",
    "    \n",
    "    print(f\"Found {len(seed_keys)} seed keys: {seed_keys}\")\n",
    "    \n",
    "    # Create structure to store raw subject values for bootstrapping\n",
    "    subject_values = {}\n",
    "    \n",
    "    # Try to load atlas overlap info if available\n",
    "    atlas_overlap_info = None\n",
    "    atlas_stats_file = f'{results_dir}/atlas_merge_statistics.json'\n",
    "    if os.path.exists(atlas_stats_file):\n",
    "        try:\n",
    "            with open(atlas_stats_file, 'r') as f:\n",
    "                atlas_stats = json.load(f)\n",
    "                if 'roi_overlaps' in atlas_stats:\n",
    "                    atlas_overlap_info = atlas_stats['roi_overlaps']\n",
    "                    print(\"Loaded atlas overlap information for auto-correlation checks\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load atlas statistics: {e}\")\n",
    "    \n",
    "    # Now process each seed group, averaging left and right hemispheres\n",
    "    for group_key, hemi_keys in grouped_data.items():\n",
    "        seed_roi = group_key\n",
    "        \n",
    "        # Skip if we don't have both hemispheres\n",
    "        if hemi_keys['left'] is None or hemi_keys['right'] is None:\n",
    "            print(f\"Skipping {group_key} - missing one hemisphere\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {group_key} (averaging L and R hemispheres)...\")\n",
    "        \n",
    "        # Collect and average data for all ROIs across both hemispheres\n",
    "        combined_roi_data = {}\n",
    "        subject_values[group_key] = {}\n",
    "        \n",
    "        # Process left hemisphere\n",
    "        for roi_id in roi_values[hemi_keys['left']]:\n",
    "            # Check for auto-correlation\n",
    "            if check_autocorrelation(seed_roi, roi_id, atlas_overlap_info):\n",
    "                print(f\"  Skipping ROI {roi_id} - potential auto-correlation with seed {seed_roi}\")\n",
    "                continue\n",
    "                \n",
    "            values_L = roi_values[hemi_keys['left']][roi_id]\n",
    "            values_L = values_L[~np.isnan(values_L) & ~np.isinf(values_L)]\n",
    "            \n",
    "            if len(values_L) > 0:\n",
    "                combined_roi_data[roi_id] = {'L': values_L, 'R': None}\n",
    "                subject_values[group_key][roi_id] = []\n",
    "        \n",
    "        # Process right hemisphere\n",
    "        for roi_id in roi_values[hemi_keys['right']]:\n",
    "            # Check for auto-correlation\n",
    "            if check_autocorrelation(seed_roi, roi_id, atlas_overlap_info):\n",
    "                print(f\"  Skipping ROI {roi_id} - potential auto-correlation with seed {seed_roi}\")\n",
    "                continue\n",
    "                \n",
    "            values_R = roi_values[hemi_keys['right']][roi_id]\n",
    "            values_R = values_R[~np.isnan(values_R) & ~np.isinf(values_R)]\n",
    "            \n",
    "            if len(values_R) > 0:\n",
    "                if roi_id in combined_roi_data:\n",
    "                    combined_roi_data[roi_id]['R'] = values_R\n",
    "                else:\n",
    "                    combined_roi_data[roi_id] = {'L': None, 'R': values_R}\n",
    "                    subject_values[group_key][roi_id] = []\n",
    "        \n",
    "        # Create data for plotting\n",
    "        roi_data = []\n",
    "        \n",
    "        # Average values for each ROI and perform statistics\n",
    "        for roi_id, hemi_values in combined_roi_data.items():\n",
    "            # Get ROI name\n",
    "            if isinstance(roi_id, int):\n",
    "                roi_name = atlas_labels[roi_id]\n",
    "                if isinstance(roi_name, bytes):\n",
    "                    roi_name = roi_name.decode('utf-8')\n",
    "            else:\n",
    "                # It's a Wang ROI\n",
    "                roi_name = f\"Wang_{roi_id}\"\n",
    "            \n",
    "            # Combine L and R values if both exist\n",
    "            if hemi_values['L'] is not None and hemi_values['R'] is not None:\n",
    "                # Calculate the mean value for each subject by averaging L and R values\n",
    "                if len(hemi_values['L']) == len(hemi_values['R']):\n",
    "                    # If same number of samples, assume they are aligned\n",
    "                    combined_values = (hemi_values['L'] + hemi_values['R']) / 2\n",
    "                else:\n",
    "                    # If different number of samples, concatenate\n",
    "                    combined_values = np.concatenate([hemi_values['L'], hemi_values['R']])\n",
    "            elif hemi_values['L'] is not None:\n",
    "                combined_values = hemi_values['L']\n",
    "            elif hemi_values['R'] is not None:\n",
    "                combined_values = hemi_values['R']\n",
    "            else:\n",
    "                continue  # Skip if no values\n",
    "            \n",
    "            # Store combined values for bootstrapping\n",
    "            subject_values[group_key][roi_id] = combined_values\n",
    "            \n",
    "            # Get basic statistics\n",
    "            mean_value = np.mean(combined_values)\n",
    "            \n",
    "            # Perform t-test if possible\n",
    "            if len(combined_values) > 1 and np.var(combined_values) > 0:\n",
    "                t_stat, p_value = ttest_1samp(combined_values, 0)\n",
    "                is_significant = True  # Since data is already cleaned\n",
    "            else:\n",
    "                t_stat, p_value = np.nan, np.nan\n",
    "                is_significant = True  # Assuming all included data is significant\n",
    "            \n",
    "            # Perform LOOV analysis\n",
    "            loov_mean, loov_std = loov_analysis(combined_values)\n",
    "            \n",
    "            # Perform bootstrap analysis with more iterations\n",
    "            print(f\"  Running bootstrap for ROI {roi_id}...\")\n",
    "            bootstrap_mean, bootstrap_ci_lower, bootstrap_ci_upper = bootstrap_analysis(\n",
    "                combined_values, n_iterations=bootstrap_iterations)\n",
    "            \n",
    "            # Add to data list\n",
    "            roi_data.append({\n",
    "                'ROI_ID': roi_id + 1 if isinstance(roi_id, int) else roi_id,  # Make 1-indexed for display\n",
    "                'ROI_Name': roi_name,\n",
    "                'Clean_Name': clean_roi_name(roi_name),\n",
    "                'Mean_Value': mean_value,\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'Significant': is_significant,\n",
    "                'LOOV_Mean': loov_mean,\n",
    "                'LOOV_Std': loov_std,\n",
    "                'Bootstrap_Mean': bootstrap_mean,\n",
    "                'Bootstrap_CI_Lower': bootstrap_ci_lower,\n",
    "                'Bootstrap_CI_Upper': bootstrap_ci_upper\n",
    "            })\n",
    "        \n",
    "        if not roi_data:\n",
    "            print(f\"No valid data for {group_key}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        results_df = pd.DataFrame(roi_data)\n",
    "        \n",
    "        # Add anatomical lobe and hemisphere information\n",
    "        results_df['Anatomical_Lobe'] = results_df['ROI_Name'].apply(map_to_anatomical_lobe)\n",
    "        \n",
    "        # Extract hemisphere and base region info\n",
    "        hemisphere_region = results_df['ROI_Name'].apply(get_hemisphere_and_region)\n",
    "        results_df['Hemisphere'] = [h for h, r in hemisphere_region]\n",
    "        results_df['Base_Region'] = [r for h, r in hemisphere_region]\n",
    "        \n",
    "        # Define a manual order for anatomical lobes - posterior to anterior\n",
    "        lobe_order = [\n",
    "            'Occipital',  # most posterior\n",
    "            'Temporal',\n",
    "            'Parietal',\n",
    "            'Insular',\n",
    "            'Cingulate',\n",
    "            'Somatomotor',\n",
    "            'Frontal',    # most anterior\n",
    "            'Other'\n",
    "        ]\n",
    "        \n",
    "        # Create a category for sorting\n",
    "        lobe_cat = pd.Categorical(results_df['Anatomical_Lobe'], categories=lobe_order, ordered=True)\n",
    "        results_df['Lobe_Sorted'] = lobe_cat\n",
    "        \n",
    "        # Custom sorting function - group same ROIs (L/R) together\n",
    "        def custom_sort(row):\n",
    "            lobe_idx = lobe_order.index(row['Anatomical_Lobe']) if row['Anatomical_Lobe'] in lobe_order else 999\n",
    "            # Sort primarily by lobe, then by base region, then by hemisphere\n",
    "            # This places left and right hemisphere of the same ROI next to each other\n",
    "            hemi_idx = 0 if row['Hemisphere'] == 'L' else 1 if row['Hemisphere'] == 'R' else 2\n",
    "            return (lobe_idx, row['Base_Region'], hemi_idx)\n",
    "        \n",
    "        # Sort using the custom function\n",
    "        results_df['sort_key'] = results_df.apply(custom_sort, axis=1)\n",
    "        results_df_sorted = results_df.sort_values('sort_key')\n",
    "        \n",
    "        # Get the new order\n",
    "        sorted_indices = results_df_sorted.index.values\n",
    "        \n",
    "        # Extract reordered data\n",
    "        mean_values_sorted = results_df_sorted['Mean_Value'].values\n",
    "        roi_names_sorted = results_df_sorted['Clean_Name'].values\n",
    "        \n",
    "        # Get bootstrap confidence intervals\n",
    "        bootstrap_ci_lower_sorted = results_df_sorted['Bootstrap_CI_Lower'].values\n",
    "        bootstrap_ci_upper_sorted = results_df_sorted['Bootstrap_CI_Upper'].values\n",
    "        \n",
    "        # Create a combined ID with hemisphere to prevent duplicate labeling across hemispheres\n",
    "        results_df_sorted['Full_ID'] = results_df_sorted.apply(\n",
    "            lambda row: f\"{row['Base_Region']}_{row['Hemisphere']}\", axis=1)\n",
    "\n",
    "        # Find peak (highest) and bottom (lowest) ROIs in each lobe\n",
    "        peak_bottom_indices = []\n",
    "        lobe_groups = results_df_sorted.groupby('Anatomical_Lobe')\n",
    "        \n",
    "        for lobe, group in lobe_groups:\n",
    "            if len(group) > 0:\n",
    "                # Get highest value (most positive) ROI\n",
    "                peak_idx = group['Mean_Value'].idxmax()\n",
    "                peak_bottom_indices.append(peak_idx)\n",
    "                \n",
    "                # Get lowest value (most negative) ROI\n",
    "                bottom_idx = group['Mean_Value'].idxmin()\n",
    "                if bottom_idx != peak_idx:  # Avoid duplicates if there's only one ROI\n",
    "                    peak_bottom_indices.append(bottom_idx)\n",
    "        \n",
    "        # Create the visualization\n",
    "        plt.figure(figsize=(18, 10))\n",
    "\n",
    "        # Define x positions\n",
    "        x = np.arange(len(mean_values_sorted))\n",
    "\n",
    "        # Add lobe divisions (lines only, no colored backgrounds)\n",
    "        current_lobe = None\n",
    "        min_val = -3.5  # Adjusted for better visibility\n",
    "\n",
    "        for i, idx in enumerate(sorted_indices):\n",
    "            lobe = results_df_sorted.iloc[i]['Anatomical_Lobe']\n",
    "            if lobe != current_lobe:\n",
    "                if current_lobe is not None:\n",
    "                    # Add vertical line to separate lobes\n",
    "                    plt.axvline(x=i-0.5, color='black', linestyle='--', alpha=0.3)\n",
    "                    \n",
    "                    # Add label for previous lobe\n",
    "                    plt.text((start_idx + i - 1) / 2, min_val, \n",
    "                            current_lobe, ha='center', fontsize=11, fontweight='bold')\n",
    "                \n",
    "                current_lobe = lobe\n",
    "                start_idx = i\n",
    "\n",
    "        # Add the last lobe label\n",
    "        plt.text((start_idx + len(mean_values_sorted) - 1) / 2, min_val, \n",
    "                current_lobe, ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "        # Create base bar plot with colors based on positive/negative values\n",
    "        base_colors = ['darkblue' if val > 0 else 'darkred' for val in mean_values_sorted]\n",
    "        bars = plt.bar(x, mean_values_sorted, color=base_colors)\n",
    "\n",
    "        # Add error bars from bootstrap confidence intervals\n",
    "        for i in range(len(mean_values_sorted)):\n",
    "            plt.errorbar(x[i], mean_values_sorted[i], \n",
    "                        yerr=[[mean_values_sorted[i] - bootstrap_ci_lower_sorted[i]], \n",
    "                              [bootstrap_ci_upper_sorted[i] - mean_values_sorted[i]]], \n",
    "                        fmt='none', ecolor='black', capsize=3)\n",
    "\n",
    "        plt.axhline(y=0, color='black', linestyle='-')\n",
    "\n",
    "        # Add a legend for bar colors\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='darkblue', alpha=1.0, label='Positive GCA'),\n",
    "            Patch(facecolor='darkred', alpha=1.0, label='Negative GCA')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "        # Label peak and bottom ROIs in each lobe\n",
    "        try:\n",
    "            from adjustText import adjust_text\n",
    "            texts = []\n",
    "            \n",
    "            # Label peak and bottom ROIs\n",
    "            for peak_idx in peak_bottom_indices:\n",
    "                # Find position in sorted array\n",
    "                position = np.where(sorted_indices == peak_idx)[0][0]\n",
    "                clean_name = results_df.loc[peak_idx, 'Clean_Name']\n",
    "                mean_value = results_df.loc[peak_idx, 'Mean_Value']\n",
    "                hemisphere = results_df.loc[peak_idx, 'Hemisphere']\n",
    "                \n",
    "                # Include hemisphere in label for clarity\n",
    "                if hemisphere in ['L', 'R']:\n",
    "                    display_name = f\"{clean_name} ({hemisphere})\"\n",
    "                else:\n",
    "                    display_name = f\"{clean_name}\"\n",
    "                \n",
    "                # Create text object\n",
    "                text = plt.text(position, mean_value, display_name,\n",
    "                            ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "                            bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.8))\n",
    "                \n",
    "                texts.append(text)\n",
    "            \n",
    "            # Optimize text positions to avoid overlaps\n",
    "            adjust_text(texts, arrowprops=dict(arrowstyle='->', lw=1.5))\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fall back to original method if adjustText is not available\n",
    "            print(\"Note: Install 'adjustText' package for better label placement\")\n",
    "            \n",
    "            # Label peak and bottom ROIs\n",
    "            for peak_idx in peak_bottom_indices:\n",
    "                # Find position in sorted array\n",
    "                position = np.where(sorted_indices == peak_idx)[0][0]\n",
    "                clean_name = results_df.loc[peak_idx, 'Clean_Name']\n",
    "                mean_value = results_df.loc[peak_idx, 'Mean_Value']\n",
    "                hemisphere = results_df.loc[peak_idx, 'Hemisphere']\n",
    "                \n",
    "                # Include hemisphere in label for clarity\n",
    "                if hemisphere in ['L', 'R']:\n",
    "                    display_name = f\"{clean_name} ({hemisphere})\"\n",
    "                else:\n",
    "                    display_name = f\"{clean_name}\"\n",
    "                \n",
    "                # Add annotation\n",
    "                plt.annotate(display_name,\n",
    "                            xy=(position, mean_value),\n",
    "                            xytext=(0, 15 if mean_value >= 0 else -20),\n",
    "                            textcoords='offset points',\n",
    "                            ha='center',\n",
    "                            va='bottom' if mean_value >= 0 else 'top',\n",
    "                            fontsize=9,\n",
    "                            fontweight='bold',\n",
    "                            bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.8),\n",
    "                            arrowprops=dict(arrowstyle='->', lw=1.5))\n",
    "\n",
    "        # Set labels\n",
    "        plt.ylabel('GCA Value')\n",
    "        plt.xlabel('ROI ID (Organized by Anatomical Region)')\n",
    "        plt.title(f'{seed_roi} - GCA Values\\n({bootstrap_iterations} bootstrap iterations)')\n",
    "        plt.xlim(-0.5, len(mean_values_sorted) - 0.5)\n",
    "        # Set y-axis limits to fixed value\n",
    "        plt.ylim(-4, 8)\n",
    "        # Remove x-axis tick labels\n",
    "        plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "        # Add grid for readability\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save figure\n",
    "        fig_path = f'{bar_plot_dir}/{seed_roi}_gca_anatomical.png'\n",
    "        plt.savefig(fig_path, dpi=300)\n",
    "        \n",
    "        print(f\"Figure saved to {fig_path}\")\n",
    "        \n",
    "        # Save results table with all statistical metrics\n",
    "        results_file = f'{bar_plot_dir}/{seed_roi}_gca_statistics.csv'\n",
    "        results_df_sorted.to_csv(results_file, index=False)\n",
    "        print(f\"Statistics saved to {results_file}\")\n",
    "        \n",
    "        # Print peak and bottom ROIs by lobe\n",
    "        print(f\"\\nGCA peak and bottom ROIs by anatomical lobe:\")\n",
    "        for lobe, group in lobe_groups:\n",
    "            if len(group) > 0:\n",
    "                print(f\"\\n{lobe} Lobe:\")\n",
    "                \n",
    "                # Peak (positive) ROI\n",
    "                peak_row = group.loc[group['Mean_Value'].idxmax()]\n",
    "                peak_hemisphere = peak_row['Hemisphere']\n",
    "                print(f\"  Peak (Positive): {peak_row['Clean_Name']} ({peak_hemisphere})\")\n",
    "                print(f\"    Value = {peak_row['Mean_Value']:.3f}\")\n",
    "                print(f\"    Bootstrap 95% CI: [{peak_row['Bootstrap_CI_Lower']:.3f}, {peak_row['Bootstrap_CI_Upper']:.3f}]\")\n",
    "                print(f\"    LOOV: {peak_row['LOOV_Mean']:.3f}  {peak_row['LOOV_Std']:.3f}\")\n",
    "                \n",
    "                # Bottom (negative) ROI\n",
    "                bottom_row = group.loc[group['Mean_Value'].idxmin()]\n",
    "                bottom_hemisphere = bottom_row['Hemisphere']\n",
    "                # Only print if different from peak\n",
    "                if bottom_row.name != peak_row.name:\n",
    "                    print(f\"  Bottom (Negative): {bottom_row['Clean_Name']} ({bottom_hemisphere})\")\n",
    "                    print(f\"    Value = {bottom_row['Mean_Value']:.3f}\")\n",
    "                    print(f\"    Bootstrap 95% CI: [{bottom_row['Bootstrap_CI_Lower']:.3f}, {bottom_row['Bootstrap_CI_Upper']:.3f}]\")\n",
    "                    print(f\"    LOOV: {bottom_row['LOOV_Mean']:.3f}  {bottom_row['LOOV_Std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run the GCA anatomical organization visualization\n",
    "\n",
    "plot_gca_anatomical_organization_enhanced(\n",
    "    roi_values=your_roi_values_dict,  \n",
    "    atlas_labels=your_atlas_labels,  \n",
    "    output_dir='path/to/your/output_directory',  \n",
    "    bootstrap_iterations=1000  # Optional, default is 1000\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
