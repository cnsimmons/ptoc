{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ptoc_params as params\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, image, plotting\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set up directories and parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "study_dir = \"/lab_data/behrmannlab/vlad/ptoc\"\n",
    "raw_dir = params.raw_dir\n",
    "results_dir = f'{curr_dir}/results'\n",
    "roi_dir = f'{curr_dir}/roiParcels'\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "subjects_to_skip = ['sub-084']\n",
    "subs = sub_info[(sub_info['group'] == 'control') & (~sub_info['sub'].isin(subjects_to_skip))]['sub'].tolist()\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "\n",
    "# FDR parameters\n",
    "alpha = 0.05  # FDR alpha level\n",
    "two_sided = False  # Only keep positive correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Merged Atlas Function\n",
    "def create_merged_atlas():\n",
    "    \"\"\"Create a merged atlas where Wang ROIs replace overlapping regions in Schaefer atlas\"\"\"\n",
    "    logging.info(\"Creating merged atlas...\")\n",
    "    \n",
    "    # Load Wang ROIs\n",
    "    roi_files = {\n",
    "        'pIPS': f'{roi_dir}/pIPS.nii.gz',\n",
    "        'LO': f'{roi_dir}/LO.nii.gz'\n",
    "    }\n",
    "    \n",
    "    rois = {}\n",
    "    for roi_name, roi_path in roi_files.items():\n",
    "        if os.path.exists(roi_path):\n",
    "            rois[roi_name] = nib.load(roi_path)\n",
    "            logging.info(f\"Loaded {roi_name} ROI\")\n",
    "        else:\n",
    "            logging.error(f\"ROI file {roi_path} not found!\")\n",
    "            return None\n",
    "    \n",
    "    # Load Schaefer atlas\n",
    "    n_rois = 200  # Schaefer atlas ROIs\n",
    "    atlas = datasets.fetch_atlas_schaefer_2018(n_rois=n_rois, yeo_networks=7, resolution_mm=2)\n",
    "    atlas_img = nib.load(atlas.maps)\n",
    "    atlas_labels = atlas.labels\n",
    "    logging.info(f\"Loaded Schaefer atlas with {len(atlas_labels)} parcels\")\n",
    "    \n",
    "    # Get atlas data\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    modified_atlas_data = atlas_data.copy()\n",
    "    \n",
    "    # Create a dictionary to store new labels\n",
    "    new_labels = list(atlas_labels)\n",
    "    \n",
    "    # Assign values for new ROIs (continuing from the end of the Schaefer atlas)\n",
    "    roi_values = {'pIPS': 201, 'LO': 202}\n",
    "    overlap_info = {}\n",
    "    \n",
    "    # Process each ROI\n",
    "    for roi_name, roi_img in rois.items():\n",
    "        # Get ROI data and create mask\n",
    "        roi_data = roi_img.get_fdata()\n",
    "        roi_mask = roi_data > 0\n",
    "        \n",
    "        # Find overlapping parcels\n",
    "        overlap_mask = (atlas_data > 0) & roi_mask\n",
    "        overlapping_labels = np.unique(atlas_data[overlap_mask])\n",
    "        overlapping_labels = overlapping_labels[overlapping_labels > 0]\n",
    "        \n",
    "        # Get number of voxels in overlap\n",
    "        overlap_voxels = {}\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (atlas_data == label) & roi_mask\n",
    "            overlap_voxels[int(label)] = np.sum(label_mask)\n",
    "        \n",
    "        # Store overlap information\n",
    "        overlap_info[roi_name] = {\n",
    "            'overlapping_labels': overlapping_labels.tolist(),\n",
    "            'overlap_voxels': overlap_voxels\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"{roi_name} overlaps with {len(overlapping_labels)} atlas parcels\")\n",
    "        for label, voxels in overlap_voxels.items():\n",
    "            label_idx = int(label) - 1  # Convert to 0-indexed\n",
    "            if 0 <= label_idx < len(atlas_labels):\n",
    "                label_name = atlas_labels[label_idx]\n",
    "                label_name = label_name.decode('utf-8') if isinstance(label_name, bytes) else str(label_name)\n",
    "                logging.info(f\"  Label {label} ({label_name}): {voxels} voxels\")\n",
    "        \n",
    "        # Remove overlapping parcels from the atlas\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (modified_atlas_data == label) & roi_mask\n",
    "            modified_atlas_data[label_mask] = 0\n",
    "        \n",
    "        # Add ROI with new label\n",
    "        modified_atlas_data[roi_mask] = roi_values[roi_name]\n",
    "        \n",
    "        # Add new label name\n",
    "        new_labels.append(f\"Wang_{roi_name}\")\n",
    "    \n",
    "    # Create the modified atlas\n",
    "    merged_atlas_img = nib.Nifti1Image(modified_atlas_data, atlas_img.affine, atlas_img.header)\n",
    "    merged_atlas_file = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    nib.save(merged_atlas_img, merged_atlas_file)\n",
    "    logging.info(f\"Saved merged atlas to: {merged_atlas_file}\")\n",
    "    \n",
    "    # Save new labels array\n",
    "    labels_file = f'{results_dir}/merged_atlas_labels.npy'\n",
    "    np.save(labels_file, new_labels)\n",
    "    logging.info(f\"Saved merged labels to: {labels_file}\")\n",
    "    \n",
    "    return merged_atlas_img, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions \n",
    "def verify_standard_space(img):\n",
    "    \"\"\"Verify image is in 2mm standard space\"\"\"\n",
    "    if img.shape[:3] != (91, 109, 91):\n",
    "        logging.warning(f\"Unexpected shape: {img.shape}\")\n",
    "        return False\n",
    "    \n",
    "    vox_size = np.sqrt(np.sum(img.affine[:3, :3] ** 2, axis=0))\n",
    "    if not np.allclose(vox_size, [2., 2., 2.], atol=0.1):\n",
    "        logging.warning(f\"Unexpected voxel size: {vox_size}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_condition_mask(run_num, ss, condition, n_timepoints):\n",
    "    \"\"\"Create a binary mask for timepoints during a specific condition\"\"\"\n",
    "    cov_dir = f'{raw_dir}/{ss}/ses-01/covs'\n",
    "    ss_num = ss.split('-')[1]\n",
    "    \n",
    "    # Load condition timing file\n",
    "    cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{run_num}_{condition}.txt'\n",
    "    if not os.path.exists(cov_file):\n",
    "        logging.warning(f'Covariate file not found: {cov_file}')\n",
    "        return np.zeros(n_timepoints, dtype=bool)\n",
    "    \n",
    "    # Load timing data\n",
    "    cov = pd.read_csv(cov_file, sep='\\t', header=None, \n",
    "                     names=['onset', 'duration', 'value'])\n",
    "    \n",
    "    # Create timepoints array\n",
    "    tr = 2.0  # TR in seconds\n",
    "    times = np.arange(0, n_timepoints * tr, tr)\n",
    "    \n",
    "    # Convert timing to binary mask\n",
    "    condition_reg, _ = compute_regressor(cov.to_numpy().T, 'spm', times)\n",
    "    \n",
    "    # Convert to binary mask and ensure it's 1D\n",
    "    return (condition_reg > 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Threshold Connectivity Matrix\n",
    "def create_and_threshold_connectivity_matrix(ss, condition, merged_atlas_img):\n",
    "    \"\"\"Create and FDR-threshold connectivity matrix for specific condition\"\"\"\n",
    "    logging.info(f\"Processing subject {ss} for condition {condition}\")\n",
    "    \n",
    "    all_runs_data = []\n",
    "    \n",
    "    for rn in runs:\n",
    "        # Load standard space data\n",
    "        run_path = f'{raw_dir}/{ss}/ses-01/derivatives/reg_standard/filtered_func_run-0{rn}_standard.nii.gz'\n",
    "        \n",
    "        if not os.path.exists(run_path):\n",
    "            logging.warning(f'Standard space data not found: {run_path}')\n",
    "            continue\n",
    "        \n",
    "        subject_img = nib.load(run_path)\n",
    "        \n",
    "        # Verify standard space\n",
    "        if not verify_standard_space(subject_img):\n",
    "            logging.warning(f\"Data not in expected standard space for {ss} run-{rn}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract time series using merged atlas\n",
    "        masker = NiftiLabelsMasker(\n",
    "            labels_img=merged_atlas_img,\n",
    "            standardize='zscore_sample',\n",
    "            memory=None,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        time_series = masker.fit_transform(subject_img)\n",
    "        logging.info(f\"Time series shape before masking: {time_series.shape}\")\n",
    "        \n",
    "        # Get condition mask\n",
    "        condition_mask = get_condition_mask(rn, ss, condition, time_series.shape[0])\n",
    "        logging.info(f\"Condition mask shape: {condition_mask.shape}\")\n",
    "        \n",
    "        # Only keep timepoints during condition\n",
    "        masked_time_series = time_series[condition_mask]\n",
    "        logging.info(f\"Time series shape after masking: {masked_time_series.shape}\")\n",
    "        \n",
    "        if masked_time_series.shape[0] > 0:  # Only append if we have data\n",
    "            all_runs_data.append(masked_time_series)\n",
    "    \n",
    "    if not all_runs_data:\n",
    "        logging.warning(f'No valid data found for subject {ss} condition {condition}')\n",
    "        return None\n",
    "    \n",
    "    # Concatenate runs\n",
    "    full_time_series = np.concatenate(all_runs_data, axis=0)\n",
    "    logging.info(f\"Full time series shape: {full_time_series.shape}\")\n",
    "    \n",
    "    # Compute connectivity matrix using ConnectivityMeasure (as in your original code)\n",
    "    correlation_measure = ConnectivityMeasure(\n",
    "        kind='correlation',\n",
    "        standardize='zscore_sample'  # Z-score the time series\n",
    "    )\n",
    "    connectivity_matrix = correlation_measure.fit_transform([full_time_series])[0]\n",
    "    \n",
    "    # Save uncorrected matrix for reference\n",
    "    output_dir = f'{results_dir}/connectivity_merged_{condition.lower()}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(f'{output_dir}/{ss}_connectivity_uncorrected_{condition.lower()}.npy', connectivity_matrix)\n",
    "    \n",
    "    # Convert correlations to Z-scores using Fisher's Z-transform\n",
    "    # This is similar to z-scoring step in the threshold script\n",
    "    z_matrix = np.arctanh(connectivity_matrix)\n",
    "    \n",
    "    # Convert Z-scores to p-values (one or two-tailed test)\n",
    "    if two_sided:\n",
    "        p_matrix = 2 * (1 - norm.cdf(np.abs(z_matrix)))\n",
    "    else:\n",
    "        p_matrix = 1 - norm.cdf(z_matrix)\n",
    "    \n",
    "    # Apply FDR correction (similar to threshold_stats_img with height_control='fdr')\n",
    "    # Extract upper triangle (excluding diagonal) for correction\n",
    "    mask = np.triu(np.ones(z_matrix.shape), k=1).astype(bool)\n",
    "    p_values = p_matrix[mask]\n",
    "    \n",
    "    # Apply FDR correction\n",
    "    significant, _ = fdrcorrection(p_values, alpha=alpha)\n",
    "    logging.info(f\"FDR thresholding: {np.sum(significant)} of {len(p_values)} connections significant at Î±={alpha}\")\n",
    "    \n",
    "    # Create thresholded matrix\n",
    "    thresholded_matrix = np.zeros_like(connectivity_matrix)\n",
    "    thresholded_matrix[mask] = connectivity_matrix[mask] * significant\n",
    "    \n",
    "    # Make symmetric (as correlation matrices are symmetric)\n",
    "    thresholded_matrix = thresholded_matrix + thresholded_matrix.T\n",
    "    np.fill_diagonal(thresholded_matrix, 1.0)  # Set diagonal to 1.0\n",
    "    \n",
    "    # Similar to threshold script: zero out negative values if needed\n",
    "    if not two_sided:\n",
    "        thresholded_matrix[thresholded_matrix < 0] = 0\n",
    "    \n",
    "    return thresholded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /user_data/csimmon2/git_repos/ptoc\n",
      "Results directory exists: True\n",
      "ROI directory exists: True\n",
      "Number of subjects: 18\n",
      "ROI file pIPS exists: True\n",
      "ROI file LO exists: True\n",
      "Creating merged atlas...\n",
      "Merged atlas created successfully\n",
      "Testing with subject sub-025...\n"
     ]
    }
   ],
   "source": [
    "# execution script with debugging\n",
    "# Debug the script execution\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Results directory exists: {os.path.exists(results_dir)}\")\n",
    "print(f\"ROI directory exists: {os.path.exists(roi_dir)}\")\n",
    "print(f\"Number of subjects: {len(subs)}\")\n",
    "\n",
    "# Check if ROI files exist\n",
    "roi_files = {\n",
    "    'pIPS': f'{roi_dir}/pIPS.nii.gz',\n",
    "    'LO': f'{roi_dir}/LO.nii.gz'\n",
    "}\n",
    "for name, path in roi_files.items():\n",
    "    print(f\"ROI file {name} exists: {os.path.exists(path)}\")\n",
    "\n",
    "# Try to run the first few steps\n",
    "try:\n",
    "    print(\"Creating merged atlas...\")\n",
    "    merged_atlas_img, merged_labels = create_merged_atlas()\n",
    "    if merged_atlas_img is None:\n",
    "        print(\"Failed to create merged atlas!\")\n",
    "    else:\n",
    "        print(\"Merged atlas created successfully\")\n",
    "        \n",
    "        # Test with one subject\n",
    "        test_sub = subs[0] if subs else None\n",
    "        if test_sub:\n",
    "            print(f\"Testing with subject {test_sub}...\")\n",
    "            result = create_and_threshold_connectivity_matrix(test_sub, \"Object\", merged_atlas_img)\n",
    "            print(f\"Test result: {'Success' if result is not None else 'Failed'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
