{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ptoc_params as params\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, image, plotting\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set up directories and parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "study_dir = \"/lab_data/behrmannlab/vlad/ptoc\"\n",
    "raw_dir = params.raw_dir\n",
    "results_dir = f'{curr_dir}/results'\n",
    "roi_dir = f'{curr_dir}/roiParcels'\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "subjects_to_skip = ['sub-084']\n",
    "subs = sub_info[(sub_info['group'] == 'control') & (~sub_info['sub'].isin(subjects_to_skip))]['sub'].tolist()\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "\n",
    "# FDR parameters\n",
    "alpha = 0.05  # FDR alpha level\n",
    "two_sided = False  # Only keep positive correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Merged Atlas Function\n",
    "def create_merged_atlas():\n",
    "    \"\"\"Create a merged atlas where Wang ROIs replace overlapping regions in Schaefer atlas\"\"\"\n",
    "    logging.info(\"Creating merged atlas...\")\n",
    "    \n",
    "    # Load Wang ROIs\n",
    "    roi_files = {\n",
    "        'pIPS': f'{roi_dir}/pIPS.nii.gz',\n",
    "        'LO': f'{roi_dir}/LO.nii.gz'\n",
    "    }\n",
    "    \n",
    "    rois = {}\n",
    "    for roi_name, roi_path in roi_files.items():\n",
    "        if os.path.exists(roi_path):\n",
    "            rois[roi_name] = nib.load(roi_path)\n",
    "            logging.info(f\"Loaded {roi_name} ROI\")\n",
    "        else:\n",
    "            logging.error(f\"ROI file {roi_path} not found!\")\n",
    "            return None\n",
    "    \n",
    "    # Load Schaefer atlas\n",
    "    n_rois = 200  # Schaefer atlas ROIs\n",
    "    atlas = datasets.fetch_atlas_schaefer_2018(n_rois=n_rois, yeo_networks=7, resolution_mm=2)\n",
    "    atlas_img = nib.load(atlas.maps)\n",
    "    atlas_labels = atlas.labels\n",
    "    logging.info(f\"Loaded Schaefer atlas with {len(atlas_labels)} parcels\")\n",
    "    \n",
    "    # Get atlas data\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    modified_atlas_data = atlas_data.copy()\n",
    "    \n",
    "    # Create a dictionary to store new labels\n",
    "    new_labels = list(atlas_labels)\n",
    "    \n",
    "    # Assign values for new ROIs (continuing from the end of the Schaefer atlas)\n",
    "    roi_values = {'pIPS': 201, 'LO': 202}\n",
    "    overlap_info = {}\n",
    "    \n",
    "    # Process each ROI\n",
    "    for roi_name, roi_img in rois.items():\n",
    "        # Get ROI data and create mask\n",
    "        roi_data = roi_img.get_fdata()\n",
    "        roi_mask = roi_data > 0\n",
    "        \n",
    "        # Find overlapping parcels\n",
    "        overlap_mask = (atlas_data > 0) & roi_mask\n",
    "        overlapping_labels = np.unique(atlas_data[overlap_mask])\n",
    "        overlapping_labels = overlapping_labels[overlapping_labels > 0]\n",
    "        \n",
    "        # Get number of voxels in overlap\n",
    "        overlap_voxels = {}\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (atlas_data == label) & roi_mask\n",
    "            overlap_voxels[int(label)] = np.sum(label_mask)\n",
    "        \n",
    "        # Store overlap information\n",
    "        overlap_info[roi_name] = {\n",
    "            'overlapping_labels': overlapping_labels.tolist(),\n",
    "            'overlap_voxels': overlap_voxels\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"{roi_name} overlaps with {len(overlapping_labels)} atlas parcels\")\n",
    "        for label, voxels in overlap_voxels.items():\n",
    "            label_idx = int(label) - 1  # Convert to 0-indexed\n",
    "            if 0 <= label_idx < len(atlas_labels):\n",
    "                label_name = atlas_labels[label_idx]\n",
    "                label_name = label_name.decode('utf-8') if isinstance(label_name, bytes) else str(label_name)\n",
    "                logging.info(f\"  Label {label} ({label_name}): {voxels} voxels\")\n",
    "        \n",
    "        # Remove overlapping parcels from the atlas\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (modified_atlas_data == label) & roi_mask\n",
    "            modified_atlas_data[label_mask] = 0\n",
    "        \n",
    "        # Add ROI with new label\n",
    "        modified_atlas_data[roi_mask] = roi_values[roi_name]\n",
    "        \n",
    "        # Add new label name\n",
    "        new_labels.append(f\"Wang_{roi_name}\")\n",
    "    \n",
    "    # Create the modified atlas\n",
    "    merged_atlas_img = nib.Nifti1Image(modified_atlas_data, atlas_img.affine, atlas_img.header)\n",
    "    merged_atlas_file = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    nib.save(merged_atlas_img, merged_atlas_file)\n",
    "    logging.info(f\"Saved merged atlas to: {merged_atlas_file}\")\n",
    "    \n",
    "    # Save new labels array\n",
    "    labels_file = f'{results_dir}/merged_atlas_labels.npy'\n",
    "    np.save(labels_file, new_labels)\n",
    "    logging.info(f\"Saved merged labels to: {labels_file}\")\n",
    "    \n",
    "    return merged_atlas_img, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions \n",
    "def verify_standard_space(img):\n",
    "    \"\"\"Verify image is in 2mm standard space\"\"\"\n",
    "    if img.shape[:3] != (91, 109, 91):\n",
    "        logging.warning(f\"Unexpected shape: {img.shape}\")\n",
    "        return False\n",
    "    \n",
    "    vox_size = np.sqrt(np.sum(img.affine[:3, :3] ** 2, axis=0))\n",
    "    if not np.allclose(vox_size, [2., 2., 2.], atol=0.1):\n",
    "        logging.warning(f\"Unexpected voxel size: {vox_size}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_condition_mask(run_num, ss, condition, n_timepoints):\n",
    "    \"\"\"Create a binary mask for timepoints during a specific condition\"\"\"\n",
    "    cov_dir = f'{raw_dir}/{ss}/ses-01/covs'\n",
    "    ss_num = ss.split('-')[1]\n",
    "    \n",
    "    # Load condition timing file\n",
    "    cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{run_num}_{condition}.txt'\n",
    "    if not os.path.exists(cov_file):\n",
    "        logging.warning(f'Covariate file not found: {cov_file}')\n",
    "        return np.zeros(n_timepoints, dtype=bool)\n",
    "    \n",
    "    # Load timing data\n",
    "    cov = pd.read_csv(cov_file, sep='\\t', header=None, \n",
    "                     names=['onset', 'duration', 'value'])\n",
    "    \n",
    "    # Create timepoints array\n",
    "    tr = 2.0  # TR in seconds\n",
    "    times = np.arange(0, n_timepoints * tr, tr)\n",
    "    \n",
    "    # Convert timing to binary mask\n",
    "    condition_reg, _ = compute_regressor(cov.to_numpy().T, 'spm', times)\n",
    "    \n",
    "    # Convert to binary mask and ensure it's 1D\n",
    "    return (condition_reg > 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connectivity matrices with merged atlas\n",
    "def create_connectivity_matrices():\n",
    "    \"\"\"Create unthresholded connectivity matrices for each subject and condition\"\"\"\n",
    "    # Create merged atlas\n",
    "    merged_atlas_img, _ = create_merged_atlas()\n",
    "    \n",
    "    for condition in ['Object', 'Scramble']:\n",
    "        output_dir = f'{results_dir}/connectivity_merged_{condition.lower()}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for ss in subs:\n",
    "            # Extract time series data\n",
    "            all_runs_data = []\n",
    "            for rn in runs:\n",
    "                run_path = f'{raw_dir}/{ss}/ses-01/derivatives/reg_standard/filtered_func_run-0{rn}_standard.nii.gz'\n",
    "                if not os.path.exists(run_path): continue\n",
    "                \n",
    "                masker = NiftiLabelsMasker(labels_img=merged_atlas_img, standardize='zscore_sample')\n",
    "                time_series = masker.fit_transform(run_path)\n",
    "                condition_mask = get_condition_mask(rn, ss, condition, time_series.shape[0])\n",
    "                masked_time_series = time_series[condition_mask]\n",
    "                \n",
    "                if masked_time_series.shape[0] > 0:\n",
    "                    all_runs_data.append(masked_time_series)\n",
    "            \n",
    "            if not all_runs_data: continue\n",
    "            \n",
    "            # Calculate correlation matrix\n",
    "            full_time_series = np.concatenate(all_runs_data, axis=0)\n",
    "            correlation_measure = ConnectivityMeasure(kind='correlation', standardize='zscore_sample')\n",
    "            conn_matrix = correlation_measure.fit_transform([full_time_series])[0]\n",
    "            \n",
    "            # Save unthresholded matrix\n",
    "            output_path = f'{output_dir}/{ss}_connectivity_{condition.lower()}.npy'\n",
    "            np.save(output_path, conn_matrix)\n",
    "            print(f\"Saved unthresholded matrix for {ss}, condition {condition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to create connectivity matrices with parcel-based FDR correction\"\"\"\n",
    "    # Create merged atlas\n",
    "    merged_atlas_img, merged_labels = create_merged_atlas()\n",
    "    \n",
    "    conditions = ['Object', 'Scramble']\n",
    "    alpha = 0.05  # FDR threshold\n",
    "    \n",
    "    for condition in conditions:\n",
    "        output_dir = f'{results_dir}/connectivity_merged_{condition.lower()}_parcel_fdr'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for ss in subs:\n",
    "            try:\n",
    "                # Process subject data\n",
    "                all_runs_data = []\n",
    "                for rn in runs:\n",
    "                    run_path = f'{raw_dir}/{ss}/ses-01/derivatives/reg_standard/filtered_func_run-0{rn}_standard.nii.gz'\n",
    "                    if not os.path.exists(run_path):\n",
    "                        continue\n",
    "                    \n",
    "                    masker = NiftiLabelsMasker(labels_img=merged_atlas_img, standardize='zscore_sample')\n",
    "                    time_series = masker.fit_transform(run_path)\n",
    "                    condition_mask = get_condition_mask(rn, ss, condition, time_series.shape[0])\n",
    "                    masked_time_series = time_series[condition_mask]\n",
    "                    \n",
    "                    if masked_time_series.shape[0] > 0:\n",
    "                        all_runs_data.append(masked_time_series)\n",
    "                \n",
    "                if not all_runs_data:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate correlation matrix\n",
    "                full_time_series = np.concatenate(all_runs_data, axis=0)\n",
    "                correlation_measure = ConnectivityMeasure(kind='correlation', standardize='zscore_sample')\n",
    "                conn_matrix = correlation_measure.fit_transform([full_time_series])[0]\n",
    "                \n",
    "                # Apply FDR correction based on parcels (not connections)\n",
    "                z_matrix = np.arctanh(conn_matrix)\n",
    "                z_matrix[np.isinf(z_matrix)] = np.finfo(float).max\n",
    "                \n",
    "                # For each parcel, correct its connections\n",
    "                n_parcels = conn_matrix.shape[0]\n",
    "                thresholded = np.zeros_like(conn_matrix)\n",
    "                \n",
    "                for i in range(n_parcels):\n",
    "                    # Get p-values for this parcel's connections\n",
    "                    p_values = 1 - norm.cdf(z_matrix[i, :])\n",
    "                    \n",
    "                    # Apply FDR\n",
    "                    significant, _ = fdrcorrection(p_values, alpha=alpha)\n",
    "                    \n",
    "                    # Set significant connections\n",
    "                    thresholded[i, :] = conn_matrix[i, :] * significant\n",
    "                \n",
    "                # Set diagonal to 1\n",
    "                np.fill_diagonal(thresholded, 1.0)\n",
    "                \n",
    "                # Save the matrix\n",
    "                output_path = f'{output_dir}/{ss}_connectivity_{condition.lower()}.npy'\n",
    "                np.save(output_path, thresholded)\n",
    "                logging.info(f\"Saved parcel-FDR matrix for {ss}, condition {condition}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {ss}, condition {condition}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full analysis for 18 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to:\n",
      "1. Merged atlas: /user_data/csimmon2/git_repos/ptoc/results/schaefer_wang_merged.nii.gz\n",
      "2. FDR-thresholded matrices: /user_data/csimmon2/git_repos/ptoc/results/connectivity_merged_object_fdr/ and /user_data/csimmon2/git_repos/ptoc/results/connectivity_merged_scramble_fdr/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in arctanh\n"
     ]
    }
   ],
   "source": [
    "# Run the complete analysis\n",
    "print(f\"Starting full analysis for {len(subs)} subjects...\")\n",
    "main()\n",
    "print(\"Analysis complete. Results saved to:\")\n",
    "print(f\"1. Merged atlas: {results_dir}/schaefer_wang_merged.nii.gz\")\n",
    "print(f\"2. FDR-thresholded matrices: {results_dir}/connectivity_merged_object_fdr/ and {results_dir}/connectivity_merged_scramble_fdr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating group average for 'object' from 18 subjects\n",
      "Saved group average to /user_data/csimmon2/git_repos/ptoc/results/connectivity_visualizations/group_average_object.png\n",
      "Creating group average for 'scramble' from 18 subjects\n",
      "Saved group average to /user_data/csimmon2/git_repos/ptoc/results/connectivity_visualizations/group_average_scramble.png\n",
      "Saved difference map to /user_data/csimmon2/git_repos/ptoc/results/connectivity_visualizations/difference_map_object_minus_scramble.png\n",
      "Group visualizations saved to /user_data/csimmon2/git_repos/ptoc/results/connectivity_visualizations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/user_data/csimmon2/git_repos/ptoc/results/connectivity_visualizations'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "def visualize_group_connectivity():\n",
    "    \"\"\"Create group average connectivity matrices with white diagonal and difference map\"\"\"\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set up visualization directory\n",
    "    viz_dir = f'{results_dir}/connectivity_visualizations'\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store group matrices\n",
    "    group_matrices = {}\n",
    "    \n",
    "    # Process each condition\n",
    "    for condition in ['object', 'scramble']:\n",
    "        matrix_dir = f'{results_dir}/connectivity_merged_{condition}'\n",
    "        \n",
    "        # Find all matrix files\n",
    "        matrix_files = [f for f in os.listdir(matrix_dir) if f.endswith('.npy')]\n",
    "        print(f\"Creating group average for '{condition}' from {len(matrix_files)} subjects\")\n",
    "        \n",
    "        # Load and average matrices\n",
    "        matrices = []\n",
    "        for matrix_file in matrix_files:\n",
    "            matrix_path = f'{matrix_dir}/{matrix_file}'\n",
    "            matrix = np.load(matrix_path)\n",
    "            # Keep only positive correlations\n",
    "            matrix[matrix < 0] = 0\n",
    "            matrices.append(matrix)\n",
    "        \n",
    "        # Create group average\n",
    "        group_matrix = np.mean(matrices, axis=0)\n",
    "        group_matrices[condition] = group_matrix.copy()  # Save a copy for the difference map\n",
    "        \n",
    "        # Set diagonal to NaN so it will appear white in the visualization\n",
    "        np.fill_diagonal(group_matrix, np.nan)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(group_matrix, cmap=\"Reds\", vmin=0, vmax=1, square=True)\n",
    "        plt.title(f\"Group Average Connectivity Matrix - {condition.capitalize()}\")\n",
    "        \n",
    "        # Save figure\n",
    "        out_path = f\"{viz_dir}/group_average_{condition}.png\"\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved group average to {out_path}\")\n",
    "    \n",
    "    # Create difference map (Object - Scramble)\n",
    "    if 'object' in group_matrices and 'scramble' in group_matrices:\n",
    "        diff_matrix = group_matrices['object'] - group_matrices['scramble']\n",
    "        \n",
    "        # Set diagonal to NaN for white display\n",
    "        np.fill_diagonal(diff_matrix, np.nan)\n",
    "        \n",
    "        # Create heatmap with diverging colormap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(diff_matrix, cmap=\"RdBu_r\", center=0, square=True)\n",
    "        plt.title(\"Difference Map (Object - Scramble)\")\n",
    "        \n",
    "        # Save figure\n",
    "        out_path = f\"{viz_dir}/difference_map_object_minus_scramble.png\"\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved difference map to {out_path}\")\n",
    "    \n",
    "    print(f\"Group visualizations saved to {viz_dir}\")\n",
    "    return viz_dir\n",
    "\n",
    "# Visualize connectivity matrices\n",
    "visualize_group_connectivity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
