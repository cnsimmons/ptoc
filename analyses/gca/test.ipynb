{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.p check input at native or standard, check mask as native or standard, check time series, check parameters too of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/nilearn/__init__.py:67: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import logging\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy import stats\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "import sys\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Scramble'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = sub_info['sub'].tolist()\n",
    "subs = ['sub-025']\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2  # in voxels, adjust as needed\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "\n",
    "def load_and_prepare_data(sub, run_combo):\n",
    "    \"\"\"\n",
    "    Load and prepare data for a single subject and run combination.\n",
    "    \n",
    "    :param sub: Subject ID\n",
    "    :param run_combo: List of run numbers to combine\n",
    "    :return: Tuple of (4D fMRI data, brain mask, psychological covariate)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data for subject {sub}, runs {run_combo}\")\n",
    "    \n",
    "    # Load and combine run data\n",
    "    run_data_list = []\n",
    "    for run in run_combo:\n",
    "        run_file = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz' #registered but not standardaized\n",
    "        run_img = image.load_img(run_file)\n",
    "        run_data = image.clean_img(run_img, standardize=True)\n",
    "        run_data_list.append(run_data)\n",
    "    \n",
    "    # Concatenate run data\n",
    "    fmri_data = image.concat_imgs(run_data_list)\n",
    "    \n",
    "    # Load brain mask\n",
    "    mask_file = f'{study_dir}/{sub}/ses-01/derivatives/rois/parcels/pIPS.nii.gz'\n",
    "    brain_mask = nib.load(mask_file).get_fdata().astype(bool)\n",
    "    \n",
    "    # Generate psychological covariate\n",
    "    psy = make_psy_cov(run_combo, sub)\n",
    "    \n",
    "    # Ensure fMRI data and psy covariate have the same number of time points\n",
    "    if fmri_data.shape[-1] != len(psy):\n",
    "        raise ValueError(f\"Mismatch in volumes: fMRI data has {fmri_data.shape[-1]}, psy has {len(psy)}\")\n",
    "    \n",
    "    # Convert fmri_data to 4D numpy array if it's not already\n",
    "    if isinstance(fmri_data, nib.Nifti1Image):\n",
    "        fmri_data = fmri_data.get_fdata()\n",
    "    \n",
    "    # Ensure fmri_data is 4D\n",
    "    if fmri_data.ndim != 4:\n",
    "        raise ValueError(f\"fMRI data must be 4D, but got shape {fmri_data.shape}\")\n",
    "    \n",
    "    return fmri_data, brain_mask, psy\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    vols_per_run, tr = 184, 2.0\n",
    "    total_vols = vols_per_run * len(runs)\n",
    "    times = np.arange(0, total_vols * tr, tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            continue\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * vols_per_run * tr\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable declaration\n",
    "global psy\n",
    "\n",
    "def gca_measure(data, mask, myrad, bcvar):\n",
    "    global psy\n",
    "    \n",
    "    data_4d = data[0]\n",
    "    data_2d = data_4d.reshape(data_4d.shape[0], -1)\n",
    "    center_ts = data_2d[:, 0]\n",
    "    other_ts = np.mean(data_2d[:, 1:], axis=1)\n",
    "    \n",
    "    gc_center_to_other = grangercausalitytests(np.column_stack((other_ts, center_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    gc_other_to_center = grangercausalitytests(np.column_stack((center_ts, other_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    \n",
    "    f_diff = gc_center_to_other[1][0]['ssr_ftest'][0] - gc_other_to_center[1][0]['ssr_ftest'][0]\n",
    "    \n",
    "    return f_diff\n",
    "\n",
    "def run_searchlight(fmri_data, brain_mask):\n",
    "    assert brain_mask.ndim == 3 and brain_mask.dtype == bool, \"Invalid brain_mask\"\n",
    "\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=10)\n",
    "    fmri_data_4d = fmri_data.transpose(3, 0, 1, 2)\n",
    "    fmri_data_list = [fmri_data_4d]\n",
    "    \n",
    "    sl.distribute(fmri_data_list, brain_mask)\n",
    "    sl_result = sl.run_searchlight(gca_measure, pool_size=1)\n",
    "    \n",
    "    return sl_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global psy\n",
    "    \n",
    "    for sub in subs:\n",
    "        for run_combo in run_combos:\n",
    "            try:\n",
    "                fmri_data, brain_mask, psy = load_and_prepare_data(sub, run_combo)\n",
    "                print(f\"Processing subject {sub}, runs {run_combo}\")\n",
    "                print(f\"Data shapes - fMRI: {fmri_data.shape}, Mask: {brain_mask.shape}, PSY: {psy.shape}\")\n",
    "                \n",
    "                assert psy.shape[0] == fmri_data.shape[3], \"PSY shape mismatch\"\n",
    "                \n",
    "                sl_result = run_searchlight(fmri_data, brain_mask)\n",
    "                \n",
    "                result_file = f'{results_dir}/searchlight_gca_{sub}_runs{\"_\".join(map(str, run_combo))}.nii.gz'\n",
    "                nib.save(nib.Nifti1Image(sl_result, affine=nib.load(f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz').affine), result_file)\n",
    "                \n",
    "                print(f\"Completed analysis for subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "            except AssertionError as e:\n",
    "                print(f\"Assertion error for subject {sub}, runs {run_combo}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subject {sub}, runs {run_combo}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NiftiMasker' from 'nilearn.masking' (/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/nilearn/masking.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38452/3960972724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbrainiak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchlight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchlight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearchlight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrangercausalitytests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNiftiMasker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Libraries loaded...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NiftiMasker' from 'nilearn.masking' (/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/nilearn/masking.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys, time, os, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from nilearn.masking import NiftiMasker\n",
    "\n",
    "print('Libraries loaded...')\n",
    "\n",
    "# Setup directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Object'\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = '/lab_data/behrmannlab/vlad/hemispace'\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv('/user_data/csimmon2/git_repos/ptoc/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = sub_info['sub'].tolist()\n",
    "subs = ['sub-025']  # For testing, using only one subject\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "shape = Ball\n",
    "\n",
    "def get_subject_mask(sub):\n",
    "    mask_path = f'{raw_dir}/{sub}/ses-01/anat/{sub}_ses-01_T1w_brain_mask.nii.gz'\n",
    "    return nib.load(mask_path)\n",
    "\n",
    "def load_data(sub, run_combo):\n",
    "    print('Loading data...')\n",
    "    \n",
    "    all_runs = []\n",
    "    for run in run_combo:\n",
    "        print(f\"Loading run {run}\")\n",
    "        run_file = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "        curr_run = image.load_img(run_file)\n",
    "        all_runs.append(curr_run)\n",
    "\n",
    "    print('Data loaded, concatenating...')\n",
    "    bold_vol = image.concat_imgs(all_runs)\n",
    "    print('Data concatenated')\n",
    "    \n",
    "    return bold_vol\n",
    "\n",
    "def make_psy_cov(run_combo, sub):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    vols_per_run, tr = 184, 2.0\n",
    "    total_vols = vols_per_run * len(runs)\n",
    "    times = np.arange(0, total_vols * tr, tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            continue\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * vols_per_run * tr\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy\n",
    "\n",
    "def gca_measure(data, mask, myrad, bcvar):\n",
    "    global psy\n",
    "    \n",
    "    data_4d = data[0]\n",
    "    data_2d = data_4d.reshape(-1, data_4d.shape[-1])\n",
    "    center_ts = data_2d[0, :]\n",
    "    other_ts = np.mean(data_2d[1:, :], axis=0)\n",
    "\n",
    "    if np.allclose(center_ts, center_ts[0]) or np.allclose(other_ts, other_ts[0]) or np.allclose(psy, psy[0]):\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        gc_center_to_other = grangercausalitytests(np.column_stack((other_ts, center_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "        gc_other_to_center = grangercausalitytests(np.column_stack((center_ts, other_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "        \n",
    "        f_diff = gc_center_to_other[1][0]['ssr_ftest'][0] - gc_other_to_center[1][0]['ssr_ftest'][0]\n",
    "        return f_diff\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "def run_searchlight(fmri_data, brain_mask):\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=max_blk_edge, shape=shape)\n",
    "    sl.distribute([fmri_data], brain_mask)\n",
    "    sl_result = sl.run_searchlight(gca_measure, pool_size=pool_size)\n",
    "    return sl_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global psy\n",
    "    \n",
    "    for sub in subs:\n",
    "        for run_combo in run_combos:\n",
    "            try:\n",
    "                # Load subject-specific mask\n",
    "                subject_mask = get_subject_mask(sub)\n",
    "                brain_masker = NiftiMasker(mask_img=subject_mask, standardize=True)\n",
    "\n",
    "                # Load fMRI data\n",
    "                fmri_data = load_data(sub, run_combo)\n",
    "                \n",
    "                # Apply mask and standardize\n",
    "                fmri_data_masked = brain_masker.fit_transform(fmri_data)\n",
    "                \n",
    "                # Generate psychological covariate\n",
    "                psy = make_psy_cov(run_combo, sub)\n",
    "                \n",
    "                print(f\"Processing subject {sub}, runs {run_combo}\")\n",
    "                print(f\"Data shapes - fMRI: {fmri_data_masked.shape}, Mask: {subject_mask.shape}, PSY: {psy.shape}\")\n",
    "                \n",
    "                # Ensure psy is the right shape\n",
    "                assert psy.shape[0] == fmri_data_masked.shape[0], \"PSY shape mismatch\"\n",
    "                \n",
    "                # Reshape fMRI data for searchlight\n",
    "                fmri_data_4d = brain_masker.inverse_transform(fmri_data_masked).get_fdata()\n",
    "                \n",
    "                # Run searchlight analysis\n",
    "                sl_result = run_searchlight(fmri_data_4d, subject_mask.get_fdata().astype(bool))\n",
    "                \n",
    "                # Save results\n",
    "                result_file = f'{results_dir}/searchlight_gca_{sub}_runs{\"_\".join(map(str, run_combo))}.nii.gz'\n",
    "                nib.save(nib.Nifti1Image(sl_result, affine=subject_mask.affine), result_file)\n",
    "                \n",
    "                print(f\"Completed analysis for subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subject {sub}, runs {run_combo}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
