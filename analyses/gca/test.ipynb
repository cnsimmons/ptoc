{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.p check input at native or standard, check mask as native or standard, check time series, check parameters too of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 10:53:10,587 - INFO - Processing subject sub-025, runs [1, 2]\n",
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/nilearn/image/image.py:1282: FutureWarning: The default strategy for standardize is currently 'zscore' which incorrectly uses population std to calculate sample zscores. The new strategy 'zscore_sample' corrects this behavior by using the sample std. In release 0.13, the default strategy will be replaced by the new strategy and the 'zscore' option will be removed. Please use 'zscore_sample' instead.\n",
      "  **clean_kwargs,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import logging\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Scramble'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = ['sub-025']  # For testing, you can include specific subjects\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2  # in voxels, adjust as needed\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "\n",
    "# Constants\n",
    "VOL_PER_RUN = 184\n",
    "TR = 2.0\n",
    "first_fix = 8  # Number of initial volumes to skip\n",
    "\n",
    "# Load whole brain mask\n",
    "whole_brain_mask = image.load_img('/user_data/csimmon2/git_repos/ptoc/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz')\n",
    "\n",
    "def load_data(sub, run_combo, raw_dir, whole_brain_mask, first_fix):\n",
    "    all_runs = []\n",
    "    for run in run_combo:\n",
    "        curr_run = image.load_img(f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/registered_data/filtered_func_data_standard.nii.gz')\n",
    "        curr_run = image.get_data(image.clean_img(curr_run, standardize=True, mask_img=whole_brain_mask))\n",
    "        curr_run = curr_run[:,:,:,first_fix:]\n",
    "        all_runs.append(curr_run)\n",
    "    \n",
    "    bold_vol = np.concatenate(np.array(all_runs), axis=3)\n",
    "    return bold_vol\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    \"\"\"\n",
    "    Create psychological covariate data for the specified runs and subject.\n",
    "    \"\"\"\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    total_vols = VOL_PER_RUN * len(runs)\n",
    "    times = np.arange(0, total_vols * TR, TR)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            return np.zeros((total_vols, 1))  # Return a zeros array if file not found\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * VOL_PER_RUN * TR\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy\n",
    "\n",
    "def extract_pips_timeseries(bold_vol, pips_mask):\n",
    "    reshaped_mask = np.reshape(pips_mask, (91, 109, 91, 1))\n",
    "    masked_img = reshaped_mask * bold_vol\n",
    "    pips_ts = masked_img.reshape(-1, bold_vol.shape[3])\n",
    "    pips_ts = pips_ts[~np.all(pips_ts == 0, axis=1)]\n",
    "    return np.mean(pips_ts, axis=0)\n",
    "\n",
    "def gca_measure(data, mask, myrad, bcvar):\n",
    "    pips_ts, psy = bcvar\n",
    "    \n",
    "    # Apply mask to get the searchlight sphere time series\n",
    "    sphere_ts = data[0][mask].mean(axis=0)\n",
    "    \n",
    "    # Combine time series into a single 2D array\n",
    "    combined_ts = np.column_stack((sphere_ts, pips_ts, psy))\n",
    "    \n",
    "    try:\n",
    "        gc_pips_to_sphere = grangercausalitytests(combined_ts[:, [1, 0, 2]], maxlag=1, verbose=False)\n",
    "        gc_sphere_to_pips = grangercausalitytests(combined_ts[:, [0, 1, 2]], maxlag=1, verbose=False)\n",
    "        \n",
    "        # Calculate the difference in F-statistics\n",
    "        f_diff = gc_pips_to_sphere[1][0]['ssr_ftest'][0] - gc_sphere_to_pips[1][0]['ssr_ftest'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in grangercausalitytests: {str(e)}\")\n",
    "        return np.nan\n",
    "    \n",
    "    return f_diff\n",
    "\n",
    "def run_searchlight(fmri_data, whole_brain_mask, pips_mask, psy):\n",
    "    # Extract pIPS time series\n",
    "    pips_ts = extract_pips_timeseries(fmri_data, pips_mask)\n",
    "    \n",
    "    # Setup searchlight\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=max_blk_edge, shape=Ball)\n",
    "    \n",
    "    # Distribute data and broadcast pips_ts and psy\n",
    "    sl.distribute([fmri_data], whole_brain_mask)\n",
    "    sl.broadcast((pips_ts, psy))\n",
    "    \n",
    "    # Run searchlight\n",
    "    sl_result = sl.run_searchlight(gca_measure, pool_size=pool_size)\n",
    "    \n",
    "    return sl_result\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    for sub in subs:\n",
    "        for run_combo in run_combos:\n",
    "            try:\n",
    "                logging.info(f\"Processing subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "                # Load fMRI data\n",
    "                fmri_data = load_data(sub, run_combo, raw_dir, whole_brain_mask, first_fix)\n",
    "                \n",
    "                # Load pIPS mask\n",
    "                pips_mask_file = f'/user_data/csimmon2/git_repos/ptoc/roiParcels/pIPS.nii.gz'\n",
    "                pips_mask = nib.load(pips_mask_file).get_fdata().astype(bool)\n",
    "                \n",
    "                # Create psychological covariate\n",
    "                psy = make_psy_cov(run_combo, sub)\n",
    "                \n",
    "                logging.info(f\"Data shapes - fMRI: {fmri_data.shape}, Whole Brain Mask: {whole_brain_mask.get_fdata().shape}, pIPS Mask: {pips_mask.shape}, PSY: {psy.shape}\")\n",
    "                \n",
    "                # Run searchlight\n",
    "                sl_result = run_searchlight(fmri_data, whole_brain_mask.get_fdata(), pips_mask, psy)\n",
    "                \n",
    "                # Save results\n",
    "                result_file = f'{results_dir}/searchlight_gca_{sub}_runs{\"_\".join(map(str, run_combo))}.nii.gz'\n",
    "                affine = nib.load(f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-01/registered_data/filtered_func_data_standard.nii.gz').affine\n",
    "                nib.save(nib.Nifti1Image(sl_result, affine), result_file)\n",
    "                \n",
    "                logging.info(f\"Completed analysis for subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subject {sub}, runs {run_combo}: {str(e)}\")\n",
    "            finally:\n",
    "                gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
