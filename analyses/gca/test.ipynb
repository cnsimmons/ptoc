{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.p check input at native or standard, check mask as native or standard, check time series, check parameters too of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first and foremost - no more changing\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import logging\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy import stats\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "import sys\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Scramble'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = sub_info['sub'].tolist()\n",
    "subs = ['sub-025']\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2  # in voxels, adjust as needed\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "\n",
    "def load_and_prepare_data(sub, run_combo):\n",
    "    \"\"\"\n",
    "    Load and prepare data for a single subject and run combination.\n",
    "    \n",
    "    :param sub: Subject ID\n",
    "    :param run_combo: List of run numbers to combine\n",
    "    :return: Tuple of (4D fMRI data, brain mask, psychological covariate)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data for subject {sub}, runs {run_combo}\")\n",
    "    \n",
    "    # Load and combine run data\n",
    "    run_data_list = []\n",
    "    for run in run_combo:\n",
    "        run_file = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz' #registered but not standardaized\n",
    "        run_img = image.load_img(run_file)\n",
    "        run_data = image.clean_img(run_img, standardize=True)\n",
    "        run_data_list.append(run_data)\n",
    "    \n",
    "    # Concatenate run data\n",
    "    fmri_data = image.concat_imgs(run_data_list)\n",
    "    \n",
    "    # Load brain mask\n",
    "    mask_file = f'{study_dir}/{sub}/ses-01/derivatives/rois/parcels/pIPS.nii.gz'\n",
    "    brain_mask = nib.load(mask_file).get_fdata().astype(bool)\n",
    "    \n",
    "    # Generate psychological covariate\n",
    "    psy = make_psy_cov(run_combo, sub)\n",
    "    \n",
    "    # Ensure fMRI data and psy covariate have the same number of time points\n",
    "    if fmri_data.shape[-1] != len(psy):\n",
    "        raise ValueError(f\"Mismatch in volumes: fMRI data has {fmri_data.shape[-1]}, psy has {len(psy)}\")\n",
    "    \n",
    "    # Convert fmri_data to 4D numpy array if it's not already\n",
    "    if isinstance(fmri_data, nib.Nifti1Image):\n",
    "        fmri_data = fmri_data.get_fdata()\n",
    "    \n",
    "    # Ensure fmri_data is 4D\n",
    "    if fmri_data.ndim != 4:\n",
    "        raise ValueError(f\"fMRI data must be 4D, but got shape {fmri_data.shape}\")\n",
    "    \n",
    "    return fmri_data, brain_mask, psy\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    vols_per_run, tr = 184, 2.0\n",
    "    total_vols = vols_per_run * len(runs)\n",
    "    times = np.arange(0, total_vols * tr, tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            continue\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * vols_per_run * tr\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy\n",
    "\n",
    "\n",
    "# Global variable declaration\n",
    "global psy\n",
    "\n",
    "def gca_measure(data, mask, myrad, bcvar):\n",
    "    global psy\n",
    "    \n",
    "    data_4d = data[0]\n",
    "    data_2d = data_4d.reshape(data_4d.shape[0], -1)\n",
    "    center_ts = data_2d[:, 0]\n",
    "    other_ts = np.mean(data_2d[:, 1:], axis=1)\n",
    "    \n",
    "    gc_center_to_other = grangercausalitytests(np.column_stack((other_ts, center_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    gc_other_to_center = grangercausalitytests(np.column_stack((center_ts, other_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    \n",
    "    f_diff = gc_center_to_other[1][0]['ssr_ftest'][0] - gc_other_to_center[1][0]['ssr_ftest'][0]\n",
    "    \n",
    "    return f_diff\n",
    "\n",
    "def run_searchlight(fmri_data, brain_mask):\n",
    "    assert brain_mask.ndim == 3 and brain_mask.dtype == bool, \"Invalid brain_mask\"\n",
    "\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=10)\n",
    "    fmri_data_4d = fmri_data.transpose(3, 0, 1, 2)\n",
    "    fmri_data_list = [fmri_data_4d]\n",
    "    \n",
    "    sl.distribute(fmri_data_list, brain_mask)\n",
    "    sl_result = sl.run_searchlight(gca_measure, pool_size=1)\n",
    "    \n",
    "    return sl_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global psy\n",
    "    \n",
    "    for sub in subs:\n",
    "        for run_combo in run_combos:\n",
    "            try:\n",
    "                fmri_data, brain_mask, psy = load_and_prepare_data(sub, run_combo)\n",
    "                print(f\"Processing subject {sub}, runs {run_combo}\")\n",
    "                print(f\"Data shapes - fMRI: {fmri_data.shape}, Mask: {brain_mask.shape}, PSY: {psy.shape}\")\n",
    "                \n",
    "                assert psy.shape[0] == fmri_data.shape[3], \"PSY shape mismatch\"\n",
    "                \n",
    "                sl_result = run_searchlight(fmri_data, brain_mask)\n",
    "                \n",
    "                result_file = f'{results_dir}/searchlight_gca_{sub}_runs{\"_\".join(map(str, run_combo))}.nii.gz'\n",
    "                nib.save(nib.Nifti1Image(sl_result, affine=nib.load(f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz').affine), result_file)\n",
    "                \n",
    "                print(f\"Completed analysis for subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "            except AssertionError as e:\n",
    "                print(f\"Assertion error for subject {sub}, runs {run_combo}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subject {sub}, runs {run_combo}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csimmon2/anaconda3/envs/brainiak_env/lib/python3.7/site-packages/nilearn/__init__.py:67: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n",
      "2024-10-07 16:55:53,267 - INFO - Loading data for subject sub-025, runs [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# take 2 gpt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import logging\n",
    "from brainiak.searchlight.searchlight import Searchlight\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Scramble'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "#subs = sub_info['sub'].tolist()\n",
    "# For testing, you can uncomment the line below to include all subjects\n",
    "subs = ['sub-025']\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2  # in voxels, adjust as needed\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "\n",
    "# Constants\n",
    "VOL_PER_RUN = 184\n",
    "TR = 2.0\n",
    "\n",
    "def load_and_prepare_data(sub, run_combo):\n",
    "    \"\"\"\n",
    "    Load and prepare data for a single subject and run combination.\n",
    "    \n",
    "    :param sub: Subject ID\n",
    "    :param run_combo: List of run numbers to combine\n",
    "    :return: Tuple of (4D fMRI data, brain mask, psychological covariate)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data for subject {sub}, runs {run_combo}\")\n",
    "\n",
    "    # Load and combine run data\n",
    "    run_data_list = []\n",
    "    for run in run_combo:\n",
    "        run_file = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "        run_img = image.load_img(run_file)\n",
    "        run_data = image.clean_img(run_img, standardize=True)\n",
    "        run_data_list.append(run_data)\n",
    "    \n",
    "    # Concatenate run data\n",
    "    fmri_data = image.concat_imgs(run_data_list)\n",
    "    \n",
    "    # Load brain mask\n",
    "    mask_file = f'{study_dir}/{sub}/ses-01/derivatives/rois/parcels/pIPS.nii.gz'\n",
    "    brain_mask = nib.load(mask_file).get_fdata().astype(bool)\n",
    "\n",
    "    # Generate psychological covariate\n",
    "    psy = make_psy_cov(run_combo, sub)\n",
    "\n",
    "    # Ensure fMRI data and psy covariate have the same number of time points\n",
    "    if fmri_data.shape[-1] != len(psy):\n",
    "        raise ValueError(f\"Mismatch in volumes: fMRI data has {fmri_data.shape[-1]}, psy has {len(psy)}\")\n",
    "    \n",
    "    # Convert fmri_data to 4D numpy array if it's not already\n",
    "    if isinstance(fmri_data, nib.Nifti1Image):\n",
    "        fmri_data = fmri_data.get_fdata()\n",
    "    \n",
    "    # Ensure fmri_data is 4D\n",
    "    if fmri_data.ndim != 4:\n",
    "        raise ValueError(f\"fMRI data must be 4D, but got shape {fmri_data.shape}\")\n",
    "    \n",
    "    return fmri_data, brain_mask, psy\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    \"\"\"\n",
    "    Create psychological covariate data for the specified runs and subject.\n",
    "\n",
    "    :param runs: List of run numbers\n",
    "    :param ss: Subject ID\n",
    "    :return: Psychological covariate as a numpy array\n",
    "    \"\"\"\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    total_vols = VOL_PER_RUN * len(runs)\n",
    "    times = np.arange(0, total_vols * TR, TR)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            return np.zeros((total_vols, 1))  # Return a zeros array if file not found\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * VOL_PER_RUN * TR\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy\n",
    "\n",
    "def gca_measure(data, mask, myrad, psy):\n",
    "    \"\"\"\n",
    "    Measure Granger causality between the center and surrounding time series.\n",
    "\n",
    "    :param data: 4D fMRI data\n",
    "    :param mask: Brain mask\n",
    "    :param myrad: Searchlight radius\n",
    "    :param psy: Psychological covariate\n",
    "    :return: Difference in Granger causality measures\n",
    "    \"\"\"\n",
    "    data_4d = data[0]\n",
    "    data_2d = data_4d.reshape(data_4d.shape[0], -1)\n",
    "    center_ts = data_2d[:, 0]\n",
    "    other_ts = np.mean(data_2d[:, 1:], axis=1)\n",
    "    \n",
    "    gc_center_to_other = grangercausalitytests(np.column_stack((other_ts, center_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    gc_other_to_center = grangercausalitytests(np.column_stack((center_ts, other_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    \n",
    "    f_diff = gc_center_to_other[1][0]['ssr_ftest'][0] - gc_other_to_center[1][0]['ssr_ftest'][0]\n",
    "    \n",
    "    return f_diff\n",
    "\n",
    "def run_searchlight(fmri_data, brain_mask, psy):\n",
    "    \"\"\"\n",
    "    Run searchlight analysis on the fMRI data.\n",
    "\n",
    "    :param fmri_data: 4D fMRI data\n",
    "    :param brain_mask: Brain mask\n",
    "    :param psy: Psychological covariate\n",
    "    :return: Searchlight results\n",
    "    \"\"\"\n",
    "    assert brain_mask.ndim == 3 and brain_mask.dtype == bool, \"Invalid brain_mask\"\n",
    "    assert fmri_data.shape[0:3] == brain_mask.shape, \"Brain mask dimensions do not match fMRI data.\"\n",
    "\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=max_blk_edge)\n",
    "    fmri_data_4d = fmri_data.transpose(3, 0, 1, 2)\n",
    "    fmri_data_list = [fmri_data_4d]\n",
    "    \n",
    "    sl.distribute(fmri_data_list, brain_mask)\n",
    "    sl_result = sl.run_searchlight(gca_measure, psy=psy, pool_size=pool_size)\n",
    "    \n",
    "    return sl_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for sub in subs:\n",
    "        for run_combo in run_combos:\n",
    "            try:\n",
    "                fmri_data, brain_mask, psy = load_and_prepare_data(sub, run_combo)\n",
    "                logging.info(f\"Processing subject {sub}, runs {run_combo}\")\n",
    "                logging.info(f\"Data shapes - fMRI: {fmri_data.shape}, Mask: {brain_mask.shape}, PSY: {psy.shape}\")\n",
    "                \n",
    "                assert psy.shape[0] == fmri_data.shape[3], \"PSY shape mismatch\"\n",
    "                \n",
    "                sl_result = run_searchlight(fmri_data, brain_mask, psy)\n",
    "                \n",
    "                result_file = f'{results_dir}/searchlight_gca_{sub}_runs{\"_\".join(map(str, run_combo))}.nii.gz'\n",
    "                affine = nib.load(f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz').affine\n",
    "                nib.save(nib.Nifti1Image(sl_result, affine), result_file)\n",
    "                \n",
    "                logging.info(f\"Completed analysis for subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "            except AssertionError as e:\n",
    "                logging.error(f\"Assertion error for subject {sub}, runs {run_combo}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subject {sub}, runs {run_combo}: {str(e)}\")\n",
    "            finally:\n",
    "                gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
