{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.p check input at native or standard, check mask as native or standard, check time series, check parameters too of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import logging\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy import stats\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "import sys\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Scramble'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = sub_info['sub'].tolist()\n",
    "subs = ['sub-025']\n",
    "\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2  # in voxels, adjust as needed\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "\n",
    "def load_and_prepare_data(sub, run_combo):\n",
    "    \"\"\"\n",
    "    Load and prepare data for a single subject and run combination.\n",
    "    \n",
    "    :param sub: Subject ID\n",
    "    :param run_combo: List of run numbers to combine\n",
    "    :return: Tuple of (4D fMRI data, brain mask, psychological covariate)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data for subject {sub}, runs {run_combo}\")\n",
    "    \n",
    "    # Load and combine run data\n",
    "    run_data_list = []\n",
    "    for run in run_combo:\n",
    "        run_file = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz' #registered but not standardaized\n",
    "        run_img = image.load_img(run_file)\n",
    "        run_data = image.clean_img(run_img, standardize=True)\n",
    "        run_data_list.append(run_data)\n",
    "    \n",
    "    # Concatenate run data\n",
    "    fmri_data = image.concat_imgs(run_data_list)\n",
    "    \n",
    "    # Load brain mask\n",
    "    mask_file = f'{study_dir}/{sub}/ses-01/derivatives/rois/parcels/pIPS.nii.gz'\n",
    "    brain_mask = nib.load(mask_file).get_fdata().astype(bool)\n",
    "    \n",
    "    # Generate psychological covariate\n",
    "    psy = make_psy_cov(run_combo, sub)\n",
    "    \n",
    "    # Ensure fMRI data and psy covariate have the same number of time points\n",
    "    if fmri_data.shape[-1] != len(psy):\n",
    "        raise ValueError(f\"Mismatch in volumes: fMRI data has {fmri_data.shape[-1]}, psy has {len(psy)}\")\n",
    "    \n",
    "    # Convert fmri_data to 4D numpy array if it's not already\n",
    "    if isinstance(fmri_data, nib.Nifti1Image):\n",
    "        fmri_data = fmri_data.get_fdata()\n",
    "    \n",
    "    # Ensure fmri_data is 4D\n",
    "    if fmri_data.ndim != 4:\n",
    "        raise ValueError(f\"fMRI data must be 4D, but got shape {fmri_data.shape}\")\n",
    "    \n",
    "    return fmri_data, brain_mask, psy\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    vols_per_run, tr = 184, 2.0\n",
    "    total_vols = vols_per_run * len(runs)\n",
    "    times = np.arange(0, total_vols * tr, tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            continue\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * vols_per_run * tr\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable declaration\n",
    "global psy\n",
    "\n",
    "def gca_measure(data, mask, myrad, bcvar):\n",
    "    global psy\n",
    "    \n",
    "    data_4d = data[0]\n",
    "    data_2d = data_4d.reshape(data_4d.shape[0], -1)\n",
    "    center_ts = data_2d[:, 0]\n",
    "    other_ts = np.mean(data_2d[:, 1:], axis=1)\n",
    "    \n",
    "    gc_center_to_other = grangercausalitytests(np.column_stack((other_ts, center_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    gc_other_to_center = grangercausalitytests(np.column_stack((center_ts, other_ts, psy.flatten())), maxlag=1, verbose=False)\n",
    "    \n",
    "    f_diff = gc_center_to_other[1][0]['ssr_ftest'][0] - gc_other_to_center[1][0]['ssr_ftest'][0]\n",
    "    \n",
    "    return f_diff\n",
    "\n",
    "def run_searchlight(fmri_data, brain_mask):\n",
    "    assert brain_mask.ndim == 3 and brain_mask.dtype == bool, \"Invalid brain_mask\"\n",
    "\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=10)\n",
    "    fmri_data_4d = fmri_data.transpose(3, 0, 1, 2)\n",
    "    fmri_data_list = [fmri_data_4d]\n",
    "    \n",
    "    sl.distribute(fmri_data_list, brain_mask)\n",
    "    sl_result = sl.run_searchlight(gca_measure, pool_size=1)\n",
    "    \n",
    "    return sl_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global psy\n",
    "    \n",
    "    for sub in subs:\n",
    "        for run_combo in run_combos:\n",
    "            try:\n",
    "                fmri_data, brain_mask, psy = load_and_prepare_data(sub, run_combo)\n",
    "                print(f\"Processing subject {sub}, runs {run_combo}\")\n",
    "                print(f\"Data shapes - fMRI: {fmri_data.shape}, Mask: {brain_mask.shape}, PSY: {psy.shape}\")\n",
    "                \n",
    "                assert psy.shape[0] == fmri_data.shape[3], \"PSY shape mismatch\"\n",
    "                \n",
    "                sl_result = run_searchlight(fmri_data, brain_mask)\n",
    "                \n",
    "                result_file = f'{results_dir}/searchlight_gca_{sub}_runs{\"_\".join(map(str, run_combo))}.nii.gz'\n",
    "                nib.save(nib.Nifti1Image(sl_result, affine=nib.load(f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz').affine), result_file)\n",
    "                \n",
    "                print(f\"Completed analysis for subject {sub}, runs {run_combo}\")\n",
    "                \n",
    "            except AssertionError as e:\n",
    "                print(f\"Assertion error for subject {sub}, runs {run_combo}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subject {sub}, runs {run_combo}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 13:30:49,475 - INFO - Running GCA Searchlight for Object...\n",
      "2024-10-07 13:30:49,482 - INFO - ROI coordinates loaded for subject sub-025\n",
      "2024-10-07 13:30:49,483 - INFO - Processing run combination [1, 2] for subject sub-025\n",
      "2024-10-07 13:30:49,485 - INFO - Loading data for subject sub-025, runs [1, 2]\n"
     ]
    }
   ],
   "source": [
    "#HELP 10.7.24\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image, input_data\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import logging\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Object'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "#subs = sub_info['sub'].tolist()\n",
    "subs = ['sub-025']\n",
    "\n",
    "rois = ['pIPS', 'LO']\n",
    "hemispheres = ['left', 'right']\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "# Searchlight parameters\n",
    "searchlight_radius = 2 \n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "\n",
    "def extract_roi_sphere(img, coords):\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius=6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    phys = np.mean(seed_time_series, axis=1).reshape(-1, 1)\n",
    "    return phys\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    vols_per_run, tr = 184, 2.0\n",
    "    total_vols = vols_per_run * len(runs)\n",
    "    times = np.arange(0, total_vols * tr, tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            continue\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * vols_per_run * tr\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy\n",
    "\n",
    "def extract_cond_ts(ts, cov):\n",
    "    block_ind = (cov==1)\n",
    "    block_ind = np.insert(block_ind, 0, True)\n",
    "    block_ind = np.delete(block_ind, len(block_ind)-1)\n",
    "    block_ind = (cov == 1).reshape((len(cov))) | block_ind\n",
    "    return ts[block_ind]\n",
    "\n",
    "from nilearn import image\n",
    "\n",
    "def load_and_prepare_data(sub, run_combo):\n",
    "    logging.info(f\"Loading data for subject {sub}, runs {run_combo}\")\n",
    "    \n",
    "    # Load whole-brain mask (assumed to be in standard space)\n",
    "    whole_brain_mask_file = f'/user_data/csimmon2/git_repos/ptoc/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz'\n",
    "    if not os.path.exists(whole_brain_mask_file):\n",
    "        raise FileNotFoundError(f\"Whole-brain mask file not found: {whole_brain_mask_file}\")\n",
    "    whole_brain_mask = nib.load(whole_brain_mask_file)\n",
    "    \n",
    "    filtered_list = []\n",
    "    for run in run_combo:\n",
    "        run_file = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "        if not os.path.exists(run_file):\n",
    "            raise FileNotFoundError(f\"Run file not found: {run_file}\")\n",
    "        \n",
    "        # Load the current run\n",
    "        curr_run = nib.load(run_file)\n",
    "        \n",
    "        # Standardize the fMRI data to match the space of the whole-brain mask\n",
    "        # You may need to adjust the target_affine and target_shape based on your specific standardization requirements\n",
    "        standardized_run = image.resample_img(curr_run, \n",
    "                                              target_affine=whole_brain_mask.affine, \n",
    "                                              target_shape=whole_brain_mask.shape,\n",
    "                                              interpolation='continuous')\n",
    "        \n",
    "        # Now clean and mask the standardized data\n",
    "        cleaned_run = image.clean_img(standardized_run, standardize=True, mask_img=whole_brain_mask)\n",
    "        \n",
    "        # Get the data as a numpy array\n",
    "        cleaned_data = image.get_data(cleaned_run)\n",
    "        \n",
    "        filtered_list.append(cleaned_data)\n",
    "    \n",
    "    # Concatenate the runs\n",
    "    fmri_data_array = np.concatenate(filtered_list, axis=-1)\n",
    "    \n",
    "    # Create a 4D image for use with nilearn functions\n",
    "    img4d = nib.Nifti1Image(fmri_data_array, whole_brain_mask.affine)\n",
    "    \n",
    "    return fmri_data_array, whole_brain_mask.get_fdata().astype(bool), img4d\n",
    "\n",
    "\n",
    "def gca_measure(data, mask, myrad, seed_ts, psy):\n",
    "    data_4d = data[0]\n",
    "    data_2d = data_4d.reshape(data_4d.shape[0], -1)\n",
    "    center_ts = data_2d[:, 0]\n",
    "    \n",
    "    center_ts_cond = extract_cond_ts(center_ts, psy)\n",
    "    seed_ts_cond = extract_cond_ts(seed_ts, psy)\n",
    "    \n",
    "    if np.allclose(center_ts_cond, center_ts_cond[0]) or np.allclose(seed_ts_cond, seed_ts_cond[0]):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        gc_center_to_seed = grangercausalitytests(np.column_stack((seed_ts_cond, center_ts_cond)), maxlag=1, verbose=False)\n",
    "        gc_seed_to_center = grangercausalitytests(np.column_stack((center_ts_cond, seed_ts_cond)), maxlag=1, verbose=False)\n",
    "        f_diff = gc_center_to_seed[1][0]['ssr_ftest'][0] - gc_seed_to_center[1][0]['ssr_ftest'][0]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error in Granger causality test: {str(e)}. Returning 0 for this voxel.\")\n",
    "        return 0\n",
    "    \n",
    "    return f_diff\n",
    "\n",
    "def run_searchlight(fmri_data, brain_mask, seed_ts, psy):\n",
    "    sl = Searchlight(sl_rad=searchlight_radius, max_blk_edge=max_blk_edge, shape=Ball)\n",
    "    sl.distribute([fmri_data], brain_mask)\n",
    "    sl.broadcast(seed_ts)\n",
    "    sl.broadcast(psy)\n",
    "    sl_result = sl.run_searchlight(gca_measure, pool_size=pool_size)\n",
    "    return sl_result\n",
    "\n",
    "def conduct_gca_searchlight():\n",
    "    logging.info(f'Running GCA Searchlight for {localizer}...')\n",
    "    \n",
    "    for ss in subs:\n",
    "        sub_dir = f'{study_dir}/{ss}/ses-01/'\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "        os.makedirs(f'{sub_dir}/derivatives/gca_searchlight', exist_ok=True)\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords_std.csv')\n",
    "        logging.info(f\"ROI coordinates loaded for subject {ss}\")\n",
    "\n",
    "        for rcn, rc in enumerate(run_combos):\n",
    "            logging.info(f\"Processing run combination {rc} for subject {ss}\")\n",
    "            \n",
    "            fmri_data, brain_mask, img4d = load_and_prepare_data(ss, rc)\n",
    "            psy = make_psy_cov(rc, ss)\n",
    "            \n",
    "            if fmri_data.shape[3] != psy.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in volumes: fMRI data has {fmri_data.shape[3]}, psy has {psy.shape[0]}\")\n",
    "            \n",
    "            for dorsal_roi in ['pIPS']:\n",
    "                for dorsal_hemi in hemispheres:\n",
    "                    dorsal_coords = roi_coords[(roi_coords['index'] == rcn) & \n",
    "                                               (roi_coords['roi'] == dorsal_roi) &\n",
    "                                               (roi_coords['hemisphere'] == dorsal_hemi)]\n",
    "                    \n",
    "                    if dorsal_coords.empty:\n",
    "                        logging.warning(f\"No coordinates found for {dorsal_roi}, {dorsal_hemi}, run combo {rc}\")\n",
    "                        continue\n",
    "\n",
    "                    dorsal_ts = extract_roi_sphere(img4d, dorsal_coords[['x', 'y', 'z']].values.tolist()[0])\n",
    "                    \n",
    "                    t1 = time.time()\n",
    "                    logging.info(f\"Begin Searchlight for {ss}, {dorsal_roi}_{dorsal_hemi}, run combo {rc}\")\n",
    "                    sl_result = run_searchlight(fmri_data, brain_mask, dorsal_ts, psy)\n",
    "                    logging.info(f\"End Searchlight - Duration: {(time.time()-t1)/60:.2f} minutes\")\n",
    "                    \n",
    "                    # Save results\n",
    "                    sl_result = sl_result.astype('double')\n",
    "                    sl_result[np.isnan(sl_result)] = 0\n",
    "                    affine = img4d.affine\n",
    "                    sl_nii = nib.Nifti1Image(sl_result, affine)\n",
    "                    output_file = f'{sub_dir}/derivatives/gca_searchlight/searchlight_gca_{ss}_{dorsal_hemi}_{dorsal_roi}_seed_rc{rcn}_{localizer.lower()}.nii.gz'\n",
    "                    nib.save(sl_nii, output_file)\n",
    "                    logging.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conduct_gca_searchlight()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
