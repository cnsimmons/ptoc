{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries loaded...\n",
      "['sub-025'] ['pIPS']\n",
      "Searchlight setup ...\n",
      "Loading data...\n",
      "Loading run 1\n",
      "Run 1 shape: (176, 256, 256, 184)\n",
      "Memory usage after run 1: 20.0933837890625 MB\n",
      "Loading run 2\n",
      "Run 2 shape: (176, 256, 256, 184)\n",
      "Memory usage after run 2: 21.25104522705078 MB\n",
      "Data loaded. Concatenating...\n",
      "Concatenated data shape: (176, 256, 256, 368)\n",
      "Final memory usage: 21.25104522705078 MB\n",
      "Data concatenated...\n",
      "bold_vol type: <class 'numpy.ndarray'>\n",
      "bold_vol shape: (176, 256, 256, 368)\n",
      "bold_vol dtype: float32\n",
      "Extracting seed time series...\n",
      "Loading seed ROI from: /lab_data/behrmannlab/vlad/ptoc/sub-025/ses-01/derivatives/rois/spheres_nifti/sub-025_pIPS_left_loc_sphere_r6mm.nii.gz\n",
      "Loaded seed ROI shape: (176, 256, 256)\n",
      "Bold volume shape: (176, 256, 256, 368)\n",
      "Extracted seed time series shape: (368, 114)\n",
      "Seed data extracted successfully.\n",
      "51.74021530151367\n",
      "Begin Searchlight None\n",
      "Distribute 51.74021530151367\n",
      "Broadcast 51.74021530151367\n",
      "Run 51.74021530151367\n"
     ]
    }
   ],
   "source": [
    "#claude trying to return to my functioning history with the error of only two runs and focusing on one subject hard coded\n",
    "#searchlight MVPD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import resource\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from nilearn import image, datasets\n",
    "import nibabel as nib\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "\n",
    "def transform_mask_to_native(subject_func, standard_mask, output_dir):\n",
    "    \"\"\"\n",
    "    Transform the standard space whole brain mask to the subject's native space.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    func_img = image.load_img(subject_func)\n",
    "    mask_img = image.load_img(standard_mask)\n",
    "    \n",
    "    native_mask = image.resample_to_img(mask_img, func_img, interpolation='nearest')\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'whole_brain_mask_native.nii.gz')\n",
    "    native_mask.to_filename(output_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "print('libraries loaded...')\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = ['sub-025']  # Uncomment for testing\n",
    "dorsal = ['pIPS'] # Run for one ROI initially\n",
    "\n",
    "print(subs, dorsal)\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Object'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = \"/lab_data/behrmannlab/vlad/hemispace\"\n",
    "exp = 'loc' \n",
    "\n",
    "out_dir = f'{study_dir}/derivatives/fc'\n",
    "sub_dir = f'{study_dir}/sub-025/ses-01/'\n",
    "cov_dir = f'{raw_dir}/covs'\n",
    "roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "exp_dir = f'{sub_dir}/derivatives/fsl/{exp}'\n",
    "\n",
    "runs = list(range(1,3))\n",
    "\n",
    "standard_mask_path = '/user_data/csimmon2/git_repos/ptoc/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz'\n",
    "native_mask_path = transform_mask_to_native(\n",
    "    f'{exp_dir}/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz',\n",
    "    standard_mask_path,\n",
    "    f'{sub_dir}/derivatives/masks'\n",
    ")\n",
    "whole_brain_mask = image.load_img(native_mask_path)\n",
    "\n",
    "affine = whole_brain_mask.affine\n",
    "dimsize = whole_brain_mask.header.get_zooms()  #get dimensions\n",
    "\n",
    "# scan parameters\n",
    "vols = 184\n",
    "first_fix = 0\n",
    "\n",
    "# threshold for PCA\n",
    "pc_thresh = .9\n",
    "\n",
    "clf = LinearRegression()\n",
    "rs = ShuffleSplit(n_splits=5, test_size=1/3, random_state=0)\n",
    "\n",
    "print('Searchlight setup ...')\n",
    "mask = image.get_data(whole_brain_mask) #the mask to search within\n",
    "\n",
    "sl_rad = 2 #radius of searchlight sphere (in voxels)\n",
    "max_blk_edge = 10 #how many blocks to send on each parallelized search\n",
    "pool_size = 1 #number of cores to work on each search\n",
    "\n",
    "voxels_proportion=1\n",
    "shape = Ball\n",
    "\n",
    "def extract_pc(data, n_components=None):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "def calc_pc_n(pca, thresh):\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    var = 0\n",
    "    for n_comp, ev in enumerate(explained_variance):\n",
    "        var += ev\n",
    "        if var >= thresh:\n",
    "            break\n",
    "    return n_comp+1\n",
    "\n",
    "def calc_mvc(seed_train,seed_test, target_train, target_test, target_pc):\n",
    "    all_corrs = []\n",
    "    for pcn in range(0,len(target_pc.explained_variance_ratio_)):\n",
    "        clf.fit(seed_train, target_train[:,pcn])\n",
    "        pred_ts = clf.predict(seed_test)\n",
    "        weighted_corr = np.corrcoef(pred_ts,target_test[:,pcn])[0,1] * target_pc.explained_variance_ratio_[pcn]\n",
    "        all_corrs.append(weighted_corr)\n",
    "    final_corr = np.sum(all_corrs)/(np.sum(target_pc.explained_variance_ratio_))\n",
    "    return final_corr\n",
    "\n",
    "def create_ts_mask(train, test):\n",
    "    train_index = []\n",
    "    test_index = []\n",
    "    for tr in train:\n",
    "        train_index = train_index + list(range((tr-1) * (vols-first_fix),((tr-1) * (vols-first_fix)) + (vols-first_fix)))\n",
    "    for te in test:\n",
    "        test_index = test_index + list(range((te-1) * (vols-first_fix),((te-1) * (vols-first_fix)) + (vols-first_fix)))\n",
    "    return train_index, test_index\n",
    "\n",
    "def mvpd(data, sl_mask, myrad, seed_ts):\n",
    "    data4D = data[0]\n",
    "    data4D = np.transpose(data4D.reshape(-1, data[0].shape[3]))\n",
    "    mvc_list = []\n",
    "    for train_runs, test_runs in rs.split(runs): \n",
    "        train_index, test_index = create_ts_mask(train_runs, test_runs)\n",
    "        seed_train = seed_ts[train_index,:]\n",
    "        seed_test = seed_ts[test_index,:]\n",
    "        target_train = data4D[train_index, :]\n",
    "        target_test = data4D[test_index, :]\n",
    "        n_comp = calc_pc_n(extract_pc(seed_train),pc_thresh)\n",
    "        seed_pca = extract_pc(seed_train, n_comp)\n",
    "        seed_train_pcs = seed_pca.transform(seed_train)\n",
    "        seed_test_pcs = seed_pca.transform(seed_test)\n",
    "        n_comp = calc_pc_n(extract_pc(target_train),pc_thresh)\n",
    "        target_pca = extract_pc(target_train, n_comp)\n",
    "        target_train_pcs = target_pca.transform(target_train)\n",
    "        target_test_pcs = target_pca.transform(target_test)\n",
    "        mvc_list.append(calc_mvc(seed_train_pcs, seed_test_pcs, target_train_pcs, target_test_pcs, target_pca))\n",
    "    return np.mean(mvc_list)   \n",
    "\n",
    "def load_data():\n",
    "    print('Loading data...')\n",
    "    all_runs = []\n",
    "    for run in runs:\n",
    "        print(f\"Loading run {run}\")\n",
    "        try:\n",
    "            curr_run = image.load_img(f\"{raw_dir}/sub-025/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz\")\n",
    "            curr_run = image.get_data(image.clean_img(curr_run, standardize=True, mask_img=whole_brain_mask))\n",
    "            print(f\"Run {run} shape: {curr_run.shape}\")\n",
    "            all_runs.append(curr_run)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading run {run}: {str(e)}\")\n",
    "        print(f\"Memory usage after run {run}: {(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024} MB\")\n",
    "    print('Data loaded. Concatenating...')\n",
    "    if not all_runs:\n",
    "        raise ValueError(\"No valid run data was loaded. Check your input files and paths.\")\n",
    "    bold_vol = np.concatenate(all_runs, axis=3)  # Compile into 4D\n",
    "    del all_runs\n",
    "    print(f\"Concatenated data shape: {bold_vol.shape}\")\n",
    "    print(f\"Final memory usage: {(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024} MB\")\n",
    "    print('Data concatenated...')\n",
    "    gc.collect()\n",
    "    return bold_vol\n",
    "\n",
    "def extract_seed_ts(bold_vol, sub, roi, hemisphere, task='loc', radius=6):\n",
    "    print(\"Extracting seed time series...\")\n",
    "    seed_roi_path = f'{study_dir}/{sub}/ses-01/derivatives/rois/spheres_nifti/{sub}_{roi}_{hemisphere}_{task}_sphere_r{radius}mm.nii.gz'\n",
    "    print(f\"Loading seed ROI from: {seed_roi_path}\")\n",
    "    try:\n",
    "        seed_roi_img = image.load_img(seed_roi_path)\n",
    "        seed_roi = image.get_data(seed_roi_img)\n",
    "        print(f\"Loaded seed ROI shape: {seed_roi.shape}\")\n",
    "        print(f\"Bold volume shape: {bold_vol.shape}\")\n",
    "        if seed_roi.shape[:3] != bold_vol.shape[:3]:\n",
    "            print(\"Warning: Seed ROI shape does not match bold volume shape. Attempting to reshape...\")\n",
    "            seed_roi = image.resample_to_img(seed_roi_img, nib.Nifti1Image(bold_vol, affine), interpolation='nearest').get_fdata()\n",
    "            print(f\"Reshaped seed ROI to: {seed_roi.shape}\")\n",
    "        if len(seed_roi.shape) == 3:\n",
    "            seed_roi = seed_roi[..., np.newaxis]\n",
    "        masked_img = seed_roi * bold_vol\n",
    "        seed_ts = masked_img.reshape(-1, bold_vol.shape[3])\n",
    "        seed_ts = seed_ts[~np.all(seed_ts == 0, axis=1)]\n",
    "        seed_ts = np.transpose(seed_ts)\n",
    "        print(f\"Extracted seed time series shape: {seed_ts.shape}\")\n",
    "        print('Seed data extracted successfully.')\n",
    "        return seed_ts\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_seed_ts: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "bold_vol = load_data()\n",
    "print(f\"bold_vol type: {type(bold_vol)}\")\n",
    "print(f\"bold_vol shape: {bold_vol.shape}\")\n",
    "print(f\"bold_vol dtype: {bold_vol.dtype}\")\n",
    "seed_ts = extract_seed_ts(bold_vol, sub='sub-025', roi='pIPS', hemisphere='left')\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Begin Searchlight\", print((resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024))\n",
    "sl = Searchlight(sl_rad=sl_rad,max_blk_edge=max_blk_edge, shape = shape)\n",
    "print('Distribute', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024)\n",
    "sl.distribute([bold_vol], mask)\n",
    "\n",
    "print('Broadcast', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024)\n",
    "sl.broadcast(seed_ts)\n",
    "print('Run', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024, flush= True)\n",
    "sl_result = sl.run_searchlight(mvpd, pool_size=pool_size)\n",
    "print(\"End Searchlight\\n\", (time.time()-t1)/60)\n",
    "\n",
    "sl_result = sl_result.astype('double')\n",
    "sl_result[np.isnan(sl_result)] = 0\n",
    "sl_nii = nib.Nifti1Image(sl_result, affine)\n",
    "nib.save(sl_nii, f'{out_dir}/{study}_sub-025_pIPS_left_mvpd.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph TD\n",
    "    A[Attention System] --> B[Top-Down Processes]\n",
    "    A --> C[Bottom-Up Processes]\n",
    "    B --> D[Executive Function]\n",
    "    B --> E[Goals]\n",
    "    B --> F[Conscious Allocation]\n",
    "    C --> G[Stimulus Salience]\n",
    "    C --> H[Environmental Factors]\n",
    "    I[Task Difficulty] --> J[Resource Allocation]\n",
    "    K[Individual Differences] --> J\n",
    "    L[Context] --> J\n",
    "    J --> M[Attentional Focus]\n",
    "    D --> M\n",
    "    E --> M\n",
    "    F --> M\n",
    "    G --> M\n",
    "    H --> M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##keep for now I feel like it is close to working\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nilearn import image, input_data\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import sys\n",
    "import nibabel as nib\n",
    "import logging\n",
    "from nilearn.image import new_img_like\n",
    "from nilearn.masking import apply_mask, unmask\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Object'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = sub_info['sub'].tolist()\n",
    "# subs = ['sub-068']  # Uncomment for testing\n",
    "\n",
    "rois = ['pIPS', 'LO']\n",
    "hemispheres = ['left', 'right']\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "def create_searchlight_sphere(center, radius, mask):\n",
    "    sphere = np.zeros(mask.shape)\n",
    "    x, y, z = np.ogrid[:mask.shape[0], :mask.shape[1], :mask.shape[2]]\n",
    "    mask_x, mask_y, mask_z = center\n",
    "    dist_from_center = np.sqrt((x - mask_x)**2 + (y - mask_y)**2 + (z - mask_z)**2)\n",
    "    sphere[dist_from_center <= radius] = 1\n",
    "    return sphere\n",
    "\n",
    "def extract_searchlight_timeseries(img, sphere_mask):\n",
    "    masked_data = apply_mask(img, sphere_mask)\n",
    "    return np.mean(masked_data, axis=1).reshape(-1, 1)\n",
    "\n",
    "def extract_roi_sphere(img, coords):\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius=6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    phys = np.mean(seed_time_series, axis=1).reshape(-1, 1)\n",
    "    return phys  # Return non-standardized time series\n",
    "\n",
    "def make_psy_cov(runs, ss):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    vols_per_run, tr = 184, 2.0\n",
    "    total_vols = vols_per_run * len(runs)\n",
    "    times = np.arange(0, total_vols * tr, tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for i, rn in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]\n",
    "        obj_cov_file = f'{cov_dir}/catloc_{ss_num}_run-0{rn}_{localizer}.txt'\n",
    "\n",
    "        if not os.path.exists(obj_cov_file):\n",
    "            logging.warning(f'Covariate file not found for run {rn}')\n",
    "            continue\n",
    "\n",
    "        obj_cov = pd.read_csv(obj_cov_file, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        if i > 0:\n",
    "            obj_cov['onset'] += i * vols_per_run * tr\n",
    "        \n",
    "        full_cov = pd.concat([full_cov, obj_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset']).reset_index(drop=True)\n",
    "    cov = full_cov.to_numpy()\n",
    "    valid_onsets = cov[:, 0] < times[-1]\n",
    "    cov = cov[valid_onsets]\n",
    "\n",
    "    if cov.shape[0] == 0:\n",
    "        logging.warning('No valid covariate data after filtering. Returning zeros array.')\n",
    "        return np.zeros((total_vols, 1))\n",
    "\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    psy[psy > 0] = 1\n",
    "    psy[psy <= 0] = 0\n",
    "    return psy\n",
    "\n",
    "def extract_cond_ts(ts, cov):\n",
    "    block_ind = (cov==1)\n",
    "    block_ind = np.insert(block_ind, 0, True)\n",
    "    block_ind = np.delete(block_ind, len(block_ind)-1)\n",
    "    block_ind = (cov == 1).reshape((len(cov))) | block_ind\n",
    "    return ts[block_ind]\n",
    "\n",
    "def conduct_gca_searchlight():\n",
    "    logging.info(f'Running GCA with searchlight for {localizer}...')\n",
    "    tasks = ['loc']\n",
    "    \n",
    "    # Load whole-brain mask\n",
    "    whole_brain_mask = nib.load(f'{curr_dir}/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz')\n",
    "    mask_data = whole_brain_mask.get_fdata().astype(bool)\n",
    "    \n",
    "    for ss in subs:\n",
    "        sub_summary = pd.DataFrame(columns=['sub', 'fold', 'task', 'center_x', 'center_y', 'center_z', 'f_diff'])\n",
    "        \n",
    "        sub_dir = f'{study_dir}/{ss}/ses-01/'\n",
    "        temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "        exp_dir = f'{temp_dir}/derivatives/fsl/loc'\n",
    "        output_dir = f'{sub_dir}/derivatives/gca_searchlight'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{sub_dir}/derivatives/rois/spheres/sphere_coords_hemisphere.csv')\n",
    "\n",
    "        for rcn, rc in enumerate(run_combos):\n",
    "            logging.info(f\"Processing run combination {rc} for subject {ss}\")\n",
    "            \n",
    "            filtered_list = []\n",
    "            for rn in rc:\n",
    "                curr_run = nib.load(f'{exp_dir}/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz')\n",
    "                curr_run_data = curr_run.get_fdata()\n",
    "                curr_run_cleaned = image.clean_img(curr_run)\n",
    "                filtered_list.append(curr_run_cleaned)\n",
    "\n",
    "            img4d = image.concat_imgs(filtered_list)\n",
    "            affine = curr_run.affine\n",
    "            logging.info(f\"Concatenated image shape: {img4d.shape}\")\n",
    "\n",
    "            psy = make_psy_cov(rc, ss)\n",
    "            \n",
    "            f_diff_3d = np.zeros(mask_data.shape)\n",
    "            \n",
    "            # Iterate over all voxels in the brain mask\n",
    "            for x, y, z in zip(*np.where(mask_data)):\n",
    "                sphere_mask = create_searchlight_sphere((x, y, z), radius=6, mask=mask_data)\n",
    "                sphere_ts = extract_searchlight_timeseries(img4d, sphere_mask)\n",
    "                \n",
    "                if sphere_ts.shape[0] != psy.shape[0]:\n",
    "                    logging.warning(f\"Mismatch in volumes: sphere_ts has {sphere_ts.shape[0]}, psy has {psy.shape[0]}\")\n",
    "                    continue\n",
    "                \n",
    "                sphere_phys = extract_cond_ts(sphere_ts, psy)\n",
    "                \n",
    "                # Perform GCA between the sphere and a reference region (e.g., pIPS)\n",
    "                pips_coords = roi_coords[(roi_coords['index'] == rcn) & \n",
    "                                         (roi_coords['task'] == 'loc') & \n",
    "                                         (roi_coords['roi'] == 'pIPS') &\n",
    "                                         (roi_coords['hemisphere'] == 'right')]\n",
    "                \n",
    "                if pips_coords.empty:\n",
    "                    logging.warning(f\"No coordinates found for pIPS, run combo {rc}\")\n",
    "                    continue\n",
    "                \n",
    "                pips_ts = extract_roi_sphere(img4d, pips_coords[['x', 'y', 'z']].values.tolist()[0])\n",
    "                pips_phys = extract_cond_ts(pips_ts, psy)\n",
    "                \n",
    "                neural_ts = pd.DataFrame({\n",
    "                    'sphere': sphere_phys.ravel(),\n",
    "                    'pips': pips_phys.ravel()\n",
    "                })\n",
    "                \n",
    "                gc_res_sphere = grangercausalitytests(neural_ts[['pips', 'sphere']], 1, verbose=False)\n",
    "                gc_res_pips = grangercausalitytests(neural_ts[['sphere', 'pips']], 1, verbose=False)\n",
    "                \n",
    "                f_diff = gc_res_sphere[1][0]['ssr_ftest'][0] - gc_res_pips[1][0]['ssr_ftest'][0]\n",
    "                \n",
    "                f_diff_3d[x, y, z] = f_diff\n",
    "                \n",
    "                curr_data = pd.Series([ss, rcn, 'loc', x, y, z, f_diff], index=sub_summary.columns)\n",
    "                sub_summary = sub_summary.append(curr_data, ignore_index=True)\n",
    "        \n",
    "        logging.info(f'Completed GCA searchlight for subject {ss}')\n",
    "        sub_summary.to_csv(f'{output_dir}/gca_searchlight_summary_{localizer.lower()}.csv', index=False)\n",
    "        \n",
    "        # Save the 3D nifti image of f_diff values\n",
    "        f_diff_3d = f_diff_3d.astype('float64')  # Convert to double precision\n",
    "        f_diff_3d[np.isnan(f_diff_3d)] = 0  # Replace NaNs with zeros\n",
    "        f_diff_img = nib.Nifti1Image(f_diff_3d, affine)\n",
    "        nib.save(f_diff_img, f'{output_dir}/gca_searchlight_f_diff_{localizer.lower()}.nii.gz')\n",
    "\n",
    "def summarize_gca_searchlight():\n",
    "    logging.info('Creating summary across subjects for searchlight GCA...')\n",
    "    \n",
    "    all_subjects_data = []\n",
    "    \n",
    "    for ss in subs:\n",
    "        sub_dir = f'{study_dir}/{ss}/ses-01/'\n",
    "        data_dir = f'{sub_dir}/derivatives/gca_searchlight'\n",
    "        \n",
    "        curr_df = pd.read_csv(f'{data_dir}/gca_searchlight_summary_{localizer.lower()}.csv')\n",
    "        curr_df['sub'] = ss\n",
    "        all_subjects_data.append(curr_df)\n",
    "    \n",
    "    df_all = pd.concat(all_subjects_data, ignore_index=True)\n",
    "    \n",
    "    # Calculate mean and std of f_diff across subjects for each voxel\n",
    "    df_summary = df_all.groupby(['center_x', 'center_y', 'center_z'])['f_diff'].agg(['mean', 'std']).reset_index()\n",
    "    df_summary.columns = ['x', 'y', 'z', 'mean_f_diff', 'std_f_diff']\n",
    "    \n",
    "    output_dir = f\"{results_dir}/gca_searchlight\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    summary_file = f\"{output_dir}/all_subjects_gca_searchlight_summary_{localizer.lower()}.csv\"\n",
    "    df_summary.to_csv(summary_file, index=False)\n",
    "    \n",
    "    logging.info(f'Summary across subjects completed and saved to {summary_file}')\n",
    "    print(df_summary.head())\n",
    "    \n",
    "    # Create and save a 3D nifti image of mean f_diff values\n",
    "    whole_brain_mask = nib.load(f'{curr_dir}/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz')\n",
    "    mean_f_diff_3d = np.zeros(whole_brain_mask.shape)\n",
    "    for _, row in df_summary.iterrows():\n",
    "        mean_f_diff_3d[int(row['x']), int(row['y']), int(row['z'])] = row['mean_f_diff']\n",
    "    \n",
    "    mean_f_diff_img = new_img_like(whole_brain_mask, mean_f_diff_3d)\n",
    "    nib.save(mean_f_diff_img, f'{output_dir}/gca_searchlight_mean_f_diff_{localizer.lower()}.nii.gz')\n",
    "    \n",
    "    return df_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conduct_gca_searchlight()\n",
    "    #summarize_gca_searchlight()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
