{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import resource\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from nilearn import image, datasets\n",
    "import nibabel as nib\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "print('libraries loaded...')\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = ['sub-025']  # Uncomment for testing\n",
    "dorsal = ['pIPS'] # Run for one ROI initially\n",
    "\n",
    "print(subs, dorsal)\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Object'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = \"/lab_data/behrmannlab/vlad/hemispace\"\n",
    "exp = 'loc' \n",
    "\n",
    "out_dir = f'{study_dir}/derivatives/fc'\n",
    "sub_dir = f'{study_dir}/sub-025/ses-01/'\n",
    "cov_dir = f'{raw_dir}/covs'\n",
    "roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "exp_dir = f'{sub_dir}/derivatives/fsl/{exp}'\n",
    "\n",
    "runs = list(range(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the one we're editing\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import resource\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from nilearn import image, datasets\n",
    "import nibabel as nib\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "'''\n",
    "# Add this near the top of your script, after imports\n",
    "gca_counter = 0\n",
    "\n",
    "def transform_mask_to_native(subject_func, standard_mask, output_dir):\n",
    "    \"\"\"\n",
    "    Transform the standard space whole brain mask to the subject's native space.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    func_img = image.load_img(subject_func)\n",
    "    mask_img = image.load_img(standard_mask)\n",
    "    \n",
    "    native_mask = image.resample_to_img(mask_img, func_img, interpolation='nearest')\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'whole_brain_mask_native.nii.gz')\n",
    "    native_mask.to_filename(output_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def extract_pc(data, n_components=None):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "def calc_pc_n(pca, thresh):\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    var = 0\n",
    "    for n_comp, ev in enumerate(explained_variance):\n",
    "        var += ev\n",
    "        if var >= thresh:\n",
    "            break\n",
    "    return n_comp+1\n",
    "\n",
    "def calc_mvc(seed_train, seed_test, target_train, target_test, target_pc):\n",
    "    all_corrs = []\n",
    "    for pcn in range(0, len(target_pc.explained_variance_ratio_)):\n",
    "        clf.fit(seed_train, target_train[:,pcn])\n",
    "        pred_ts = clf.predict(seed_test)\n",
    "        weighted_corr = np.corrcoef(pred_ts, target_test[:,pcn])[0,1] * target_pc.explained_variance_ratio_[pcn]\n",
    "        all_corrs.append(weighted_corr)\n",
    "    final_corr = np.sum(all_corrs) / (np.sum(target_pc.explained_variance_ratio_))\n",
    "    return final_corr\n",
    "\n",
    "def create_ts_mask(train, test):\n",
    "    train_index = []\n",
    "    test_index = []\n",
    "    for tr in train:\n",
    "        train_index = train_index + list(range((tr-1) * (vols-first_fix), ((tr-1) * (vols-first_fix)) + (vols-first_fix)))\n",
    "    for te in test:\n",
    "        test_index = test_index + list(range((te-1) * (vols-first_fix), ((te-1) * (vols-first_fix)) + (vols-first_fix)))\n",
    "    return train_index, test_index\n",
    "\n",
    "def gca_searchlight(data, sl_mask, myrad, seed_ts):\n",
    "    global gca_counter\n",
    "    gca_counter += 1\n",
    "    \n",
    "    print(f\"GCA analysis {gca_counter}: Starting...\")\n",
    "    target_data = data[0]\n",
    "    target_voxels = np.sum(sl_mask != 0)\n",
    "    print(f\"Voxels in target sphere: {target_voxels}\")\n",
    "    \n",
    "    # Reshape target data to 2D: (time_points, voxels)\n",
    "    target_ts = target_data.reshape(-1, target_data.shape[-1]).T\n",
    "    \n",
    "    # Average time series across voxels in the target sphere\n",
    "    target_mean_ts = np.mean(target_ts, axis=1)\n",
    "    \n",
    "    try:\n",
    "        neural_ts = pd.DataFrame({'seed': seed_ts, 'target': target_mean_ts})\n",
    "        \n",
    "        # Check for constant columns and add small random noise if necessary\n",
    "        for col in neural_ts.columns:\n",
    "            if neural_ts[col].std() == 0:\n",
    "                neural_ts[col] += np.random.normal(0, 1e-10, len(neural_ts[col]))\n",
    "        \n",
    "        gc_res_seed_to_target = grangercausalitytests(neural_ts[['seed', 'target']], 1, verbose=False)\n",
    "        gc_res_target_to_seed = grangercausalitytests(neural_ts[['target', 'seed']], 1, verbose=False)\n",
    "        f_diff = gc_res_seed_to_target[1][0]['ssr_ftest'][0] - gc_res_target_to_seed[1][0]['ssr_ftest'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GCA analysis {gca_counter}: {str(e)}\")\n",
    "        f_diff = 0\n",
    "    \n",
    "    print(f\"GCA analysis {gca_counter}: Completed\")\n",
    "    return f_diff\n",
    "\n",
    "def load_data():\n",
    "    print('Loading data...')\n",
    "    all_runs = []\n",
    "    for run in runs:\n",
    "        print(f\"Loading run {run}\")\n",
    "        try:\n",
    "            curr_run = image.load_img(f\"{raw_dir}/sub-025/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz\")\n",
    "            curr_run = image.get_data(image.clean_img(curr_run, standardize=True, mask_img=whole_brain_mask))\n",
    "            print(f\"Run {run} shape: {curr_run.shape}\")\n",
    "            all_runs.append(curr_run)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading run {run}: {str(e)}\")\n",
    "        print(f\"Memory usage after run {run}: {(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024} MB\")\n",
    "    print('Data loaded. Concatenating...')\n",
    "    if not all_runs:\n",
    "        raise ValueError(\"No valid run data was loaded. Check your input files and paths.\")\n",
    "    bold_vol = np.concatenate(all_runs, axis=3)  # Compile into 4D\n",
    "    del all_runs\n",
    "    print(f\"Concatenated data shape: {bold_vol.shape}\")\n",
    "    print(f\"Final memory usage: {(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024} MB\")\n",
    "    print('Data concatenated...')\n",
    "    gc.collect()\n",
    "    return bold_vol\n",
    "\n",
    "def extract_seed_ts(bold_vol, sub, roi, hemisphere, task='loc', radius=6):\n",
    "    print(\"Extracting seed time series...\")\n",
    "    seed_roi_path = f'{study_dir}/{sub}/ses-01/derivatives/rois/spheres_nifti/{sub}_{roi}_{hemisphere}_{task}_sphere_r{radius}mm.nii.gz'\n",
    "    print(f\"Loading seed ROI from: {seed_roi_path}\")\n",
    "    try:\n",
    "        seed_roi_img = image.load_img(seed_roi_path)\n",
    "        seed_roi = image.get_data(seed_roi_img)\n",
    "        print(f\"Loaded seed ROI shape: {seed_roi.shape}\")\n",
    "        print(f\"Bold volume shape: {bold_vol.shape}\")\n",
    "        if seed_roi.shape[:3] != bold_vol.shape[:3]:\n",
    "            print(\"Warning: Seed ROI shape does not match bold volume shape. Attempting to reshape...\")\n",
    "            seed_roi = image.resample_to_img(seed_roi_img, nib.Nifti1Image(bold_vol[:,:,:,0], affine), interpolation='nearest').get_fdata()\n",
    "            print(f\"Reshaped seed ROI to: {seed_roi.shape}\")\n",
    "        \n",
    "        # Expand seed_roi to match bold_vol dimensions\n",
    "        seed_roi_4d = np.repeat(seed_roi[:,:,:,np.newaxis], bold_vol.shape[-1], axis=3)\n",
    "        \n",
    "        masked_img = seed_roi_4d * bold_vol\n",
    "        seed_ts = masked_img.reshape(-1, bold_vol.shape[-1])\n",
    "        seed_ts = seed_ts[~np.all(seed_ts == 0, axis=1)]\n",
    "        seed_ts = np.mean(seed_ts, axis=0)  # Average across voxels in the seed ROI\n",
    "        print(f\"Extracted seed time series shape: {seed_ts.shape}\")\n",
    "        print('Seed data extracted successfully.')\n",
    "        return seed_ts\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_seed_ts: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Main execution\n",
    "\n",
    "standard_mask_path = '/user_data/csimmon2/git_repos/ptoc/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz'\n",
    "native_mask_path = transform_mask_to_native(\n",
    "    f'{exp_dir}/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz',\n",
    "    standard_mask_path,\n",
    "    f'{sub_dir}/derivatives/masks'\n",
    ")\n",
    "whole_brain_mask = image.load_img(native_mask_path)\n",
    "\n",
    "affine = whole_brain_mask.affine\n",
    "dimsize = whole_brain_mask.header.get_zooms()  #get dimensions\n",
    "\n",
    "# scan parameters\n",
    "vols = 184\n",
    "first_fix = 0\n",
    "\n",
    "# threshold for PCA\n",
    "pc_thresh = .9\n",
    "\n",
    "clf = LinearRegression()\n",
    "rs = ShuffleSplit(n_splits=5, test_size=1/3, random_state=0)\n",
    "\n",
    "print('Searchlight setup ...')\n",
    "mask = image.get_data(whole_brain_mask) #the mask to search within\n",
    "\n",
    "# Searchlight parameters\n",
    "sl_rad = 2\n",
    "max_blk_edge = 10\n",
    "pool_size = 1\n",
    "shape = Ball\n",
    "\n",
    "print(f\"Searchlight parameters:\")\n",
    "print(f\"Radius: {sl_rad} voxels\")\n",
    "print(f\"Max block edge: {max_blk_edge}\")\n",
    "print(f\"Pool size: {pool_size}\")\n",
    "print(f\"Shape: {shape.__name__}\")\n",
    "\n",
    "bold_vol = load_data()\n",
    "print(f\"bold_vol type: {type(bold_vol)}\")\n",
    "print(f\"bold_vol shape: {bold_vol.shape}\")\n",
    "print(f\"bold_vol dtype: {bold_vol.dtype}\")\n",
    "seed_ts = extract_seed_ts(bold_vol, sub='sub-025', roi='pIPS', hemisphere='left')\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Begin Searchlight\", print((resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024))\n",
    "sl = Searchlight(sl_rad=sl_rad, max_blk_edge=max_blk_edge, shape=shape)\n",
    "print('Distribute', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024)\n",
    "sl.distribute([bold_vol], mask)\n",
    "\n",
    "print('Broadcast', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024)\n",
    "sl.broadcast(seed_ts)\n",
    "print('Run', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024, flush=True)\n",
    "sl_result = sl.run_searchlight(gca_searchlight, pool_size=pool_size)\n",
    "print(\"End Searchlight\\n\", (time.time()-t1)/60)\n",
    "\n",
    "print(f\"Searchlight result shape: {sl_result.shape}\")\n",
    "print(f\"Non-zero voxels in result: {np.sum(sl_result != 0)}\")\n",
    "print(f\"Total GCA analyses performed: {gca_counter}\")\n",
    "\n",
    "sl_result = sl_result.astype('double')\n",
    "sl_result[np.isnan(sl_result)] = 0\n",
    "sl_nii = nib.Nifti1Image(sl_result, affine)\n",
    "nib.save(sl_nii, f'{out_dir}/{study}_sub-025_pIPS_left_gca.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searchlight GCA \n",
    "#fix # runs, hemi, rois, subs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import resource\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from nilearn import image, datasets\n",
    "import nibabel as nib\n",
    "from brainiak.searchlight.searchlight import Searchlight, Ball\n",
    "\n",
    "def transform_mask_to_native(subject_func, standard_mask, output_dir):\n",
    "    \"\"\"\n",
    "    Transform the standard space whole brain mask to the subject's native space.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    func_img = image.load_img(subject_func)\n",
    "    mask_img = image.load_img(standard_mask)\n",
    "    \n",
    "    native_mask = image.resample_to_img(mask_img, func_img, interpolation='nearest')\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'whole_brain_mask_native.nii.gz')\n",
    "    native_mask.to_filename(output_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Import your parameters\n",
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "sys.path.insert(0, curr_dir)\n",
    "import ptoc_params as params\n",
    "\n",
    "print('libraries loaded...')\n",
    "\n",
    "# Load subject information\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "sub_info = sub_info[sub_info['group'] == 'control']\n",
    "subs = ['sub-025']  # Uncomment for testing\n",
    "dorsal = ['pIPS'] # Run for one ROI initially\n",
    "\n",
    "print(subs, dorsal)\n",
    "\n",
    "# Set up directories and parameters\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "localizer = 'Object'  # scramble or object. This is the localizer task.\n",
    "results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "raw_dir = \"/lab_data/behrmannlab/vlad/hemispace\"\n",
    "exp = 'loc' \n",
    "\n",
    "out_dir = f'{study_dir}/derivatives/fc'\n",
    "sub_dir = f'{study_dir}/sub-025/ses-01/'\n",
    "cov_dir = f'{raw_dir}/covs'\n",
    "roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "exp_dir = f'{sub_dir}/derivatives/fsl/{exp}'\n",
    "\n",
    "runs = list(range(1,3))\n",
    "\n",
    "standard_mask_path = '/user_data/csimmon2/git_repos/ptoc/roiParcels/mruczek_parcels/binary/all_visual_areas.nii.gz'\n",
    "native_mask_path = transform_mask_to_native(\n",
    "    f'{exp_dir}/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz',\n",
    "    standard_mask_path,\n",
    "    f'{sub_dir}/derivatives/masks'\n",
    ")\n",
    "whole_brain_mask = image.load_img(native_mask_path)\n",
    "\n",
    "affine = whole_brain_mask.affine\n",
    "dimsize = whole_brain_mask.header.get_zooms()  #get dimensions\n",
    "\n",
    "# scan parameters\n",
    "vols = 184\n",
    "first_fix = 0\n",
    "\n",
    "# threshold for PCA\n",
    "pc_thresh = .9\n",
    "\n",
    "clf = LinearRegression()\n",
    "rs = ShuffleSplit(n_splits=5, test_size=1/3, random_state=0)\n",
    "\n",
    "print('Searchlight setup ...')\n",
    "mask = image.get_data(whole_brain_mask) #the mask to search within\n",
    "\n",
    "sl_rad = 2 #radius of searchlight sphere (in voxels)\n",
    "max_blk_edge = 10 #how many blocks to send on each parallelized search\n",
    "pool_size = 1 #number of cores to work on each search\n",
    "\n",
    "voxels_proportion=1\n",
    "shape = Ball\n",
    "\n",
    "def extract_pc(data, n_components=None):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "def calc_pc_n(pca, thresh):\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    var = 0\n",
    "    for n_comp, ev in enumerate(explained_variance):\n",
    "        var += ev\n",
    "        if var >= thresh:\n",
    "            break\n",
    "    return n_comp+1\n",
    "\n",
    "def calc_mvc(seed_train,seed_test, target_train, target_test, target_pc):\n",
    "    all_corrs = []\n",
    "    for pcn in range(0,len(target_pc.explained_variance_ratio_)):\n",
    "        clf.fit(seed_train, target_train[:,pcn])\n",
    "        pred_ts = clf.predict(seed_test)\n",
    "        weighted_corr = np.corrcoef(pred_ts,target_test[:,pcn])[0,1] * target_pc.explained_variance_ratio_[pcn]\n",
    "        all_corrs.append(weighted_corr)\n",
    "    final_corr = np.sum(all_corrs)/(np.sum(target_pc.explained_variance_ratio_))\n",
    "    return final_corr\n",
    "\n",
    "def create_ts_mask(train, test):\n",
    "    train_index = []\n",
    "    test_index = []\n",
    "    for tr in train:\n",
    "        train_index = train_index + list(range((tr-1) * (vols-first_fix),((tr-1) * (vols-first_fix)) + (vols-first_fix)))\n",
    "    for te in test:\n",
    "        test_index = test_index + list(range((te-1) * (vols-first_fix),((te-1) * (vols-first_fix)) + (vols-first_fix)))\n",
    "    return train_index, test_index\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def gca_searchlight(data, sl_mask, myrad, seed_ts):\n",
    "    data4D = data[0]\n",
    "    data4D = data4D.reshape(-1, data4D.shape[-1]).T  # Reshape to 2D: (time_points, voxels)\n",
    "    gca_results = []\n",
    "    for train_runs, test_runs in rs.split(runs): \n",
    "        train_index, test_index = create_ts_mask(train_runs, test_runs)\n",
    "        seed_train = seed_ts[train_index]\n",
    "        target_train = data4D[train_index, :]\n",
    "        \n",
    "        # Perform GCA\n",
    "        f_diffs = []\n",
    "        for voxel in range(target_train.shape[1]):\n",
    "            voxel_ts = target_train[:, voxel]\n",
    "            \n",
    "            # Check if either time series is constant\n",
    "            if np.all(seed_train == seed_train[0]) or np.all(voxel_ts == voxel_ts[0]):\n",
    "                f_diffs.append(0)  # Assign a neutral value for constant time series\n",
    "                continue\n",
    "            \n",
    "            # Check for sufficient variation\n",
    "            if np.std(seed_train) < 1e-6 or np.std(voxel_ts) < 1e-6:\n",
    "                f_diffs.append(0)  # Assign a neutral value for low-variance time series\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                neural_ts = pd.DataFrame({'seed': seed_train, 'target': voxel_ts})\n",
    "                gc_res_seed_to_target = grangercausalitytests(neural_ts[['seed', 'target']], 1, verbose=False)\n",
    "                gc_res_target_to_seed = grangercausalitytests(neural_ts[['target', 'seed']], 1, verbose=False)\n",
    "                f_diff = gc_res_seed_to_target[1][0]['ssr_ftest'][0] - gc_res_target_to_seed[1][0]['ssr_ftest'][0]\n",
    "                f_diffs.append(f_diff)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in GCA for voxel {voxel}: {str(e)}\")\n",
    "                f_diffs.append(0)  # Assign a neutral value for failed tests\n",
    "        \n",
    "        # Average F-difference across voxels in the searchlight sphere\n",
    "        gca_results.append(np.mean(f_diffs))\n",
    "    \n",
    "    return np.mean(gca_results)\n",
    "\n",
    "def load_data():\n",
    "    print('Loading data...')\n",
    "    all_runs = []\n",
    "    for run in runs:\n",
    "        print(f\"Loading run {run}\")\n",
    "        try:\n",
    "            curr_run = image.load_img(f\"{raw_dir}/sub-025/ses-01/derivatives/fsl/loc/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz\")\n",
    "            curr_run = image.get_data(image.clean_img(curr_run, standardize=True, mask_img=whole_brain_mask))\n",
    "            print(f\"Run {run} shape: {curr_run.shape}\")\n",
    "            all_runs.append(curr_run)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading run {run}: {str(e)}\")\n",
    "        print(f\"Memory usage after run {run}: {(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024} MB\")\n",
    "    print('Data loaded. Concatenating...')\n",
    "    if not all_runs:\n",
    "        raise ValueError(\"No valid run data was loaded. Check your input files and paths.\")\n",
    "    bold_vol = np.concatenate(all_runs, axis=3)  # Compile into 4D\n",
    "    del all_runs\n",
    "    print(f\"Concatenated data shape: {bold_vol.shape}\")\n",
    "    print(f\"Final memory usage: {(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024} MB\")\n",
    "    print('Data concatenated...')\n",
    "    gc.collect()\n",
    "    return bold_vol\n",
    "\n",
    "def extract_seed_ts(bold_vol, sub, roi, hemisphere, task='loc', radius=6):\n",
    "    print(\"Extracting seed time series...\")\n",
    "    seed_roi_path = f'{study_dir}/{sub}/ses-01/derivatives/rois/spheres_nifti/{sub}_{roi}_{hemisphere}_{task}_sphere_r{radius}mm.nii.gz'\n",
    "    print(f\"Loading seed ROI from: {seed_roi_path}\")\n",
    "    try:\n",
    "        seed_roi_img = image.load_img(seed_roi_path)\n",
    "        seed_roi = image.get_data(seed_roi_img)\n",
    "        print(f\"Loaded seed ROI shape: {seed_roi.shape}\")\n",
    "        print(f\"Bold volume shape: {bold_vol.shape}\")\n",
    "        if seed_roi.shape[:3] != bold_vol.shape[:3]:\n",
    "            print(\"Warning: Seed ROI shape does not match bold volume shape. Attempting to reshape...\")\n",
    "            seed_roi = image.resample_to_img(seed_roi_img, nib.Nifti1Image(bold_vol[:,:,:,0], affine), interpolation='nearest').get_fdata()\n",
    "            print(f\"Reshaped seed ROI to: {seed_roi.shape}\")\n",
    "        \n",
    "        # Expand seed_roi to match bold_vol dimensions\n",
    "        seed_roi_4d = np.repeat(seed_roi[:,:,:,np.newaxis], bold_vol.shape[-1], axis=3)\n",
    "        \n",
    "        masked_img = seed_roi_4d * bold_vol\n",
    "        seed_ts = masked_img.reshape(-1, bold_vol.shape[-1])\n",
    "        seed_ts = seed_ts[~np.all(seed_ts == 0, axis=1)]\n",
    "        seed_ts = np.mean(seed_ts, axis=0)  # Average across voxels in the seed ROI\n",
    "        print(f\"Extracted seed time series shape: {seed_ts.shape}\")\n",
    "        print('Seed data extracted successfully.')\n",
    "        return seed_ts\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_seed_ts: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "bold_vol = load_data()\n",
    "print(f\"bold_vol type: {type(bold_vol)}\")\n",
    "print(f\"bold_vol shape: {bold_vol.shape}\")\n",
    "print(f\"bold_vol dtype: {bold_vol.dtype}\")\n",
    "seed_ts = extract_seed_ts(bold_vol, sub='sub-025', roi='pIPS', hemisphere='left')\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Begin Searchlight\", print((resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024))\n",
    "sl = Searchlight(sl_rad=sl_rad,max_blk_edge=max_blk_edge, shape = shape)\n",
    "print('Distribute', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024)\n",
    "sl.distribute([bold_vol], mask)\n",
    "\n",
    "print('Broadcast', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024)\n",
    "sl.broadcast(seed_ts)\n",
    "print('Run', (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)/1024, flush= True)\n",
    "sl_result = sl.run_searchlight(gca_searchlight, pool_size=pool_size)\n",
    "print(\"End Searchlight\\n\", (time.time()-t1)/60)\n",
    "\n",
    "sl_result = sl_result.astype('double')\n",
    "sl_result[np.isnan(sl_result)] = 0\n",
    "sl_nii = nib.Nifti1Image(sl_result, affine)\n",
    "nib.save(sl_nii, f'{out_dir}/{study}_sub-025_pIPS_left_gca.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
