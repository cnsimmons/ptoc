{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,curr_dir)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "import statsmodels.api as s\n",
    "from sklearn import metrics\n",
    "\n",
    "import pdb\n",
    "import ptoc_params as params\n",
    "\n",
    "from plotnine import *\n",
    "#from plotnine import ggplot, aes, geom_point\n",
    "\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#load additional libraries\n",
    "from nilearn import image, plotting, input_data, glm\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import nibabel as nib\n",
    "import statsmodels.api as sm\n",
    "from nilearn.datasets import load_mni152_brain_mask, load_mni152_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = params.data_dir\n",
    "results_dir = params.results_dir\n",
    "fig_dir = params.fig_dir\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "sub_info = params.sub_info\n",
    "task_info = params.task_info\n",
    "\n",
    "suf = params.suf\n",
    "rois = params.rois\n",
    "hemis = params.hemis\n",
    "\n",
    "#load subject info\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "\n",
    "#mni = load_mni152_brain_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''exp info'''\n",
    "subs = ['sub-064']  # Run for one subject initially\n",
    "#subs = sub_info['sub'].tolist()\n",
    "\n",
    "#Just controls\n",
    "subs = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "study = 'ptoc'\n",
    "data_dir = 'hemispace'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "out_dir = f'{study_dir}/derivatives/fc'\n",
    "results_dir = '/user_data/csimmon2/GitHub_Repos/ptoc/results'\n",
    "exp = ''\n",
    "rois = ['LO']  # Run for one ROI initially\n",
    "control_tasks = ['loc']\n",
    "file_suf = ''\n",
    "\n",
    "'''scan params'''\n",
    "#tr = 1 #in the original code\n",
    "#vols = 321 #in the original code\n",
    "\n",
    "tr = 2 #ptoc_params\n",
    "vols = 184 #ptoc_params\n",
    "\n",
    "whole_brain_mask = load_mni152_brain_mask()\n",
    "mni = load_mni152_template()\n",
    "brain_masker = NiftiMasker(whole_brain_mask, smoothing_fwhm=0, standardize=True)\n",
    "\n",
    "'''run info'''\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = []\n",
    "\n",
    "for rn1 in range(1, run_num + 1):\n",
    "    for rn2 in range(rn1 + 1, run_num + 1):\n",
    "        run_combos.append([rn1, rn2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extract ROI coordinates\n",
    "def extract_roi_coords():\n",
    "    \"\"\"\n",
    "    Define ROIs\n",
    "    \"\"\"\n",
    "    parcels = ['V1', 'aIPS', 'PFS', 'pIPS', 'LO']\n",
    "    subs = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "\n",
    "    for ss in subs:\n",
    "        print(f'Processing subject: {ss}')\n",
    "        sub_dir = f'{study_dir}/{ss}/ses-01'\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "        os.makedirs(f'{roi_dir}/spheres', exist_ok=True)\n",
    "        \n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl'\n",
    "        parcel_dir = f'{roi_dir}/parcels'\n",
    "        roi_coords = pd.DataFrame(columns=['index', 'task', 'roi', 'x', 'y', 'z'])\n",
    "        \n",
    "        for rcn, rc in enumerate(run_combos):\n",
    "            roi_runs = [ele for ele in runs if ele not in rc]\n",
    "            \n",
    "            #load each run\n",
    "            all_runs = []\n",
    "            for rn in roi_runs:\n",
    "                curr_run_path = f'{exp_dir}/loc/run-0{rn}/1stLevel.feat/stats/zstat3_reg.nii.gz'\n",
    "                if os.path.exists(curr_run_path):\n",
    "                    curr_run = image.load_img(curr_run_path)\n",
    "                    all_runs.append(curr_run)\n",
    "                else:\n",
    "                    print(f'File does not exist: {curr_run_path}')\n",
    "            \n",
    "            mean_zstat = image.mean_img(all_runs)\n",
    "            affine = mean_zstat.affine\n",
    "\n",
    "            for pr in parcels:\n",
    "                roi_path = f'{parcel_dir}/{pr}.nii.gz'\n",
    "                if os.path.exists(roi_path):\n",
    "                    roi = image.load_img(roi_path)\n",
    "                    roi = image.math_img('img > 0', img=roi)\n",
    "\n",
    "                    coords = plotting.find_xyz_cut_coords(mean_zstat, mask_img=roi, activation_threshold=0.99)\n",
    "                    \n",
    "                    masked_stat = image.math_img('img1 * img2', img1=roi, img2=mean_zstat)\n",
    "                    masked_stat = image.get_data(masked_stat)\n",
    "                    np_coords = np.where(masked_stat == np.max(masked_stat))\n",
    "                    \n",
    "                    curr_coords = pd.Series([rcn, 'loc', pr] + coords, index=roi_coords.columns)\n",
    "                    roi_coords = roi_coords.append(curr_coords, ignore_index=True)\n",
    "\n",
    "                    # control task ROI\n",
    "                    control_zstat_path = f'{exp_dir}/loc/HighLevel.gfeat/cope3.feat/stats/zstat1.nii.gz'\n",
    "                    if os.path.exists(control_zstat_path):\n",
    "                        control_zstat = image.load_img(control_zstat_path)\n",
    "                        coords = plotting.find_xyz_cut_coords(control_zstat, mask_img=roi, activation_threshold=0.99)\n",
    "                        \n",
    "                        curr_coords = pd.Series([rcn, 'highlevel', pr] + coords, index=roi_coords.columns)\n",
    "                        roi_coords = roi_coords.append(curr_coords, ignore_index=True)\n",
    "                    else:\n",
    "                        print(f'File does not exist: {control_zstat_path}')\n",
    "                else:\n",
    "                    print(f'File does not exist: {roi_path}')\n",
    "            \n",
    "        roi_coords.to_csv(f'{roi_dir}/spheres/sphere_coords.csv', index=False)\n",
    "\n",
    "# Call the function\n",
    "#extract_roi_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ROI Sphere and Make Psy Cov\n",
    "\n",
    "#PHYS\n",
    "def extract_roi_sphere(img, coords):\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius = 6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    \n",
    "    phys = np.mean(seed_time_series, axis= 1)\n",
    "    #phys = (phys - np.mean(phys)) / np.std(phys) #TRY WITHOUT STANDARDIZING AT SOME POINT\n",
    "    phys = phys.reshape((phys.shape[0],1))\n",
    "    \n",
    "    return phys\n",
    "\n",
    "#PSY\n",
    "def make_psy_cov(runs,ss):\n",
    "    #rois = ['LO']\n",
    "    #tsk = 'loc'\n",
    "    #rr = 'LO'\n",
    "    #ss = '064'\n",
    "    #runs = [1,2,3]\n",
    "    vols = 184\n",
    "    tr = 2\n",
    "    \n",
    "    raw_dir = params.raw_dir\n",
    "    temp_dir = f'{raw_dir}/sub-{ss}/ses-01' #raw_dir is from hemispace\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    \n",
    "    times = np.arange(0, vols*len(runs), tr)\n",
    "    full_cov = pd.DataFrame(columns = ['onset','duration', 'value'])\n",
    "    \n",
    "    for rn, run in enumerate(runs):    \n",
    "        \n",
    "        curr_cov = pd.read_csv(f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt', sep = '\\t', header = None, names = ['onset','duration', 'value'])\n",
    "        curr_cov_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt'\n",
    "        print(f'Loaded curr_cov from: {curr_cov_path}')\n",
    "        print(curr_cov)\n",
    "        #contrasting (neg) cov\n",
    "\n",
    "        curr_cont = pd.read_csv(f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt', sep = '\\t', header =None, names =['onset','duration', 'value'])\n",
    "        curr_cont_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt'\n",
    "        print(f'Loaded curr_cont from: {curr_cont_path}')\n",
    "        print(curr_cont)\n",
    "        curr_cont.iloc[:,2] = curr_cont.iloc[:,2] *-1 #make contrasting cov neg\n",
    "        \n",
    "        curr_cov = curr_cov.append(curr_cont) #append to positive\n",
    "\n",
    "        curr_cov['onset'] = curr_cov['onset'] + (vols*rn) ##THIS IS THE PROBLEM I THINK\n",
    "        #curr_cov['onset'] = curr_cov['onset'] + ((rn_n)*vols) ##alternative\n",
    "        full_cov = full_cov.append(curr_cov)\n",
    "        #add number of vols to the timing cols based on what run you are on\n",
    "        #e.g., for run 1, add 0, for run 2, add 321\n",
    "        #curr_cov['onset'] = curr_cov['onset'] + ((rn_n)*vols) \n",
    "        \n",
    "        \n",
    "        #append to concatenated cov\n",
    "    full_cov = full_cov.sort_values(by =['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "\n",
    "    #convolve to hrf\n",
    "    psy, name = glm.first_level.compute_regressor(cov.T, 'spm', times)\n",
    "        \n",
    "\n",
    "    return psy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONDUCT PPI - repeat memory issues\n",
    "#rois = ['LO']\n",
    "#rr = 'LO'\n",
    "#runs = [1] #maybe it is supposed to by [0,1,2]?\n",
    "#rn = 1\n",
    "subs = ['064']\n",
    "ss = ['064']\n",
    "tsk = 'loc'\n",
    "tr = 2\n",
    "vols = 184\n",
    "\n",
    "def conduct_ppi():\n",
    "    for ss in subs:\n",
    "        print(ss)\n",
    "        sub_dir = f'{study_dir}/sub-{ss}/ses-01/' #study is PTOC\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois' #rois in PTOC\n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl/{exp}' #PTOC\n",
    "        \n",
    "        raw_dir = params.raw_dir\n",
    "        temp_dir = f'{raw_dir}/sub-{ss}/ses-01' #hemispace \n",
    "        cov_dir = f'{temp_dir}/covs' #hemispace\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords.csv') #load ROI coordinates\n",
    "                                \n",
    "        for rr in rois:\n",
    "            all_runs = [] #this will get filled with the data from each run\n",
    "            \n",
    "            for rcn, rc in enumerate(run_combos): #determine which runs to use\n",
    "                curr_coords = roi_coords[(roi_coords['index'] == rcn) & (roi_coords['task'] ==tsk) & (roi_coords['roi'] ==rr)] \n",
    "                filtered_list = []\n",
    "                for rn in rc:\n",
    "                    print (rn)\n",
    "                    run_path = f'{temp_dir}/derivatives/fsl/loc/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "\n",
    "                    if os.path.exists(run_path):\n",
    "                        curr_run = image.load_img(run_path) #load image data\n",
    "                        affine = curr_run.affine\n",
    "                        curr_run = image.clean_img(curr_run, standardize=True)\n",
    "                        filtered_list.append(curr_run)\n",
    "                        print(f'Loaded {run_path}')\n",
    "                    else:\n",
    "                        print(f\"File {run_path} does not exist.\")\n",
    "                    \n",
    "                img4d = image.concat_imgs(filtered_list)\n",
    "                phys = extract_roi_sphere(img4d,curr_coords[['x','y','z']].values.tolist()[0]) #clarify which coords \n",
    "                \n",
    "                #load behavioral data\n",
    "                psy = make_psy_cov(rc, ss) #load psy covariates\n",
    "                \n",
    "                #combine phys (seed TS) and psy (task TS) into a regressor \n",
    "                confounds = pd.DataFrame(columns =['psy', 'phys'])\n",
    "                confounds['psy'] = psy[:,0]\n",
    "                confounds['phys'] =phys[:,0]\n",
    "\n",
    "                #create PPI cov by multiply psy * phys \n",
    "                ppi = psy*phys\n",
    "                ppi = ppi.reshape((ppi.shape[0],1))\n",
    "\n",
    "                brain_time_series = brain_masker.fit_transform(img4d, confounds=[confounds]) #change this line to remove confounds \n",
    "                brain_time_series_4FC = brain_masker.fit_transform(img4d) #change this line to remove confounds\n",
    "\n",
    "                #Correlate interaction term to TS for vox in the brain\n",
    "                seed_to_voxel_correlations = (np.dot(brain_time_series.T, ppi) /\n",
    "                                ppi.shape[0])\n",
    "                print(ss, rr, tsk, seed_to_voxel_correlations.max())\n",
    "                \n",
    "                #Correlate interaction term to TS for vox in the brain\n",
    "                seed_to_voxel_correlations = (np.dot(brain_time_series_4FC.T, psy) /\n",
    "                                psy.shape[0])\n",
    "                \n",
    "                #transform correlation back to brain space\n",
    "                seed_to_voxel_correlations = np.arctanh(seed_to_voxel_correlations)\n",
    "                \n",
    "                #transform correlation map back to brain\n",
    "                seed_to_voxel_correlations_img = brain_masker.inverse_transform(seed_to_voxel_correlations.T)\n",
    "                \n",
    "                all_runs.append(seed_to_voxel_correlations_img)\n",
    "\n",
    "            mean_fc = image.mean_img(all_runs)\n",
    "                \n",
    "            #nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_ppi.nii.gz') #creates the summary file for the PPI analysis (stop here each seed region and the rest of the brain)\n",
    "            #nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_fc_4FC.nii.gz') #creates the summary file for the PSY analysis\n",
    "            \n",
    "conduct_ppi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time points in fMRI data: 184\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "def get_num_time_points(fmri_file):\n",
    "    img = nib.load(fmri_file)\n",
    "    data_shape = img.shape\n",
    "    num_time_points = data_shape[3]  # The 4th dimension in the NIfTI data shape\n",
    "    return num_time_points\n",
    "\n",
    "sub_dir = f'{study_dir}/sub-{ss}/ses-01/' #study is PTOC\n",
    "roi_dir = f'{sub_dir}/derivatives/rois' #rois in PTOC\n",
    "exp_dir = f'{sub_dir}/derivatives/fsl/{exp}' #PTOC\n",
    "\n",
    "raw_dir = params.raw_dir\n",
    "temp_dir = f'{raw_dir}/sub-064/ses-01' #hemispace \n",
    "cov_dir = f'{temp_dir}/covs' #hemispace\n",
    "\n",
    "# Example usage\n",
    "fmri_file = f'{temp_dir}/derivatives/fsl/loc/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "num_time_points = get_num_time_points(fmri_file)\n",
    "print(f'Number of time points in fMRI data: {num_time_points}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_psy_cov(runs, ss):\n",
    "    # Assuming `vols` is set to 184\n",
    "    vols = 184\n",
    "    tr = 2  # Repetition time, adjust as needed\n",
    "    times = np.arange(0, vols * len(runs), tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "    \n",
    "    for rn, run in enumerate(runs):\n",
    "        curr_cov = pd.read_csv(f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt', sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        curr_cont = pd.read_csv(f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt', sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        curr_cont.iloc[:, 2] = curr_cont.iloc[:, 2] * -1\n",
    "        \n",
    "        curr_cov = curr_cov.append(curr_cont)\n",
    "        curr_cov['onset'] = curr_cov['onset'] + (vols * rn)\n",
    "        full_cov = full_cov.append(curr_cov)\n",
    "    \n",
    "    full_cov = full_cov.sort_values(by=['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "    \n",
    "    psy, name = glm.first_level.compute_regressor(cov.T, 'spm', times)\n",
    "    \n",
    "    # Check the shape of psy\n",
    "    print(f'Shape of psy: {psy.shape}')\n",
    "    \n",
    "    return psy\n",
    "\n",
    "make_psy_cov(runs, '064')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-01_Object.txt\n",
      "     onset  duration  value\n",
      "0   80.015        16    1.0\n",
      "1  248.012        16    1.0\n",
      "2  320.012        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-01_Scramble.txt\n",
      "     onset  duration  value\n",
      "0  128.009        16    1.0\n",
      "1  272.010        16    1.0\n",
      "2  344.009        16    1.0\n",
      "Adjusted curr_cov for run 1:\n",
      "     onset  duration  value\n",
      "0   80.015        16    1.0\n",
      "0  128.009        16   -1.0\n",
      "1  248.012        16    1.0\n",
      "1  272.010        16   -1.0\n",
      "2  320.012        16    1.0\n",
      "     onset  duration  value\n",
      "0  128.009        16   -1.0\n",
      "1  248.012        16    1.0\n",
      "1  272.010        16   -1.0\n",
      "2  320.012        16    1.0\n",
      "2  344.009        16   -1.0\n",
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-02_Object.txt\n",
      "     onset  duration  value\n",
      "0   56.016        16    1.0\n",
      "1  128.008        16    1.0\n",
      "2  320.010        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-02_Scramble.txt\n",
      "     onset  duration  value\n",
      "0   32.009        16    1.0\n",
      "1  224.013        16    1.0\n",
      "2  296.007        16    1.0\n",
      "Adjusted curr_cov for run 2:\n",
      "     onset  duration  value\n",
      "0  216.009        16   -1.0\n",
      "0  240.016        16    1.0\n",
      "1  312.008        16    1.0\n",
      "1  408.013        16   -1.0\n",
      "2  480.007        16   -1.0\n",
      "     onset  duration  value\n",
      "0  240.016        16    1.0\n",
      "1  312.008        16    1.0\n",
      "1  408.013        16   -1.0\n",
      "2  480.007        16   -1.0\n",
      "2  504.010        16    1.0\n",
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-03_Object.txt\n",
      "     onset  duration  value\n",
      "0   32.007        16    1.0\n",
      "1  152.010        16    1.0\n",
      "2  296.010        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-03_Scramble.txt\n",
      "     onset  duration  value\n",
      "0  128.007        16    1.0\n",
      "1  200.013        16    1.0\n",
      "2  272.009        16    1.0\n",
      "Adjusted curr_cov for run 3:\n",
      "     onset  duration  value\n",
      "0  400.007        16    1.0\n",
      "0  496.007        16   -1.0\n",
      "1  520.010        16    1.0\n",
      "1  568.013        16   -1.0\n",
      "2  640.009        16   -1.0\n",
      "     onset  duration  value\n",
      "0  496.007        16   -1.0\n",
      "1  520.010        16    1.0\n",
      "1  568.013        16   -1.0\n",
      "2  640.009        16   -1.0\n",
      "2  664.010        16    1.0\n",
      "Full covariates DataFrame:\n",
      "     onset duration  value\n",
      "0   80.015       16    1.0\n",
      "0  128.009       16   -1.0\n",
      "1  248.012       16    1.0\n",
      "1  272.010       16   -1.0\n",
      "2  320.012       16    1.0\n",
      "2  344.009       16   -1.0\n",
      "0  216.009       16   -1.0\n",
      "0  240.016       16    1.0\n",
      "1  312.008       16    1.0\n",
      "1  408.013       16   -1.0\n",
      "2  480.007       16   -1.0\n",
      "2  504.010       16    1.0\n",
      "0  400.007       16    1.0\n",
      "0  496.007       16   -1.0\n",
      "1  520.010       16    1.0\n",
      "1  568.013       16   -1.0\n",
      "2  640.009       16   -1.0\n",
      "2  664.010       16    1.0\n",
      "     onset duration  value\n",
      "0   80.015       16    1.0\n",
      "0  128.009       16   -1.0\n",
      "1  248.012       16    1.0\n",
      "1  272.010       16   -1.0\n",
      "2  320.012       16    1.0\n",
      "2  344.009       16   -1.0\n",
      "0  216.009       16   -1.0\n",
      "0  240.016       16    1.0\n",
      "1  312.008       16    1.0\n",
      "1  408.013       16   -1.0\n",
      "2  480.007       16   -1.0\n",
      "2  504.010       16    1.0\n",
      "0  400.007       16    1.0\n",
      "0  496.007       16   -1.0\n",
      "1  520.010       16    1.0\n",
      "1  568.013       16   -1.0\n",
      "2  640.009       16   -1.0\n",
      "2  664.010       16    1.0\n",
      "Unique onsets in full_cov: [ 80.015 128.009 248.012 272.01  320.012 344.009 216.009 240.016 312.008\n",
      " 408.013]...[312.008 408.013 480.007 504.01  400.007 496.007 520.01  568.013 640.009\n",
      " 664.01 ]\n",
      "Expected number of onsets: 276\n",
      "Covered number of onsets: 18\n",
      "Shape of cov array: (18, 3)\n",
      "Number of time points expected: 276\n",
      "Shape of psy: (276, 1)\n",
      "Number of time points in psy: 276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 1.74917768e-02],\n",
       "       [ 2.47658727e-01],\n",
       "       [ 6.55101909e-01],\n",
       "       [ 9.63665935e-01],\n",
       "       [ 1.10817006e+00],\n",
       "       [ 1.14466936e+00],\n",
       "       [ 1.12805054e+00],\n",
       "       [ 1.09251449e+00],\n",
       "       [ 1.04002746e+00],\n",
       "       [ 7.83856170e-01],\n",
       "       [ 3.60349746e-01],\n",
       "       [ 4.31801177e-02],\n",
       "       [-1.05427947e-01],\n",
       "       [-1.43702091e-01],\n",
       "       [-1.27789294e-01],\n",
       "       [-9.25144857e-02],\n",
       "       [-5.75192385e-02],\n",
       "       [-3.15148971e-02],\n",
       "       [-1.54516542e-02],\n",
       "       [-6.84605288e-03],\n",
       "       [-2.74211596e-03],\n",
       "       [-9.67271650e-04],\n",
       "       [-2.61247238e-04],\n",
       "       [ 0.00000000e+00],\n",
       "       [-1.74917768e-02],\n",
       "       [-2.47658727e-01],\n",
       "       [-6.55101909e-01],\n",
       "       [-9.63665935e-01],\n",
       "       [-1.10817006e+00],\n",
       "       [-1.14466936e+00],\n",
       "       [-1.12805054e+00],\n",
       "       [-1.09251449e+00],\n",
       "       [-1.04002746e+00],\n",
       "       [-7.83856170e-01],\n",
       "       [-3.60349746e-01],\n",
       "       [-4.31801177e-02],\n",
       "       [ 1.05427947e-01],\n",
       "       [ 1.43702091e-01],\n",
       "       [ 1.27789294e-01],\n",
       "       [ 9.25144857e-02],\n",
       "       [ 5.75192385e-02],\n",
       "       [ 3.15148971e-02],\n",
       "       [ 1.54516542e-02],\n",
       "       [ 6.84605288e-03],\n",
       "       [ 2.74211596e-03],\n",
       "       [ 9.67271650e-04],\n",
       "       [ 2.61247238e-04],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [-1.74917768e-02],\n",
       "       [-2.47658727e-01],\n",
       "       [-6.55101909e-01],\n",
       "       [-9.63665935e-01],\n",
       "       [-1.10817006e+00],\n",
       "       [-1.14466936e+00],\n",
       "       [-1.12805054e+00],\n",
       "       [-1.09251449e+00],\n",
       "       [-1.04002746e+00],\n",
       "       [-7.83856170e-01],\n",
       "       [-3.60349746e-01],\n",
       "       [-4.31801177e-02],\n",
       "       [ 1.22919724e-01],\n",
       "       [ 3.91360818e-01],\n",
       "       [ 7.82891203e-01],\n",
       "       [ 1.05618042e+00],\n",
       "       [ 1.18318108e+00],\n",
       "       [ 1.42384299e+00],\n",
       "       [ 1.79860410e+00],\n",
       "       [ 2.06302647e+00],\n",
       "       [ 2.15093964e+00],\n",
       "       [ 1.92949280e+00],\n",
       "       [ 1.48866153e+00],\n",
       "       [ 1.13569460e+00],\n",
       "       [ 9.34599514e-01],\n",
       "       [ 6.40154079e-01],\n",
       "       [ 2.32560452e-01],\n",
       "       [-4.93343680e-02],\n",
       "       [-1.80438963e-01],\n",
       "       [-4.22875715e-01],\n",
       "       [-7.98342857e-01],\n",
       "       [-1.06302647e+00],\n",
       "       [-1.16843142e+00],\n",
       "       [-1.17715153e+00],\n",
       "       [-1.14376344e+00],\n",
       "       [-1.09936054e+00],\n",
       "       [-1.04276958e+00],\n",
       "       [-7.84823442e-01],\n",
       "       [-3.60610993e-01],\n",
       "       [-4.31801177e-02],\n",
       "       [ 1.05427947e-01],\n",
       "       [ 1.43702091e-01],\n",
       "       [ 1.27789294e-01],\n",
       "       [ 9.25144857e-02],\n",
       "       [ 5.75192385e-02],\n",
       "       [ 3.15148971e-02],\n",
       "       [ 1.54516542e-02],\n",
       "       [ 6.84605288e-03],\n",
       "       [ 2.02338928e-02],\n",
       "       [ 2.48625999e-01],\n",
       "       [ 6.55363156e-01],\n",
       "       [ 9.63665935e-01],\n",
       "       [ 1.12566184e+00],\n",
       "       [ 1.39232809e+00],\n",
       "       [ 1.78315245e+00],\n",
       "       [ 2.05618042e+00],\n",
       "       [ 2.14819752e+00],\n",
       "       [ 1.92852553e+00],\n",
       "       [ 1.48840029e+00],\n",
       "       [ 1.13569460e+00],\n",
       "       [ 9.34599514e-01],\n",
       "       [ 6.40154079e-01],\n",
       "       [ 2.32560452e-01],\n",
       "       [-4.93343680e-02],\n",
       "       [-1.80438963e-01],\n",
       "       [-4.22875715e-01],\n",
       "       [-7.98342857e-01],\n",
       "       [-1.06302647e+00],\n",
       "       [-1.16843142e+00],\n",
       "       [-1.17715153e+00],\n",
       "       [-1.14376344e+00],\n",
       "       [-1.09936054e+00],\n",
       "       [-1.04276958e+00],\n",
       "       [-7.84823442e-01],\n",
       "       [-3.60610993e-01],\n",
       "       [-4.31801177e-02],\n",
       "       [ 1.05427947e-01],\n",
       "       [ 1.43702091e-01],\n",
       "       [ 1.27789294e-01],\n",
       "       [ 9.25144857e-02],\n",
       "       [ 5.75192385e-02],\n",
       "       [ 3.15148971e-02],\n",
       "       [ 1.54516542e-02],\n",
       "       [ 6.84605288e-03],\n",
       "       [ 2.74211596e-03],\n",
       "       [ 9.67271650e-04],\n",
       "       [ 2.61247238e-04],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 1.74917768e-02],\n",
       "       [ 2.47658727e-01],\n",
       "       [ 6.55101909e-01],\n",
       "       [ 9.63665935e-01],\n",
       "       [ 1.09067829e+00],\n",
       "       [ 8.97010636e-01],\n",
       "       [ 4.72948633e-01],\n",
       "       [ 1.28848551e-01],\n",
       "       [-6.81426016e-02],\n",
       "       [-3.60813193e-01],\n",
       "       [-7.67700796e-01],\n",
       "       [-1.04933437e+00],\n",
       "       [-1.14545541e+00],\n",
       "       [-9.27558261e-01],\n",
       "       [-4.88139040e-01],\n",
       "       [-1.35694603e-01],\n",
       "       [ 4.79087088e-02],\n",
       "       [ 1.12187194e-01],\n",
       "       [ 1.12337640e-01],\n",
       "       [ 8.56684329e-02],\n",
       "       [ 5.47771225e-02],\n",
       "       [ 3.05476255e-02],\n",
       "       [ 1.51904070e-02],\n",
       "       [ 6.84605288e-03],\n",
       "       [ 2.74211596e-03],\n",
       "       [ 9.67271650e-04],\n",
       "       [ 2.61247238e-04],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [ 0.00000000e+00],\n",
       "       [-1.74917768e-02],\n",
       "       [-2.47658727e-01],\n",
       "       [-6.55101909e-01],\n",
       "       [-9.63665935e-01],\n",
       "       [-1.10817006e+00],\n",
       "       [-1.14466936e+00],\n",
       "       [-1.12805054e+00],\n",
       "       [-1.09251449e+00],\n",
       "       [-1.05751924e+00],\n",
       "       [-1.03151490e+00],\n",
       "       [-1.01545165e+00],\n",
       "       [-1.00684605e+00],\n",
       "       [-9.85250339e-01],\n",
       "       [-7.53308545e-01],\n",
       "       [-3.45159339e-01],\n",
       "       [-3.63340648e-02],\n",
       "       [ 1.25661840e-01],\n",
       "       [ 3.92328090e-01],\n",
       "       [ 7.83152450e-01],\n",
       "       [ 1.05618042e+00],\n",
       "       [ 1.16568930e+00],\n",
       "       [ 1.17618426e+00],\n",
       "       [ 1.14350220e+00],\n",
       "       [ 1.09936054e+00],\n",
       "       [ 1.06026135e+00],\n",
       "       [ 1.03248217e+00],\n",
       "       [ 1.01571290e+00],\n",
       "       [ 1.00684605e+00],\n",
       "       [ 9.85250339e-01],\n",
       "       [ 7.53308545e-01],\n",
       "       [ 3.45159339e-01],\n",
       "       [ 3.63340648e-02],\n",
       "       [-1.08170063e-01],\n",
       "       [-1.44669363e-01],\n",
       "       [-1.28050541e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_psy_cov(runs, ss):\n",
    "    # Define parameters\n",
    "    vols = 184\n",
    "    tr = 2\n",
    "    sub_dir = f'{study_dir}/sub-{ss}/ses-01/'\n",
    "    temp_dir = f'{raw_dir}/sub-{ss}/ses-01/'\n",
    "    cov_dir = f'{temp_dir}covs'\n",
    "    times = np.arange(0, vols * len(runs), tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for rn, run in enumerate(runs):\n",
    "        curr_cov_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt'\n",
    "        curr_cov = pd.read_csv(curr_cov_path, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        print(f'Loaded curr_cov from: {curr_cov_path}')\n",
    "        print(curr_cov.head())\n",
    "\n",
    "        curr_cont_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt'\n",
    "        curr_cont = pd.read_csv(curr_cont_path, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        print(f'Loaded curr_cont from: {curr_cont_path}')\n",
    "        print(curr_cont.head())\n",
    "\n",
    "        curr_cont.iloc[:, 2] = curr_cont.iloc[:, 2] * -1\n",
    "        curr_cov = curr_cov.append(curr_cont)\n",
    "        curr_cov['onset'] = curr_cov['onset'] + (vols * rn)\n",
    "        curr_cov = curr_cov.sort_values(by=['onset'])\n",
    "        print(f'Adjusted curr_cov for run {rn + 1}:')\n",
    "        print(curr_cov.head())\n",
    "        print(curr_cov.tail())\n",
    "\n",
    "        full_cov = full_cov.append(curr_cov)\n",
    "\n",
    "    # Ensure all time points are covered\n",
    "    print(\"Full covariates DataFrame:\")\n",
    "    print(full_cov.head(20))\n",
    "    print(full_cov.tail(20))\n",
    "\n",
    "    unique_onsets = full_cov['onset'].unique()\n",
    "    print(f\"Unique onsets in full_cov: {unique_onsets[:10]}...{unique_onsets[-10:]}\")\n",
    "    print(f\"Expected number of onsets: {len(times)}\")\n",
    "    print(f\"Covered number of onsets: {len(unique_onsets)}\")\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "    print(f'Shape of cov array: {cov.shape}')\n",
    "    print(f'Number of time points expected: {len(times)}')\n",
    "\n",
    "    psy, name = glm.first_level.compute_regressor(cov.T, 'spm', times)\n",
    "    print(f'Shape of psy: {psy.shape}')\n",
    "    print(f'Number of time points in psy: {psy.shape[0]}')\n",
    "\n",
    "    return psy\n",
    "\n",
    "make_psy_cov(runs, '064')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-01_Object.txt\n",
      "     onset  duration  value\n",
      "0   80.015        16    1.0\n",
      "1  248.012        16    1.0\n",
      "2  320.012        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-01_Scramble.txt\n",
      "     onset  duration  value\n",
      "0  128.009        16    1.0\n",
      "1  272.010        16    1.0\n",
      "2  344.009        16    1.0\n",
      "Adjusted curr_cov for run 1:\n",
      "     onset  duration  value\n",
      "0   80.015        16    1.0\n",
      "0  128.009        16   -1.0\n",
      "1  248.012        16    1.0\n",
      "1  272.010        16   -1.0\n",
      "2  320.012        16    1.0\n",
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-02_Object.txt\n",
      "     onset  duration  value\n",
      "0   56.016        16    1.0\n",
      "1  128.008        16    1.0\n",
      "2  320.010        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-02_Scramble.txt\n",
      "     onset  duration  value\n",
      "0   32.009        16    1.0\n",
      "1  224.013        16    1.0\n",
      "2  296.007        16    1.0\n",
      "Adjusted curr_cov for run 2:\n",
      "     onset  duration  value\n",
      "0  216.009        16   -1.0\n",
      "0  240.016        16    1.0\n",
      "1  312.008        16    1.0\n",
      "1  408.013        16   -1.0\n",
      "2  480.007        16   -1.0\n",
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-03_Object.txt\n",
      "     onset  duration  value\n",
      "0   32.007        16    1.0\n",
      "1  152.010        16    1.0\n",
      "2  296.010        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-03_Scramble.txt\n",
      "     onset  duration  value\n",
      "0  128.007        16    1.0\n",
      "1  200.013        16    1.0\n",
      "2  272.009        16    1.0\n",
      "Adjusted curr_cov for run 3:\n",
      "     onset  duration  value\n",
      "0  400.007        16    1.0\n",
      "0  496.007        16   -1.0\n",
      "1  520.010        16    1.0\n",
      "1  568.013        16   -1.0\n",
      "2  640.009        16   -1.0\n",
      "Shape of cov array: (18, 3)\n",
      "Shape of psy: (276, 1)\n"
     ]
    }
   ],
   "source": [
    "#chat_my_gpt\n",
    "#independent psy\n",
    "def make_psy_cov(runs, ss):\n",
    "    # Define parameters\n",
    "    vols = 184\n",
    "    tr = 2\n",
    "    sub_dir = f'{study_dir}/sub-{ss}/ses-01/'\n",
    "    temp_dir = f'{raw_dir}/sub-{ss}/ses-01/'\n",
    "    cov_dir = f'{temp_dir}covs'\n",
    "    times = np.arange(0, vols * len(runs), tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for rn, run in enumerate(runs):\n",
    "        curr_cov_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt'\n",
    "        curr_cov = pd.read_csv(curr_cov_path, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        print(f'Loaded curr_cov from: {curr_cov_path}')\n",
    "        print(curr_cov.head())\n",
    "\n",
    "        curr_cont_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt'\n",
    "        curr_cont = pd.read_csv(curr_cont_path, sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        print(f'Loaded curr_cont from: {curr_cont_path}')\n",
    "        print(curr_cont.head())\n",
    "\n",
    "        curr_cont.iloc[:, 2] = curr_cont.iloc[:, 2] * -1\n",
    "        curr_cov = curr_cov.append(curr_cont)\n",
    "        curr_cov['onset'] = curr_cov['onset'] + (vols * rn)\n",
    "        curr_cov = curr_cov.sort_values(by=['onset'])\n",
    "        print(f'Adjusted curr_cov for run {rn + 1}:')\n",
    "        print(curr_cov.head())\n",
    "\n",
    "        full_cov = full_cov.append(curr_cov)\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "    print(f'Shape of cov array: {cov.shape}')\n",
    "\n",
    "    psy, name = glm.first_level.compute_regressor(cov.T, 'spm', times)\n",
    "    print(f'Shape of psy: {psy.shape}')\n",
    "\n",
    "    return psy\n",
    "\n",
    "make_psy_cov(runs, '064')\n",
    "\n",
    "def conduct_ppi():\n",
    "    for ss in subs:\n",
    "        print(ss)\n",
    "        sub_dir = f'{study_dir}/sub-{ss}/ses-01/'\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl/{exp}'\n",
    "        \n",
    "        raw_dir = params.raw_dir\n",
    "        temp_dir = f'{raw_dir}/sub-{ss}/ses-01'\n",
    "        cov_dir = f'{temp_dir}/covs'\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords.csv')\n",
    "                                \n",
    "        for rr in rois:\n",
    "            all_runs = []\n",
    "            for rcn, rc in enumerate(run_combos):\n",
    "                curr_coords = roi_coords[(roi_coords['index'] == rcn) & (roi_coords['task'] == tsk) & (roi_coords['roi'] == rr)]\n",
    "                filtered_list = []\n",
    "                for rn in rc:\n",
    "                    print(rn)\n",
    "                    run_path = f'{temp_dir}/derivatives/fsl/loc/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "\n",
    "                    if os.path.exists(run_path):\n",
    "                        curr_run = image.load_img(run_path)\n",
    "                        affine = curr_run.affine\n",
    "                        curr_run = image.clean_img(curr_run, standardize=True)\n",
    "                        filtered_list.append(curr_run)\n",
    "                        print(f'Loaded {run_path}')\n",
    "                    else:\n",
    "                        print(f\"File {run_path} does not exist.\")\n",
    "                    \n",
    "                img4d = image.concat_imgs(filtered_list)\n",
    "                phys = extract_roi_sphere(img4d, curr_coords[['x', 'y', 'z']].values.tolist()[0])\n",
    "                \n",
    "                # Load and debug behavioral data\n",
    "                psy = make_psy_cov(rc, ss)\n",
    "                print(f'Shape of psy: {psy.shape}')# Should be (num_time_points, 1)\n",
    "                print(f'Shape of phys: {phys.shape}')# Should be (num_time_points, 1)\n",
    "                \n",
    "                # Combine phys and psy\n",
    "                #confounds = pd.DataFrame(columns=['psy', 'phys']) #og\n",
    "                confounds = pd.DataFrame(index=range(len(psy)))  # Set index to match length of psy\n",
    "                confounds['psy'] = psy[:, 0]\n",
    "                confounds['phys'] = phys[:, 0]\n",
    "\n",
    "                ppi = psy * phys\n",
    "                ppi = ppi.reshape((ppi.shape[0], 1))\n",
    "\n",
    "                brain_time_series = brain_masker.fit_transform(img4d, confounds=[confounds])\n",
    "                brain_time_series_4FC = brain_masker.fit_transform(img4d)\n",
    "\n",
    "                seed_to_voxel_correlations = (np.dot(brain_time_series.T, ppi) / ppi.shape[0])\n",
    "                print(ss, rr, tsk, seed_to_voxel_correlations.max())\n",
    "                \n",
    "                seed_to_voxel_correlations = (np.dot(brain_time_series_4FC.T, psy) / psy.shape[0])\n",
    "                seed_to_voxel_correlations = np.arctanh(seed_to_voxel_correlations)\n",
    "                \n",
    "                seed_to_voxel_correlations_img = brain_masker.inverse_transform(seed_to_voxel_correlations.T)\n",
    "                \n",
    "                all_runs.append(seed_to_voxel_correlations_img)\n",
    "\n",
    "            mean_fc = image.mean_img(all_runs)\n",
    "            nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_ppi.nii.gz')\n",
    "            nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_fc_4FC.nii.gz')\n",
    "#conduct_ppi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pause before this section to review output with Vlad\n",
    "\n",
    "def create_summary():\n",
    "    ventral_rois = ['LO_toolloc']\n",
    "    rois = [\"PPC_spaceloc\"]\n",
    "    print(subs)\n",
    "    \n",
    "    for lrv in ['l', 'r']:\n",
    "        for vr in ventral_rois:\n",
    "            summary_df = pd.DataFrame(columns=['sub'] + ['l' + rr for rr in rois] + ['r' + rr for rr in rois])\n",
    "            ventral = f'{lrv}{vr}'\n",
    "            print(ventral)\n",
    "            \n",
    "            for ss in subs:\n",
    "                sub_dir = f'{study_dir}/sub-{study}{ss}/ses-01/'\n",
    "                roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "                \n",
    "                ventral_mask = image.load_img(f'{roi_dir}/{ventral}.nii.gz')\n",
    "                ventral_mask = NiftiMasker(ventral_mask)\n",
    "                \n",
    "                roi_mean = []\n",
    "                roi_mean.append(ss)\n",
    "                \n",
    "                for lr in ['l', 'r']:\n",
    "                    for rr in rois:\n",
    "                        roi = f'{roi_dir}/{lr}{rr}.nii.gz'\n",
    "                        fc_img = image.load_img(f'{out_dir}/sub-{study}{ss}_{lr}{rr}_ppi.nii.gz')\n",
    "                        \n",
    "                        mask_img = ventral_mask.fit_transform(fc_img)\n",
    "                        mean_val = mask_img.mean()\n",
    "                        roi_mean.append(mean_val)\n",
    "                \n",
    "                summary_df.loc[len(summary_df)] = roi_mean\n",
    "            \n",
    "            summary_df.to_csv(f'{results_dir}/ppi_summary_{ventral}.csv', index=False)\n",
    "            \n",
    "create_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original with comments\n",
    "#pause after this to check the output || run brain_time_series without confounds, and instead of ppi you would use phys || change the two lines to be the same, and then you can see the correlation between the two time series. || change the name of the output and the two lines \n",
    "def conduct_ppi():\n",
    "    for ss in subs:\n",
    "        print(ss)\n",
    "        sub_dir = f'{study_dir}/sub-{study}{ss}/ses-01/'\n",
    "        cov_dir = f'{sub_dir}/covs'\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl/{exp}'\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords.csv')\n",
    "\n",
    "        for tsk in ['spaceloc','distloc']: #just include one task for now get rid of the loop and extract current coords\n",
    "            for rr in rois:\n",
    "                all_runs = [] #this will get filled with the data from each run\n",
    "                for rcn, rc in enumerate(run_combos): #determine which runs to use for creating ROIs | run combos\n",
    "                    curr_coords = roi_coords[(roi_coords['index'] == rcn) & (roi_coords['task'] ==tsk) & (roi_coords['roi'] ==rr)]\n",
    "\n",
    "                    filtered_list = []\n",
    "                    for rn in rc:\n",
    "                        \n",
    "                        curr_run = image.load_img(f'{exp_dir}/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz') #filtered_func data - cope and zstat is a mean image, while the filtered is preprocessed data - standardized\n",
    "                        #double check the above exists.\n",
    "                        curr_run = image.clean_img(curr_run,standardize=True)\n",
    "                        filtered_list.append(curr_run)\n",
    "                        \n",
    "                    img4d = image.concat_imgs(filtered_list)\n",
    "                    phys = extract_roi_sphere(img4d,curr_coords[['x','y','z']].values.tolist()[0]) #extract ROI spehere coordinate, pulls out just the time series from that part of the brain, every voxel of the brain, time series for just the spheres we've pulled out\n",
    "                    #load behavioral data\n",
    "                    #CONVOLE TO HRF\n",
    "                    psy = make_psy_cov(rc, ss) #this is the one that goes to the covariate folder and grabs the covariate we care about and converts the three coloumn into binary data. \n",
    "\n",
    "                    #combine phys (seed TS) and psy (task TS) into a regressor ||  TS = time series, CNS\n",
    "                    confounds = pd.DataFrame(columns =['psy', 'phys'])\n",
    "                    confounds['psy'] = psy[:,0]\n",
    "                    confounds['phys'] =phys[:,0]\n",
    "\n",
    "                    #create PPI cov by multiply psy * phys #this is createing the interaction term, the is the PPI time course. There are the individual, so we can get a brain time series with sine phys regressed out\n",
    "                    ppi = psy*phys\n",
    "                    ppi = ppi.reshape((ppi.shape[0],1))\n",
    "\n",
    "                    brain_time_series = brain_masker.fit_transform(img4d, confounds=[confounds]) #change this line to remove confounds \n",
    "                    #brain_time_series_4FC = brain_masker.fit_transform(img4d) #change this line to remove confounds\n",
    "\n",
    "                    #Correlate interaction term to TS for vox in the brain\n",
    "                    seed_to_voxel_correlations = (np.dot(brain_time_series.T, ppi) /\n",
    "                                    ppi.shape[0])\n",
    "                    print(ss, rr, tsk, seed_to_voxel_correlations.max())\n",
    "                    \n",
    "                    #Correlate interaction term to TS for vox in the brain\n",
    "                    #seed_to_voxel_correlations = (np.dot(brain_time_series_4FC.T, psy) /\n",
    "                                    #psy.shape[0])\n",
    "                    \n",
    "                    seed_to_voxel_correlations = np.arctanh(seed_to_voxel_correlations) # transform back to brain space\n",
    "                    #transform correlation map back to brain\n",
    "                    seed_to_voxel_correlations_img = brain_masker.inverse_transform(seed_to_voxel_correlations.T)\n",
    "                    \n",
    "                    all_runs.append(seed_to_voxel_correlations_img)\n",
    "\n",
    "                mean_fc = image.mean_img(all_runs)\n",
    "                    \n",
    "                nib.save(mean_fc, f'{out_dir}/sub-{study}{ss}_{rr}_{tsk}_ppi.nii.gz') #creates the summary file for the PPI analysis (stop here each seed region and the rest of the brain)\n",
    "                #nib.save(mean_fc, f'{out_dir}/sub-{study}{ss}_{rr}_{tsk}_fc_4FC.nii.gz') #creates the summary file for the PSY analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85d6a1f04e70c5556da1eb33c5679a806be4c5365a0d8ae0b55875cb552fe2b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
