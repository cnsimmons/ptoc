{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,curr_dir)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "import statsmodels.api as s\n",
    "from sklearn import metrics\n",
    "\n",
    "import pdb\n",
    "import ptoc_params as params\n",
    "\n",
    "from plotnine import *\n",
    "#from plotnine import ggplot, aes, geom_point\n",
    "\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#load additional libraries\n",
    "from nilearn import image, plotting, input_data, glm\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import nibabel as nib\n",
    "import statsmodels.api as sm\n",
    "from nilearn.datasets import load_mni152_brain_mask, load_mni152_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = params.data_dir\n",
    "results_dir = params.results_dir\n",
    "fig_dir = params.fig_dir\n",
    "raw_dir = params.raw_dir\n",
    "\n",
    "sub_info = params.sub_info\n",
    "task_info = params.task_info\n",
    "\n",
    "suf = params.suf\n",
    "rois = params.rois\n",
    "hemis = params.hemis\n",
    "\n",
    "#load subject info\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "\n",
    "#mni = load_mni152_brain_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''exp info'''\n",
    "subs = ['sub-064']  # Run for one subject initially\n",
    "#subs = sub_info['sub'].tolist()\n",
    "\n",
    "#Just controls\n",
    "subs = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "study = 'ptoc'\n",
    "data_dir = 'hemispace'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "out_dir = f'{study_dir}/derivatives/fc'\n",
    "results_dir = '/user_data/csimmon2/GitHub_Repos/ptoc/results'\n",
    "exp = ''\n",
    "rois = ['LO']  # Run for one ROI initially\n",
    "control_tasks = ['loc']\n",
    "file_suf = ''\n",
    "\n",
    "'''scan params'''\n",
    "#tr = 1 #in the original code\n",
    "#vols = 321 #in the original code\n",
    "\n",
    "tr = 2 #ptoc_params\n",
    "vols = 184 #ptoc_params\n",
    "\n",
    "whole_brain_mask = load_mni152_brain_mask()\n",
    "mni = load_mni152_template()\n",
    "brain_masker = NiftiMasker(whole_brain_mask, smoothing_fwhm=0, standardize=True)\n",
    "\n",
    "'''run info'''\n",
    "run_num = 3\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = []\n",
    "\n",
    "for rn1 in range(1, run_num + 1):\n",
    "    for rn2 in range(rn1 + 1, run_num + 1):\n",
    "        run_combos.append([rn1, rn2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extract ROI coordinates\n",
    "def extract_roi_coords():\n",
    "    \"\"\"\n",
    "    Define ROIs\n",
    "    \"\"\"\n",
    "    parcels = ['V1', 'aIPS', 'PFS', 'pIPS', 'LO']\n",
    "    subs = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "\n",
    "    for ss in subs:\n",
    "        print(f'Processing subject: {ss}')\n",
    "        sub_dir = f'{study_dir}/{ss}/ses-01'\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "        os.makedirs(f'{roi_dir}/spheres', exist_ok=True)\n",
    "        \n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl'\n",
    "        parcel_dir = f'{roi_dir}/parcels'\n",
    "        roi_coords = pd.DataFrame(columns=['index', 'task', 'roi', 'x', 'y', 'z'])\n",
    "        \n",
    "        for rcn, rc in enumerate(run_combos):\n",
    "            roi_runs = [ele for ele in runs if ele not in rc]\n",
    "            \n",
    "            #load each run\n",
    "            all_runs = []\n",
    "            for rn in roi_runs:\n",
    "                curr_run_path = f'{exp_dir}/loc/run-0{rn}/1stLevel.feat/stats/zstat3_reg.nii.gz'\n",
    "                if os.path.exists(curr_run_path):\n",
    "                    curr_run = image.load_img(curr_run_path)\n",
    "                    all_runs.append(curr_run)\n",
    "                else:\n",
    "                    print(f'File does not exist: {curr_run_path}')\n",
    "            \n",
    "            mean_zstat = image.mean_img(all_runs)\n",
    "            affine = mean_zstat.affine\n",
    "\n",
    "            for pr in parcels:\n",
    "                roi_path = f'{parcel_dir}/{pr}.nii.gz'\n",
    "                if os.path.exists(roi_path):\n",
    "                    roi = image.load_img(roi_path)\n",
    "                    roi = image.math_img('img > 0', img=roi)\n",
    "\n",
    "                    coords = plotting.find_xyz_cut_coords(mean_zstat, mask_img=roi, activation_threshold=0.99)\n",
    "                    \n",
    "                    masked_stat = image.math_img('img1 * img2', img1=roi, img2=mean_zstat)\n",
    "                    masked_stat = image.get_data(masked_stat)\n",
    "                    np_coords = np.where(masked_stat == np.max(masked_stat))\n",
    "                    \n",
    "                    curr_coords = pd.Series([rcn, 'loc', pr] + coords, index=roi_coords.columns)\n",
    "                    roi_coords = roi_coords.append(curr_coords, ignore_index=True)\n",
    "\n",
    "                    # control task ROI\n",
    "                    control_zstat_path = f'{exp_dir}/loc/HighLevel.gfeat/cope3.feat/stats/zstat1.nii.gz'\n",
    "                    if os.path.exists(control_zstat_path):\n",
    "                        control_zstat = image.load_img(control_zstat_path)\n",
    "                        coords = plotting.find_xyz_cut_coords(control_zstat, mask_img=roi, activation_threshold=0.99)\n",
    "                        \n",
    "                        curr_coords = pd.Series([rcn, 'highlevel', pr] + coords, index=roi_coords.columns)\n",
    "                        roi_coords = roi_coords.append(curr_coords, ignore_index=True)\n",
    "                    else:\n",
    "                        print(f'File does not exist: {control_zstat_path}')\n",
    "                else:\n",
    "                    print(f'File does not exist: {roi_path}')\n",
    "            \n",
    "        roi_coords.to_csv(f'{roi_dir}/spheres/sphere_coords.csv', index=False)\n",
    "\n",
    "# Call the function\n",
    "#extract_roi_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_roi_sphere(img, coords):\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius = 6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    \n",
    "    phys = np.mean(seed_time_series, axis= 1)\n",
    "    #phys = (phys - np.mean(phys)) / np.std(phys) #TRY WITHOUT STANDARDIZING AT SOME POINT\n",
    "    phys = phys.reshape((phys.shape[0],1))\n",
    "    \n",
    "    return phys\n",
    "\n",
    "#these paths are tricky\n",
    "def make_psy_cov(runs,ss):\n",
    "    #rois = ['LO']\n",
    "    #tsk = 'loc'\n",
    "    #rr = 'LO'\n",
    "    #ss = '064'\n",
    "    #runs = [1,2,3]\n",
    "    vols = 184\n",
    "    tr = 2\n",
    "    \n",
    "    raw_dir = params.raw_dir\n",
    "    temp_dir = f'{raw_dir}/sub-{ss}/ses-01' #raw_dir is from hemispace\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    \n",
    "    times = np.arange(0, vols*len(runs), tr)\n",
    "    full_cov = pd.DataFrame(columns = ['onset','duration', 'value'])\n",
    "    \n",
    "    for rn, run in enumerate(runs):    \n",
    "        \n",
    "        curr_cov = pd.read_csv(f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt', sep = '\\t', header = None, names = ['onset','duration', 'value'])\n",
    "        curr_cov_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Object.txt'\n",
    "        print(f'Loaded curr_cov from: {curr_cov_path}')\n",
    "        print(curr_cov)\n",
    "        #contrasting (neg) cov\n",
    "\n",
    "        curr_cont = pd.read_csv(f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt', sep = '\\t', header =None, names =['onset','duration', 'value'])\n",
    "        curr_cont_path = f'{cov_dir}/catloc_{ss}_run-0{run}_Scramble.txt'\n",
    "        print(f'Loaded curr_cont from: {curr_cont_path}')\n",
    "        print(curr_cont)\n",
    "        curr_cont.iloc[:,2] = curr_cont.iloc[:,2] *-1 #make contrasting cov neg\n",
    "        \n",
    "        curr_cov = curr_cov.append(curr_cont) #append to positive\n",
    "\n",
    "        curr_cov['onset'] = curr_cov['onset'] + (vols*rn)\n",
    "        full_cov = full_cov.append(curr_cov)\n",
    "        #add number of vols to the timing cols based on what run you are on\n",
    "        #e.g., for run 1, add 0, for run 2, add 321\n",
    "        #curr_cov['onset'] = curr_cov['onset'] + ((rn_n)*vols) \n",
    "        \n",
    "        \n",
    "        #append to concatenated cov\n",
    "    full_cov = full_cov.sort_values(by =['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "\n",
    "    #convolve to hrf\n",
    "    psy, name = glm.first_level.compute_regressor(cov.T, 'spm', times)\n",
    "        \n",
    "\n",
    "    return psy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "064\n",
      "1\n",
      "Loaded /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/derivatives/fsl/loc/run-01/1stLevel.feat/filtered_func_data_reg.nii.gz\n",
      "2\n",
      "Loaded /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/derivatives/fsl/loc/run-02/1stLevel.feat/filtered_func_data_reg.nii.gz\n",
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-01_Object.txt\n",
      "     onset  duration  value\n",
      "0   80.015        16    1.0\n",
      "1  248.012        16    1.0\n",
      "2  320.012        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-01_Scramble.txt\n",
      "     onset  duration  value\n",
      "0  128.009        16    1.0\n",
      "1  272.010        16    1.0\n",
      "2  344.009        16    1.0\n",
      "Loaded curr_cov from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-02_Object.txt\n",
      "     onset  duration  value\n",
      "0   56.016        16    1.0\n",
      "1  128.008        16    1.0\n",
      "2  320.010        16    1.0\n",
      "Loaded curr_cont from: /lab_data/behrmannlab/vlad/hemispace/sub-064/ses-01/covs/catloc_064_run-02_Scramble.txt\n",
      "     onset  duration  value\n",
      "0   32.009        16    1.0\n",
      "1  224.013        16    1.0\n",
      "2  296.007        16    1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (368) does not match length of index (184)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     80\u001b[0m             mean_fc \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mmean_img(all_runs)\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;66;03m#nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_ppi.nii.gz') #creates the summary file for the PPI analysis (stop here each seed region and the rest of the brain)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;66;03m#nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_fc_4FC.nii.gz') #creates the summary file for the PSY analysis\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mconduct_ppi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m, in \u001b[0;36mconduct_ppi\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m confounds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpsy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphys\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     53\u001b[0m confounds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpsy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m psy[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 54\u001b[0m \u001b[43mconfounds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphys\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39mphys[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#create PPI cov by multiply psy * phys \u001b[39;00m\n\u001b[1;32m     57\u001b[0m ppi \u001b[38;5;241m=\u001b[39m psy\u001b[38;5;241m*\u001b[39mphys\n",
      "File \u001b[0;32m~/anaconda3/envs/fmri/lib/python3.9/site-packages/pandas/core/frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fmri/lib/python3.9/site-packages/pandas/core/frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4172\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4174\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4177\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4178\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4179\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4180\u001b[0m     ):\n\u001b[1;32m   4181\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/anaconda3/envs/fmri/lib/python3.9/site-packages/pandas/core/frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4915\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/fmri/lib/python3.9/site-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (368) does not match length of index (184)"
     ]
    }
   ],
   "source": [
    "#STUCK 7/29/24 - repeated memory issues || one ROI, one sub, one run\n",
    "#rois = ['LO']\n",
    "#rr = 'LO'\n",
    "#runs = [1] #maybe it is supposed to by [0,1,2]?\n",
    "#rn = 1\n",
    "subs = ['064']\n",
    "ss = ['064']\n",
    "tsk = 'loc'\n",
    "tr = 2\n",
    "vols = 184\n",
    "\n",
    "def conduct_ppi():\n",
    "    for ss in subs:\n",
    "        print(ss)\n",
    "        sub_dir = f'{study_dir}/sub-{ss}/ses-01/' #study is PTOC\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois' #rois in PTOC\n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl/{exp}' #PTOC\n",
    "        \n",
    "        raw_dir = params.raw_dir\n",
    "        temp_dir = f'{raw_dir}/sub-{ss}/ses-01' #hemispace \n",
    "        cov_dir = f'{temp_dir}/covs' #hemispace\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords.csv') #load ROI coordinates\n",
    "                                \n",
    "        for rr in rois:\n",
    "            all_runs = [] #this will get filled with the data from each run\n",
    "            \n",
    "            for rcn, rc in enumerate(run_combos): #determine which runs to use\n",
    "                curr_coords = roi_coords[(roi_coords['index'] == rcn) & (roi_coords['task'] ==tsk) & (roi_coords['roi'] ==rr)] \n",
    "                filtered_list = []\n",
    "                for rn in rc:\n",
    "                    print (rn)\n",
    "                    run_path = f'{temp_dir}/derivatives/fsl/loc/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz'\n",
    "\n",
    "                    if os.path.exists(run_path):\n",
    "                        curr_run = image.load_img(run_path) #load image data\n",
    "                        affine = curr_run.affine\n",
    "                        curr_run = image.clean_img(curr_run, standardize=True)\n",
    "                        filtered_list.append(curr_run)\n",
    "                        print(f'Loaded {run_path}')\n",
    "                    else:\n",
    "                        print(f\"File {run_path} does not exist.\")\n",
    "                    \n",
    "                img4d = image.concat_imgs(filtered_list)\n",
    "                phys = extract_roi_sphere(img4d,curr_coords[['x','y','z']].values.tolist()[0]) #clarify which coords \n",
    "                \n",
    "                #load behavioral data\n",
    "                psy = make_psy_cov(rc, ss) #load psy covariates\n",
    "                \n",
    "                #combine phys (seed TS) and psy (task TS) into a regressor \n",
    "                confounds = pd.DataFrame(columns =['psy', 'phys'])\n",
    "                confounds['psy'] = psy[:,0]\n",
    "                confounds['phys'] =phys[:,0]\n",
    "\n",
    "                #create PPI cov by multiply psy * phys \n",
    "                ppi = psy*phys\n",
    "                ppi = ppi.reshape((ppi.shape[0],1))\n",
    "\n",
    "                brain_time_series = brain_masker.fit_transform(img4d, confounds=[confounds]) #change this line to remove confounds \n",
    "                brain_time_series_4FC = brain_masker.fit_transform(img4d) #change this line to remove confounds\n",
    "\n",
    "                #Correlate interaction term to TS for vox in the brain\n",
    "                seed_to_voxel_correlations = (np.dot(brain_time_series.T, ppi) /\n",
    "                                ppi.shape[0])\n",
    "                print(ss, rr, tsk, seed_to_voxel_correlations.max())\n",
    "                \n",
    "                #Correlate interaction term to TS for vox in the brain\n",
    "                seed_to_voxel_correlations = (np.dot(brain_time_series_4FC.T, psy) /\n",
    "                                psy.shape[0])\n",
    "                \n",
    "                #transform correlation back to brain space\n",
    "                seed_to_voxel_correlations = np.arctanh(seed_to_voxel_correlations)\n",
    "                \n",
    "                #transform correlation map back to brain\n",
    "                seed_to_voxel_correlations_img = brain_masker.inverse_transform(seed_to_voxel_correlations.T)\n",
    "                \n",
    "                all_runs.append(seed_to_voxel_correlations_img)\n",
    "\n",
    "            mean_fc = image.mean_img(all_runs)\n",
    "                \n",
    "            #nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_ppi.nii.gz') #creates the summary file for the PPI analysis (stop here each seed region and the rest of the brain)\n",
    "            #nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_fc_4FC.nii.gz') #creates the summary file for the PSY analysis\n",
    "            \n",
    "conduct_ppi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pause before this section to review output with Vlad\n",
    "\n",
    "def create_summary():\n",
    "    ventral_rois = ['LO_toolloc']\n",
    "    rois = [\"PPC_spaceloc\"]\n",
    "    print(subs)\n",
    "    \n",
    "    for lrv in ['l', 'r']:\n",
    "        for vr in ventral_rois:\n",
    "            summary_df = pd.DataFrame(columns=['sub'] + ['l' + rr for rr in rois] + ['r' + rr for rr in rois])\n",
    "            ventral = f'{lrv}{vr}'\n",
    "            print(ventral)\n",
    "            \n",
    "            for ss in subs:\n",
    "                sub_dir = f'{study_dir}/sub-{study}{ss}/ses-01/'\n",
    "                roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "                \n",
    "                ventral_mask = image.load_img(f'{roi_dir}/{ventral}.nii.gz')\n",
    "                ventral_mask = NiftiMasker(ventral_mask)\n",
    "                \n",
    "                roi_mean = []\n",
    "                roi_mean.append(ss)\n",
    "                \n",
    "                for lr in ['l', 'r']:\n",
    "                    for rr in rois:\n",
    "                        roi = f'{roi_dir}/{lr}{rr}.nii.gz'\n",
    "                        fc_img = image.load_img(f'{out_dir}/sub-{study}{ss}_{lr}{rr}_ppi.nii.gz')\n",
    "                        \n",
    "                        mask_img = ventral_mask.fit_transform(fc_img)\n",
    "                        mean_val = mask_img.mean()\n",
    "                        roi_mean.append(mean_val)\n",
    "                \n",
    "                summary_df.loc[len(summary_df)] = roi_mean\n",
    "            \n",
    "            summary_df.to_csv(f'{results_dir}/ppi_summary_{ventral}.csv', index=False)\n",
    "            \n",
    "create_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original with comments\n",
    "#pause after this to check the output || run brain_time_series without confounds, and instead of ppi you would use phys || change the two lines to be the same, and then you can see the correlation between the two time series. || change the name of the output and the two lines \n",
    "def conduct_ppi():\n",
    "    for ss in subs:\n",
    "        print(ss)\n",
    "        sub_dir = f'{study_dir}/sub-{study}{ss}/ses-01/'\n",
    "        cov_dir = f'{sub_dir}/covs'\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'\n",
    "        exp_dir = f'{sub_dir}/derivatives/fsl/{exp}'\n",
    "\n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords.csv')\n",
    "\n",
    "        for tsk in ['spaceloc','distloc']: #just include one task for now get rid of the loop and extract current coords\n",
    "            for rr in rois:\n",
    "                all_runs = [] #this will get filled with the data from each run\n",
    "                for rcn, rc in enumerate(run_combos): #determine which runs to use for creating ROIs | run combos\n",
    "                    curr_coords = roi_coords[(roi_coords['index'] == rcn) & (roi_coords['task'] ==tsk) & (roi_coords['roi'] ==rr)]\n",
    "\n",
    "                    filtered_list = []\n",
    "                    for rn in rc:\n",
    "                        \n",
    "                        curr_run = image.load_img(f'{exp_dir}/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz') #filtered_func data - cope and zstat is a mean image, while the filtered is preprocessed data - standardized\n",
    "                        #double check the above exists.\n",
    "                        curr_run = image.clean_img(curr_run,standardize=True)\n",
    "                        filtered_list.append(curr_run)\n",
    "                        \n",
    "                    img4d = image.concat_imgs(filtered_list)\n",
    "                    phys = extract_roi_sphere(img4d,curr_coords[['x','y','z']].values.tolist()[0]) #extract ROI spehere coordinate, pulls out just the time series from that part of the brain, every voxel of the brain, time series for just the spheres we've pulled out\n",
    "                    #load behavioral data\n",
    "                    #CONVOLE TO HRF\n",
    "                    psy = make_psy_cov(rc, ss) #this is the one that goes to the covariate folder and grabs the covariate we care about and converts the three coloumn into binary data. \n",
    "\n",
    "                    #combine phys (seed TS) and psy (task TS) into a regressor ||  TS = time series, CNS\n",
    "                    confounds = pd.DataFrame(columns =['psy', 'phys'])\n",
    "                    confounds['psy'] = psy[:,0]\n",
    "                    confounds['phys'] =phys[:,0]\n",
    "\n",
    "                    #create PPI cov by multiply psy * phys #this is createing the interaction term, the is the PPI time course. There are the individual, so we can get a brain time series with sine phys regressed out\n",
    "                    ppi = psy*phys\n",
    "                    ppi = ppi.reshape((ppi.shape[0],1))\n",
    "\n",
    "                    brain_time_series = brain_masker.fit_transform(img4d, confounds=[confounds]) #change this line to remove confounds \n",
    "                    #brain_time_series_4FC = brain_masker.fit_transform(img4d) #change this line to remove confounds\n",
    "\n",
    "                    #Correlate interaction term to TS for vox in the brain\n",
    "                    seed_to_voxel_correlations = (np.dot(brain_time_series.T, ppi) /\n",
    "                                    ppi.shape[0])\n",
    "                    print(ss, rr, tsk, seed_to_voxel_correlations.max())\n",
    "                    \n",
    "                    #Correlate interaction term to TS for vox in the brain\n",
    "                    #seed_to_voxel_correlations = (np.dot(brain_time_series_4FC.T, psy) /\n",
    "                                    #psy.shape[0])\n",
    "                    \n",
    "                    seed_to_voxel_correlations = np.arctanh(seed_to_voxel_correlations) # transform back to brain space\n",
    "                    #transform correlation map back to brain\n",
    "                    seed_to_voxel_correlations_img = brain_masker.inverse_transform(seed_to_voxel_correlations.T)\n",
    "                    \n",
    "                    all_runs.append(seed_to_voxel_correlations_img)\n",
    "\n",
    "                mean_fc = image.mean_img(all_runs)\n",
    "                    \n",
    "                nib.save(mean_fc, f'{out_dir}/sub-{study}{ss}_{rr}_{tsk}_ppi.nii.gz') #creates the summary file for the PPI analysis (stop here each seed region and the rest of the brain)\n",
    "                #nib.save(mean_fc, f'{out_dir}/sub-{study}{ss}_{rr}_{tsk}_fc_4FC.nii.gz') #creates the summary file for the PSY analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85d6a1f04e70c5556da1eb33c5679a806be4c5365a0d8ae0b55875cb552fe2b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
