{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run to start\n",
    "curr_dir = f'/user_data/csimmon2/git_repos/ptoc'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,curr_dir)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "import statsmodels.api as s\n",
    "from sklearn import metrics\n",
    "\n",
    "import pdb\n",
    "import ptoc_params as params\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#load additional libraries\n",
    "from nilearn import image, plotting, input_data, glm\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import nibabel as nib\n",
    "import statsmodels.api as sm\n",
    "from nilearn.datasets import load_mni152_brain_mask, load_mni152_template\n",
    "from nilearn.glm.first_level import compute_regressor \n",
    "\n",
    "data_dir = params.data_dir\n",
    "results_dir = params.results_dir\n",
    "fig_dir = params.fig_dir\n",
    "raw_dir = params.raw_dir\n",
    "sub_info = params.sub_info\n",
    "task_info = params.task_info\n",
    "\n",
    "suf = params.suf\n",
    "#mni = load_mni152_brain_mask()\n",
    "\n",
    "'''exp info'''\n",
    "#load subject info\n",
    "sub_info = pd.read_csv(f'{curr_dir}/sub_info.csv')\n",
    "subs = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "\n",
    "study = 'ptoc'\n",
    "study_dir = f\"/lab_data/behrmannlab/vlad/{study}\"\n",
    "results_dir = '/user_data/csimmon2/GitHub_Repos/ptoc/results'\n",
    "exp = ''\n",
    "control_tasks = ['loc']\n",
    "file_suf = ''\n",
    "\n",
    "'''scan params'''\n",
    "tr = 2 #ptoc_params\n",
    "vols = 184 #ptoc_params\n",
    "\n",
    "whole_brain_mask = load_mni152_brain_mask()\n",
    "mni = load_mni152_template()\n",
    "brain_masker = NiftiMasker(whole_brain_mask, smoothing_fwhm=0, standardize=True)\n",
    "\n",
    "#PPI\n",
    "#rois = params.rois\n",
    "#rois = ['LO', 'PFS', 'pIPS', 'aIPS', 'V1']\n",
    "rois = ['V1']\n",
    "\n",
    "'''run info'''\n",
    "run_num =3\n",
    "runs = list(range(1,run_num+1))\n",
    "run_combos = []\n",
    "#determine the number of left out run combos\n",
    "\n",
    "for rn1 in range(1,run_num+1):\n",
    "    for rn2 in range(rn1+1,run_num+1):\n",
    "        run_combos.append([rn1,rn2])\n",
    "        \n",
    "\n",
    "#phys\n",
    "def extract_roi_sphere(img, coords):\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius = 6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    \n",
    "    phys = np.mean(seed_time_series, axis= 1)\n",
    "    phys = phys.reshape((phys.shape[0],1))\n",
    "    print (f'phys just ran')\n",
    "    return phys\n",
    "\n",
    "#psy\n",
    "def make_psy_cov(runs, ss):\n",
    "    temp_dir = f'{raw_dir}/{ss}/ses-01'\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    \n",
    "    # Only for a single run\n",
    "    times = np.arange(0, vols * tr, tr)  # Create time array covering the whole run duration\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "    \n",
    "    for rn, run in enumerate(runs):\n",
    "        ss_num = ss.split('-')[1]  # Strips the \"sub-\" from the subject number\n",
    "        curr_cov = pd.read_csv(f'{cov_dir}/catloc_{ss_num}_run-0{run}_Object.txt', sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        \n",
    "        # Contrasting (negative) covariate\n",
    "        curr_cont = pd.read_csv(f'{cov_dir}/catloc_{ss_num}_run-0{run}_Scramble.txt', sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "        curr_cont.iloc[:, 2] = curr_cont.iloc[:, 2] * -1  # Make contrasting cov negative\n",
    "        \n",
    "        curr_cov = curr_cov.append(curr_cont)  # Append to positive\n",
    "        \n",
    "        # Append to concatenated cov\n",
    "        full_cov = full_cov.append(curr_cov)\n",
    "    \n",
    "    full_cov = full_cov.sort_values(by=['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "\n",
    "    # Convolve to HRF\n",
    "    psy, name = compute_regressor(cov.T, 'spm', times)\n",
    "    \n",
    "    # Debug: Print the shape of the created psy array\n",
    "    print(f'Full covariate matrix shape: {cov.shape}')\n",
    "    print(f'Created psy array shape: {psy.shape}')\n",
    "    print (f'psy just ran')\n",
    "    return psy\n",
    "\n",
    "#ppi\n",
    "def conduct_ppi():\n",
    "    for ss in subs:\n",
    "        print(ss)\n",
    "        sub_dir = f'{study_dir}/{ss}/ses-01/'  # study is PTOC\n",
    "        roi_dir = f'{sub_dir}/derivatives/rois'  # rois in PTOC\n",
    "        raw_dir = params.raw_dir  # hemispace\n",
    "        temp_dir = f'{raw_dir}/{ss}/ses-01/derivatives/fsl/loc'  # hemispace\n",
    "        \n",
    "        roi_coords = pd.read_csv(f'{roi_dir}/spheres/sphere_coords.csv')  # load ROI coordinates\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        out_dir = f'{study_dir}/{ss}/ses-01/derivatives/fc'\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        print(f'Output directory ensured at {out_dir}')\n",
    "\n",
    "        for tsk in ['loc']:\n",
    "            for rr in rois:\n",
    "                # Check if the file already exists\n",
    "                output_file = f'{out_dir}/{ss}_{rr}_{tsk}_ppi.nii.gz'\n",
    "                if os.path.exists(output_file):\n",
    "                    print(f\"Output file {output_file} already exists. Skipping analysis.\")\n",
    "                    continue\n",
    "                        \n",
    "                all_runs = []\n",
    "                for rcn, rc in enumerate(run_combos):  # run combos\n",
    "                    curr_coords = roi_coords[(roi_coords['index'] == rcn) & (roi_coords['task'] == tsk) & (roi_coords['roi'] == rr)]\n",
    "                    for rn in rc:\n",
    "                        filtered_list = []\n",
    "                        curr_run = image.load_img(f'{temp_dir}/run-0{rn}/1stLevel.feat/filtered_func_data_reg.nii.gz') \n",
    "                        curr_run = image.clean_img(curr_run, standardize=True)\n",
    "                        filtered_list.append(curr_run)    \n",
    "                    print('Loaded filtered data')\n",
    "                    \n",
    "                    img4d = image.concat_imgs(filtered_list)\n",
    "                    print('Loaded 4D image') \n",
    "                    \n",
    "                    phys = extract_roi_sphere(img4d, curr_coords[['x', 'y', 'z']].values.tolist()[0])  # extract ROI sphere coordinate\n",
    "                    print('Extracted sphere') \n",
    "                    \n",
    "                    # Load behavioral data\n",
    "                    psy = make_psy_cov([rn], ss)  # grabs the covariate and converts the three-column into binary data\n",
    "                    print('Loaded psy cov')\n",
    "                    \n",
    "                    # Debug: Print shapes of img4d and the concatenated phys and psy arrays\n",
    "                    print(f'Shape of img4d: {img4d.shape}')\n",
    "                    print(f'Length of phys: {phys.shape[0]}')\n",
    "                    print(f'Length of psy: {psy.shape[0]}')\n",
    "                    \n",
    "                    # Ensure phys and psy lengths match\n",
    "                    assert phys.shape[0] == psy.shape[0], f\"Length mismatch: phys={phys.shape[0]}, psy={psy.shape[0]}\"\n",
    "                    \n",
    "                    # Combine phys (seed TS) and psy (task TS) into a regressor\n",
    "                    confounds = pd.DataFrame(columns=['psy', 'phys'])\n",
    "                    confounds['psy'] = psy[:, 0]\n",
    "                    confounds['phys'] = phys[:, 0]\n",
    "                    print('Combined psy and phys')\n",
    "\n",
    "                    # Create PPI cov by multiplying psy * phys\n",
    "                    ppi = psy * phys\n",
    "                    ppi = ppi.reshape((ppi.shape[0], 1))\n",
    "                    print('Created PPI')\n",
    "\n",
    "                    # Extract brain time series with confounds removal\n",
    "                    brain_time_series = brain_masker.fit_transform(img4d, confounds=[confounds])\n",
    "                    print('Extracted brain TS with confounds removal')\n",
    "\n",
    "                    # If needed, you can create a version without confounds removal here (if required)\n",
    "                    # brain_time_series_4FC = brain_masker.fit_transform(img4d)  # This line is not required if the next steps do not use it\n",
    "\n",
    "                    # Correlate interaction term to TS for vox in the brain\n",
    "                    seed_to_voxel_correlations = (np.dot(brain_time_series.T, ppi) / ppi.shape[0])\n",
    "                    print(ss, rr, tsk, seed_to_voxel_correlations.max())\n",
    "                    print('Correlated interaction term')\n",
    "\n",
    "                    # Correlate psy term\n",
    "                    seed_to_voxel_correlations = (np.dot(brain_time_series.T, psy) / psy.shape[0])\n",
    "                    print('Correlated psy term')\n",
    "\n",
    "                    # Transform correlation back to brain space\n",
    "                    seed_to_voxel_correlations = np.arctanh(seed_to_voxel_correlations)\n",
    "                    print('Transformed correlation')\n",
    "\n",
    "                    # Transform correlation map back to brain\n",
    "                    seed_to_voxel_correlations_img = brain_masker.inverse_transform(seed_to_voxel_correlations.T)\n",
    "                    print('Transformed correlation map')\n",
    "\n",
    "                    all_runs.append(seed_to_voxel_correlations_img)\n",
    "\n",
    "                mean_fc = image.mean_img(all_runs)\n",
    "                \n",
    "                # Ensure output directory exists\n",
    "                out_dir = f'{study_dir}/{ss}/ses-01/derivatives/fc'\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                print(f'Output directory ensured at {out_dir}')\n",
    "                \n",
    "                # Save the result\n",
    "                nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_{tsk}_ppi.nii.gz')\n",
    "                print(f'Saved PPI to {out_dir}/{ss}_{rr}_{tsk}_ppi.nii.gz')\n",
    "                nib.save(mean_fc, f'{out_dir}/{ss}_{rr}_{tsk}_fc_4FC.nii.gz')\n",
    "                print(f'Saved FC to {out_dir}/{ss}_{rr}_{tsk}_fc_4FC.nii.gz')\n",
    "\n",
    "\n",
    "conduct_ppi()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85d6a1f04e70c5556da1eb33c5679a806be4c5365a0d8ae0b55875cb552fe2b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
