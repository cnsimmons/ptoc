{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ROI: LO, Hemisphere: left\n",
      "Both FC and PPI files exist for LO left. Skipping...\n",
      "Processing ROI: LO, Hemisphere: right\n",
      "Both FC and PPI files exist for LO right. Skipping...\n",
      "Processing ROI: pIPS, Hemisphere: left\n",
      "Both FC and PPI files exist for pIPS left. Skipping...\n",
      "Processing ROI: pIPS, Hemisphere: right\n",
      "Processing run combination: [1, 2]\n",
      "Saved FC result for pIPS right\n",
      "Saved PPI result for pIPS right\n"
     ]
    }
   ],
   "source": [
    "# runs 2nd level analyses for PPI and FC\n",
    "import sys\n",
    "sys.path.insert(0, '/user_data/csimmon2/git_repos/ptoc')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import gc\n",
    "from nilearn import image, input_data\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import warnings\n",
    "import ptoc_params as params\n",
    "import argparse\n",
    "\n",
    "raw_dir = params.raw_dir\n",
    "results_dir = params.results_dir\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up run combinations like obj script\n",
    "run_num = 2  # Adjust based on your data\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) \n",
    "              for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "tr = 1\n",
    "vols = 341\n",
    "\n",
    "def extract_roi_timeseries(img, roi_mask, hemisphere='left'):\n",
    "    \"\"\"Extract ROI timeseries using parcels and hemisphere masking.\"\"\"\n",
    "    roi_data = roi_mask.get_fdata()\n",
    "    mid_x = roi_data.shape[0] // 2\n",
    "    \n",
    "    hemi_mask = np.zeros_like(roi_data)\n",
    "    if hemisphere == 'left':\n",
    "        hemi_mask[:mid_x, :, :] = 1\n",
    "    else:\n",
    "        hemi_mask[mid_x:, :, :] = 1\n",
    "    \n",
    "    combined_mask = roi_data * hemi_mask\n",
    "    mask_img = nib.Nifti1Image(combined_mask, roi_mask.affine)\n",
    "    \n",
    "    roi_masker = input_data.NiftiMasker(mask_img, standardize=True)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    return np.mean(seed_time_series, axis=1).reshape(-1, 1)\n",
    "\n",
    "def make_psy_cov(runs, ss, temp_dir):\n",
    "    \"\"\"Create psychological covariates for specified runs.\"\"\"\n",
    "    cov_dir = f'{temp_dir}/covs'\n",
    "    times = np.arange(0, vols * len(runs), tr)\n",
    "    full_cov = pd.DataFrame(columns=['onset', 'duration', 'value'])\n",
    "\n",
    "    for run in runs:\n",
    "        ss_num = ss.split('-')[1].replace('spaceloc', '')\n",
    "        tool_cov_file = f'{cov_dir}/ToolLoc_spaceloc{ss_num}_run{run}_tool.txt'\n",
    "        nontool_cov_file = f'{cov_dir}/ToolLoc_spaceloc{ss_num}_run{run}_non_tool.txt'\n",
    "\n",
    "        if os.path.exists(tool_cov_file) and os.path.exists(nontool_cov_file):\n",
    "            tool_cov = pd.read_csv(tool_cov_file, sep='\\t', header=None, \n",
    "                                 names=['onset', 'duration', 'value'])\n",
    "            nontool_cov = pd.read_csv(nontool_cov_file, sep='\\t', header=None, \n",
    "                                    names=['onset', 'duration', 'value'])\n",
    "            nontool_cov['value'] *= -1\n",
    "\n",
    "            # Adjust onsets for concatenated runs\n",
    "            run_offset = vols * runs.index(run)\n",
    "            tool_cov['onset'] += run_offset\n",
    "            nontool_cov['onset'] += run_offset\n",
    "            \n",
    "            full_cov = pd.concat([full_cov, tool_cov, nontool_cov])\n",
    "\n",
    "    full_cov = full_cov.sort_values(by=['onset'])\n",
    "    cov = full_cov.to_numpy()\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    return psy\n",
    "\n",
    "def conduct_analyses(sub, rois=['LO', 'pIPS']):\n",
    "    import glob  # Add at top\n",
    "    \n",
    "    temp_dir = f'{raw_dir}/{sub}/ses-01/derivatives/fsl/toolloc'\n",
    "    roi_dir = f'{raw_dir}/{sub}/ses-01/derivatives/rois'\n",
    "    out_dir = f'/user_data/csimmon2/temp_derivatives/{sub}/ses-01/derivatives'\n",
    "    os.makedirs(f'{out_dir}/fc', exist_ok=True)\n",
    "    \n",
    "    mask_path = f'{raw_dir}/{sub}/ses-01/anat/{sub}_ses-01_T1w_brain_mask.nii.gz'\n",
    "    whole_brain_mask = nib.load(mask_path)\n",
    "    brain_masker = input_data.NiftiMasker(whole_brain_mask, standardize=True)\n",
    "\n",
    "    run_combos = [[rn1, rn2] for rn1 in range(1, run_num + 1) \n",
    "                  for rn2 in range(rn1 + 1, run_num + 1)]\n",
    "\n",
    "    for rr in rois:\n",
    "        roi_path = f'{roi_dir}/parcels/{rr}.nii.gz'\n",
    "        roi_img = nib.load(roi_path)\n",
    "        \n",
    "        for hemi in ['left', 'right']:\n",
    "            print(f\"Processing ROI: {rr}, Hemisphere: {hemi}\")\n",
    "            \n",
    "            fc_file = f'{out_dir}/fc/{sub}_{rr}_{hemi}_toolloc_fc_native.nii.gz'\n",
    "            ppi_file = f'{out_dir}/fc/{sub}_{rr}_{hemi}_toolloc_ppi_native.nii.gz'\n",
    "            \n",
    "            do_fc = not os.path.exists(fc_file)\n",
    "            do_ppi = not os.path.exists(ppi_file)\n",
    "            \n",
    "            if not do_fc and not do_ppi:\n",
    "                print(f'Both FC and PPI files exist for {rr} {hemi}. Skipping...')\n",
    "                continue\n",
    "            \n",
    "            for rc in run_combos:\n",
    "                print(f\"Processing run combination: {rc}\")\n",
    "                \n",
    "                filtered_list = []\n",
    "                for run in rc:\n",
    "                    curr_run = image.load_img(\n",
    "                        f'{temp_dir}/run-0{run}/1stLevel.feat/filtered_func_data_reg.nii.gz')\n",
    "                    curr_run = image.clean_img(curr_run, standardize=True)\n",
    "                    filtered_list.append(curr_run)\n",
    "\n",
    "                img4d = image.concat_imgs(filtered_list)\n",
    "                phys = extract_roi_timeseries(img4d, roi_img, hemisphere=hemi)\n",
    "                brain_time_series = brain_masker.fit_transform(img4d)\n",
    "                \n",
    "                if do_fc:\n",
    "                    correlations = np.dot(brain_time_series.T, phys) / phys.shape[0]\n",
    "                    correlations = np.arctanh(correlations)\n",
    "                    correlations = correlations.reshape(1, -1)\n",
    "                    correlation_img = brain_masker.inverse_transform(correlations)\n",
    "                    temp_fc_file = f'{out_dir}/fc/{sub}_{rr}_{hemi}_run{rc[0]}{rc[1]}_fc_temp.nii.gz'\n",
    "                    nib.save(correlation_img, temp_fc_file)\n",
    "                \n",
    "                if do_ppi:\n",
    "                    psy = make_psy_cov(rc, sub, f'{raw_dir}/{sub}/ses-01')\n",
    "                    \n",
    "                    min_length = min(psy.shape[0], phys.shape[0])\n",
    "                    psy = psy[:min_length]\n",
    "                    phys = phys[:min_length]\n",
    "                    brain_time_series = brain_time_series[:min_length]\n",
    "                    \n",
    "                    ppi = psy * phys\n",
    "                    correlations = np.dot(brain_time_series.T, ppi) / ppi.shape[0]\n",
    "                    correlations = np.arctanh(correlations)\n",
    "                    correlations = correlations.reshape(1, -1)\n",
    "                    correlation_img = brain_masker.inverse_transform(correlations)\n",
    "                    temp_ppi_file = f'{out_dir}/fc/{sub}_{rr}_{hemi}_run{rc[0]}{rc[1]}_ppi_temp.nii.gz'\n",
    "                    nib.save(correlation_img, temp_ppi_file)\n",
    "                \n",
    "                del img4d, phys, brain_time_series, correlations, correlation_img\n",
    "                if 'ppi' in locals(): del ppi\n",
    "                gc.collect()\n",
    "            \n",
    "            # After processing all run combinations, average temp files\n",
    "            if do_fc:\n",
    "                temp_fc_files = sorted(glob.glob(f'{out_dir}/fc/{sub}_{rr}_{hemi}_run*_fc_temp.nii.gz'))\n",
    "                mean_fc = image.mean_img(temp_fc_files)\n",
    "                nib.save(mean_fc, fc_file)\n",
    "                for f in temp_fc_files: os.remove(f)\n",
    "                print(f'Saved FC result for {rr} {hemi}')\n",
    "            \n",
    "            if do_ppi:\n",
    "                temp_ppi_files = sorted(glob.glob(f'{out_dir}/fc/{sub}_{rr}_{hemi}_run*_ppi_temp.nii.gz'))\n",
    "                mean_ppi = image.mean_img(temp_ppi_files)\n",
    "                nib.save(mean_ppi, ppi_file)\n",
    "                for f in temp_ppi_files: os.remove(f)\n",
    "                print(f'Saved PPI result for {rr} {hemi}')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rois = ['LO', 'pIPS']\n",
    "    sub = 'sub-spaceloc1007'\n",
    "    conduct_analyses(sub, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by Python: 0.06 GB\n",
      "Total system memory used: 3.5%\n"
     ]
    }
   ],
   "source": [
    "# check memory usage\n",
    "import psutil\n",
    "process = psutil.Process()\n",
    "print(f\"Memory used by Python: {process.memory_info().rss / 1024**3:.2f} GB\")\n",
    "print(f\"Total system memory used: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nilearn import image, plotting\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set directories\n",
    "raw_dir = params.raw_dir\n",
    "sub_dir = '/user_data/csimmon2/temp_derivatives/{sub}/ses-01'\n",
    "roi_dir = os.path.join(raw_dir, '{sub}/ses-01/derivatives/rois')\n",
    "parcel_dir = os.path.join(raw_dir, '{sub}/ses-01/derivatives/rois/parcels')\n",
    "output_dir = '/user_data/csimmon2/git_repos/ptoc/tools'\n",
    "\n",
    "# Read subject information and filter for spaceloc\n",
    "sub_info = pd.read_csv('/user_data/csimmon2/git_repos/ptoc/sub_info_tool.csv')\n",
    "subs = sub_info[sub_info['exp'] == 'spaceloc']['sub'].tolist()\n",
    "\n",
    "# Define parameters\n",
    "parcels = ['pIPS', 'LO', 'PFS', 'aIPS']\n",
    "run_combos = [[1, 2], [2, 1]]\n",
    "zstats = {'tools': 3, 'scramble': 8}  # Dictionary to map condition names to zstat numbers\n",
    "\n",
    "def extract_roi_coords():\n",
    "    # Initialize DataFrame to store all results\n",
    "    roi_coords = pd.DataFrame(columns=['subject', 'run_combo', 'task', 'condition', 'roi', 'hemisphere', 'x', 'y', 'z'])\n",
    "    \n",
    "    for ss in subs:\n",
    "        os.makedirs(f\"{sub_dir.format(sub=ss)}/spheres\", exist_ok=True)\n",
    "\n",
    "        for rcn, rc in enumerate(run_combos):\n",
    "            for run_num in rc:\n",
    "                # Process each condition (tools and scramble)\n",
    "                for condition, zstat_num in zstats.items():\n",
    "                    all_runs = [image.load_img(f\"{sub_dir.format(sub=ss)}/derivatives/stats/zstat{zstat_num}_reg_run{run_num}.nii.gz\")]\n",
    "                    mean_zstat = image.mean_img(all_runs)\n",
    "                    affine = mean_zstat.affine\n",
    "\n",
    "                    for pr in parcels:\n",
    "                        roi = image.load_img(f\"{parcel_dir.format(sub=ss)}/{pr}.nii.gz\")\n",
    "                        roi_data = roi.get_fdata()\n",
    "                        \n",
    "                        # Create hemisphere masks\n",
    "                        center_x = roi_data.shape[0] // 2\n",
    "                        left_mask = np.zeros_like(roi_data)\n",
    "                        right_mask = np.zeros_like(roi_data)\n",
    "                        left_mask[:center_x, :, :] = 1\n",
    "                        right_mask[center_x:, :, :] = 1\n",
    "                        \n",
    "                        for lr, hemi_mask in [('l', left_mask), ('r', right_mask)]:\n",
    "                            hemi_roi_data = roi_data * hemi_mask\n",
    "                            hemi_roi = image.new_img_like(roi, hemi_roi_data)\n",
    "                            hemi_roi = image.math_img('img > 0', img=hemi_roi)\n",
    "                            \n",
    "                            if np.sum(hemi_roi.get_fdata()) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            try:\n",
    "                                # Create masked statistical map\n",
    "                                masked_stat = image.math_img('img1 * img2', img1=hemi_roi, img2=mean_zstat)\n",
    "                                masked_data = masked_stat.get_fdata()\n",
    "                                \n",
    "                                # Find peak coordinates using argmax\n",
    "                                peak_idx = np.unravel_index(np.argmax(masked_data), masked_data.shape)\n",
    "                                coords = image.coord_transform(peak_idx[0], peak_idx[1], peak_idx[2], mean_zstat.affine)\n",
    "                                \n",
    "                                # Add results to DataFrame\n",
    "                                new_row = pd.DataFrame({\n",
    "                                    'subject': [ss],\n",
    "                                    'run_combo': [rcn],\n",
    "                                    'task': ['ToolLoc'],\n",
    "                                    'condition': [condition],\n",
    "                                    'roi': [f\"{lr}{pr}\"],\n",
    "                                    'hemisphere': [lr],\n",
    "                                    'x': [coords[0]],\n",
    "                                    'y': [coords[1]],\n",
    "                                    'z': [coords[2]]\n",
    "                                })\n",
    "                                roi_coords = pd.concat([roi_coords, new_row], ignore_index=True)\n",
    "                                \n",
    "                            except ValueError:\n",
    "                                continue\n",
    "\n",
    "    # Save all results to CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'roi_coordinates.csv')\n",
    "    roi_coords.to_csv(output_file, index=False)\n",
    "\n",
    "extract_roi_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:44:02,844 - INFO - Processing subject: sub-spaceloc1001\n",
      "2024-12-09 14:44:02,858 - INFO - Processing pIPS left\n"
     ]
    }
   ],
   "source": [
    "#fc_ppi native space with hemispheres\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/user_data/csimmon2/git_repos/ptoc')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import gc\n",
    "from nilearn import image, input_data, plotting\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "from nilearn.glm.first_level import compute_regressor\n",
    "import warnings\n",
    "import ptoc_params as params\n",
    "import time\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import logging\n",
    "\n",
    "# Settings\n",
    "raw_dir = params.raw_dir\n",
    "results_dir = params.results_dir\n",
    "sub_info_path = '/user_data/csimmon2/git_repos/ptoc/sub_info_tool.csv'\n",
    "\n",
    "# Load subject info\n",
    "sub_info = pd.read_csv(sub_info_path)\n",
    "subs = sub_info[sub_info['exp'] == 'spaceloc']['sub'].tolist()\n",
    "rois = ['pIPS', 'LO', 'PFS', 'aIPS']\n",
    "hemispheres = ['left', 'right']\n",
    "\n",
    "# Run parameters\n",
    "tr = 1\n",
    "vols = 341\n",
    "run_num = 2\n",
    "runs = list(range(1, run_num + 1))\n",
    "run_combos = [[1,2], [2,1]]\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from nilearn import image, plotting\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import ptoc_params as params\n",
    "\n",
    "# Set directories\n",
    "raw_dir = params.raw_dir\n",
    "sub_dir = '/user_data/csimmon2/temp_derivatives/{sub}/ses-01'\n",
    "roi_dir = os.path.join(raw_dir, '{sub}/ses-01/derivatives/rois')\n",
    "parcel_dir = os.path.join(raw_dir, '{sub}/ses-01/derivatives/rois/parcels')\n",
    "output_dir = '/user_data/csimmon2/git_repos/ptoc/tools'\n",
    "\n",
    "# Read subject information and filter for spaceloc\n",
    "sub_info = pd.read_csv('/user_data/csimmon2/git_repos/ptoc/sub_info_tool.csv')\n",
    "subs = sub_info[sub_info['exp'] == 'spaceloc']['sub'].tolist()\n",
    "\n",
    "# Define parameters\n",
    "parcels = ['pIPS', 'LO', 'PFS', 'aIPS']\n",
    "run_combos = [[1, 2], [2, 1]]\n",
    "zstats = {'tools': 3, 'scramble': 8}  # Dictionary to map condition names to zstat numbers\n",
    "\n",
    "def extract_roi_coords():\n",
    "    # Initialize DataFrame to store all results\n",
    "    roi_coords = pd.DataFrame(columns=['subject', 'run_combo', 'task', 'condition', 'roi', 'hemisphere', 'x', 'y', 'z'])\n",
    "    \n",
    "    for ss in subs:\n",
    "        os.makedirs(f\"{sub_dir.format(sub=ss)}/spheres\", exist_ok=True)\n",
    "\n",
    "        for rcn, rc in enumerate(run_combos):\n",
    "            for run_num in rc:\n",
    "                # Process each condition (tools and scramble)\n",
    "                for condition, zstat_num in zstats.items():\n",
    "                    all_runs = [image.load_img(f\"{sub_dir.format(sub=ss)}/derivatives/stats/zstat{zstat_num}_reg_run{run_num}.nii.gz\")]\n",
    "                    mean_zstat = image.mean_img(all_runs)\n",
    "                    affine = mean_zstat.affine\n",
    "\n",
    "                    for pr in parcels:\n",
    "                        roi = image.load_img(f\"{parcel_dir.format(sub=ss)}/{pr}.nii.gz\")\n",
    "                        roi_data = roi.get_fdata()\n",
    "                        \n",
    "                        # Create hemisphere masks\n",
    "                        center_x = roi_data.shape[0] // 2\n",
    "                        left_mask = np.zeros_like(roi_data)\n",
    "                        right_mask = np.zeros_like(roi_data)\n",
    "                        left_mask[:center_x, :, :] = 1\n",
    "                        right_mask[center_x:, :, :] = 1\n",
    "                        \n",
    "                        for lr, hemi_mask in [('l', left_mask), ('r', right_mask)]:\n",
    "                            hemi_roi_data = roi_data * hemi_mask\n",
    "                            hemi_roi = image.new_img_like(roi, hemi_roi_data)\n",
    "                            hemi_roi = image.math_img('img > 0', img=hemi_roi)\n",
    "                            \n",
    "                            if np.sum(hemi_roi.get_fdata()) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            try:\n",
    "                                # Create masked statistical map\n",
    "                                masked_stat = image.math_img('img1 * img2', img1=hemi_roi, img2=mean_zstat)\n",
    "                                masked_data = masked_stat.get_fdata()\n",
    "                                \n",
    "                                # Find peak coordinates using argmax\n",
    "                                peak_idx = np.unravel_index(np.argmax(masked_data), masked_data.shape)\n",
    "                                coords = image.coord_transform(peak_idx[0], peak_idx[1], peak_idx[2], mean_zstat.affine)\n",
    "                                \n",
    "                                # Add results to DataFrame\n",
    "                                new_row = pd.DataFrame({\n",
    "                                    'subject': [ss],\n",
    "                                    'run_combo': [rcn],\n",
    "                                    'task': ['ToolLoc'],\n",
    "                                    'condition': [condition],\n",
    "                                    'roi': [f\"{lr}{pr}\"],\n",
    "                                    'hemisphere': [lr],\n",
    "                                    'x': [coords[0]],\n",
    "                                    'y': [coords[1]],\n",
    "                                    'z': [coords[2]]\n",
    "                                })\n",
    "                                roi_coords = pd.concat([roi_coords, new_row], ignore_index=True)\n",
    "                                \n",
    "                            except ValueError:\n",
    "                                continue\n",
    "\n",
    "    # Save all results to CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'roi_coordinates.csv')\n",
    "    roi_coords.to_csv(output_file, index=False)\n",
    "\n",
    "def extract_roi_sphere(img, coords):\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius=6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    return np.mean(seed_time_series, axis=1).reshape(-1, 1)\n",
    "\n",
    "def make_psy_cov(run, ss):\n",
    "    \"\"\"Generate psychological covariates combining tool and scramble conditions\"\"\"\n",
    "    cov_dir = f'{raw_dir}/{ss}/ses-01/covs'\n",
    "    times = np.arange(0, vols * tr, tr)\n",
    "    \n",
    "    # Load tool condition\n",
    "    tool_cov = pd.read_csv(f'{cov_dir}/ToolLoc_spaceloc{ss}_run{run}_tool.txt', \n",
    "                          sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "    \n",
    "    # Load and negate scramble condition\n",
    "    scramble_cov = pd.read_csv(f'{cov_dir}/ToolLoc_spaceloc{ss}_run{run}_scramble.txt', \n",
    "                              sep='\\t', header=None, names=['onset', 'duration', 'value'])\n",
    "    scramble_cov['value'] *= -1\n",
    "    \n",
    "    # Combine conditions\n",
    "    full_cov = pd.concat([tool_cov, scramble_cov])\n",
    "    full_cov = full_cov.sort_values(by=['onset'])\n",
    "    \n",
    "    # Create regressor\n",
    "    cov = full_cov.to_numpy()\n",
    "    psy, _ = compute_regressor(cov.T, 'spm', times)\n",
    "    return psy\n",
    "\n",
    "def extract_roi_sphere(img, coords):\n",
    "    \"\"\"Extract mean time series from spherical ROI\"\"\"\n",
    "    roi_masker = input_data.NiftiSpheresMasker([tuple(coords)], radius=6)\n",
    "    seed_time_series = roi_masker.fit_transform(img)\n",
    "    return np.mean(seed_time_series, axis=1).reshape(-1, 1)\n",
    "\n",
    "def conduct_analyses():\n",
    "    \"\"\"Conduct PPI analyses for all subjects and ROIs\"\"\"\n",
    "    logger = setup_logging()\n",
    "    \n",
    "    for ss in subs:\n",
    "        logger.info(f\"Processing subject: {ss}\")\n",
    "        \n",
    "        temp_dir = f'{raw_dir}/{ss}/ses-01/derivatives/fsl/toolloc'\n",
    "        mask_path = f'{raw_dir}/{ss}/ses-01/anat/{ss}_ses-01_T1w_brain_mask.nii.gz'\n",
    "        out_dir = f'/user_data/csimmon2/temp_derivatives/{ss}/ses-01/derivatives'\n",
    "        os.makedirs(f'{out_dir}/fc', exist_ok=True)\n",
    "        \n",
    "        roi_coords = pd.read_csv(f'{output_dir}/roi_coordinates.csv')\n",
    "        \n",
    "        try:\n",
    "            whole_brain_mask = nib.load(mask_path)\n",
    "            brain_masker = NiftiMasker(whole_brain_mask, standardize=True)\n",
    "            \n",
    "            for roi in rois:\n",
    "                for hemi in hemispheres:\n",
    "                    hemi_prefix = hemi[0]\n",
    "                    logger.info(f\"Processing {roi} {hemi}\")\n",
    "                    \n",
    "                    fc_file = f'{out_dir}/fc/{ss}_{roi}_{hemi}_ToolLoc_fc.nii.gz'\n",
    "                    ppi_file = f'{out_dir}/fc/{ss}_{roi}_{hemi}_ToolLoc_ppi.nii.gz'\n",
    "                    \n",
    "                    all_runs_fc = []\n",
    "                    all_runs_ppi = []\n",
    "                    \n",
    "                    for rcn, rc in enumerate(run_combos):\n",
    "                        roi_run = rc[0]\n",
    "                        analysis_run = rc[1]\n",
    "                        \n",
    "                        try:\n",
    "                            curr_coords = roi_coords[\n",
    "                                (roi_coords['subject'] == ss) &\n",
    "                                (roi_coords['run_combo'] == rcn) & \n",
    "                                (roi_coords['roi'] == f\"{hemi_prefix}{roi}\") &\n",
    "                                (roi_coords['hemisphere'] == hemi_prefix)\n",
    "                            ]\n",
    "                            \n",
    "                            if curr_coords.empty:\n",
    "                                logger.warning(f\"No coordinates found for {ss} {roi} {hemi} run_combo {rcn}\")\n",
    "                                continue\n",
    "                                \n",
    "                            coords = [\n",
    "                                curr_coords['x'].values[0],\n",
    "                                curr_coords['y'].values[0],\n",
    "                                curr_coords['z'].values[0]\n",
    "                            ]\n",
    "                            \n",
    "                            img = image.clean_img(\n",
    "                                image.load_img(f'{temp_dir}/run-0{analysis_run}/1stLevel.feat/filtered_func_data_reg.nii.gz'),\n",
    "                                standardize=True\n",
    "                            )\n",
    "                            \n",
    "                            phys = extract_roi_sphere(img, coords)\n",
    "                            brain_time_series = brain_masker.fit_transform(img)\n",
    "                            \n",
    "                            # FC Analysis\n",
    "                            correlations = np.dot(brain_time_series.T, phys) / phys.shape[0]\n",
    "                            correlations = np.arctanh(correlations.ravel())\n",
    "                            correlation_img = brain_masker.inverse_transform(correlations)\n",
    "                            all_runs_fc.append(correlation_img)\n",
    "                            \n",
    "                            # PPI Analysis\n",
    "                            psy = make_psy_cov(analysis_run, ss)\n",
    "                            \n",
    "                            min_length = min(psy.shape[0], phys.shape[0], brain_time_series.shape[0])\n",
    "                            psy = psy[:min_length]\n",
    "                            phys = phys[:min_length]\n",
    "                            \n",
    "                            confounds = pd.DataFrame({\n",
    "                                'psy': psy[:,0],\n",
    "                                'phys': phys[:,0]\n",
    "                            })\n",
    "                            \n",
    "                            brain_time_series = brain_masker.fit_transform(img, confounds=[confounds])\n",
    "                            \n",
    "                            ppi_regressor = phys * psy\n",
    "                            ppi_correlations = np.dot(brain_time_series.T, ppi_regressor) / ppi_regressor.shape[0]\n",
    "                            ppi_correlations = np.arctanh(ppi_correlations.ravel())\n",
    "                            ppi_img = brain_masker.inverse_transform(ppi_correlations)\n",
    "                            all_runs_ppi.append(ppi_img)\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Error in run combo {rc}: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if all_runs_fc:\n",
    "                        mean_fc = image.mean_img(all_runs_fc)\n",
    "                        nib.save(mean_fc, fc_file)\n",
    "                    \n",
    "                    if all_runs_ppi:\n",
    "                        mean_ppi = image.mean_img(all_runs_ppi)\n",
    "                        nib.save(mean_ppi, ppi_file)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing subject {ss}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "def create_summary():\n",
    "    \"\"\"Extract average PPI values for each ROI pair\"\"\"\n",
    "    \n",
    "    summary_df = pd.DataFrame(columns=['sub'] + [f\"{h}{r}\" for h in hemispheres for r in rois])\n",
    "    \n",
    "    for ss in subs:\n",
    "        roi_means = [ss]\n",
    "        \n",
    "        # For each target ROI\n",
    "        for target_hemi in hemispheres:\n",
    "            for target_roi in rois:\n",
    "                roi = f\"{target_hemi[0]}{target_roi}\"\n",
    "                \n",
    "                try:\n",
    "                    # Load ROI mask\n",
    "                    roi_mask = image.load_img(f'{raw_dir}/{ss}/ses-01/derivatives/rois/{roi}.nii.gz')\n",
    "                    roi_masker = input_data.NiftiMasker(roi_mask)\n",
    "                    \n",
    "                    # Load PPI map\n",
    "                    ppi_img = image.load_img(f'{out_dir}/sub-{ss}_{roi}_fc.nii.gz')\n",
    "                    \n",
    "                    # Extract mean value\n",
    "                    acts = roi_masker.fit_transform(ppi_img)\n",
    "                    roi_means.append(acts.mean())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    roi_means.append(np.nan)\n",
    "                    \n",
    "        summary_df = summary_df.append(pd.Series(roi_means, index=summary_df.columns), ignore_index=True)\n",
    "    \n",
    "    summary_df.to_csv(f'{results_dir}/roi_ppi_summary.csv', index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    logger = setup_logging()\n",
    "    #extract_roi_coords() # completed and saved to roi_coordinates.csv\n",
    "    create_summary()\n",
    "    conduct_analyses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
